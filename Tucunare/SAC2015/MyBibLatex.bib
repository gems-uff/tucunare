
@inproceedings{yu_reviewer_2014,
    series = {{ICSME}},
	title = {Reviewer Recommender of Pull-Requests in {GitHub}},
	url = {http://conferences.computer.org/icsme/2014/papers/6146a609.pdf},
	doi = {10.1109/ICSME.2014.107},
	abstract = {Pull-Request ({PR}) is the primary method for code
contributions from thousands of developers in {GitHub}. To main-
tain the quality of software projects, {PR} review is an essential
part of distributed software development. Assigning new {PRs} to appropriate reviewers will make the review process more
effective which can reduce the time between the submission of a
{PR} and the actual review of it. However, reviewer assignment
is now organized manually in {GitHub}. To reduce this cost,
we propose a reviewer recommender to predict highly relevant
reviewers of incoming {PRs}. Combining information retrieval with
social network analyzing, our approach takes full advantage
of the textual semantic of {PRs} and the social relations of
developers. We implement an online system to show how the
reviewer recommender helps project managers to find potential
reviewers from crowds. Our approach can reach a precision of
74\% for top-1 recommendation, and a recall of 71\% for top-10
recommendation.},
	urldate = {2014-11-03},
	booktitle = {Proceedings of the 30th International Conference on Software Maintenance and Evolution},
	publisher = {{IEEE}},
	year = {2014},
	author = {Yu, Yue and Wang, Huaimin and Yin, Gang and Ling, Charles X.},
	pages = {609--612},
	file = {Reviewer Recommender of Pull-Requests in GitHub - 6146a609.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\IJMGQKD3\\6146a609.pdf:application/pdf}
}


@inproceedings{lamkanfi_predicting_2010,
	title = {Predicting the severity of a reported bug},
	doi = {10.1109/MSR.2010.5463284},
	abstract = {The severity of a reported bug is a critical factor in deciding how soon it needs to be fixed. Unfortunately, while clear guidelines exist on how to assign the severity of a bug, it remains an inherent manual process left to the person reporting the bug. In this paper we investigate whether we can accurately predict the severity of a reported bug by analyzing its textual description using text mining algorithms. Based on three cases drawn from the open-source community (Mozilla, Eclipse and {GNOME}), we conclude that given a training set of sufficient size (approximately 500 reports per severity), it is possible to predict the severity with a reasonable accuracy (both precision and recall vary between 0.65-0.75 with Mozilla and Eclipse; 0.70-0.85 in the case of {GNOME}).},
	eventtitle = {2010 7th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	pages = {1--10},
	author = {Lamkanfi, A. and Demeyer, S. and Giger, E. and Goethals, B.},
	date = {2010},
	note = {Cited by 0028},
	keywords = {Algorithm design and analysis, Computer architecture, Computer bugs, Computer crashes, data mining, Eclipse, {GNOME}, Guidelines, Mozilla, open source community, program debugging, Programming, public domain software, reported bug, Seals, severity prediction, Software debugging, Software systems, text mining, text mining algorithms, textual description}
}

@article{bhattacharya_automated_2012,
	title = {Automated, highly-accurate, bug assignment using machine learning and tossing graphs},
	volume = {85},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121212001240},
	doi = {10.1016/j.jss.2012.04.053},
	abstract = {Empirical studies indicate that automating the bug assignment process has the potential to significantly reduce software evolution effort and costs. Prior work has used machine learning techniques to automate bug assignment but has employed a narrow band of tools which can be ineffective in large, long-lived software projects. To redress this situation, in this paper we employ a comprehensive set of machine learning tools and a probabilistic graph-based model (bug tossing graphs) that lead to highly-accurate predictions, and lay the foundation for the next generation of machine learning-based bug assignment. Our work is the first to examine the impact of multiple machine learning dimensions (classifiers, attributes, and training history) along with bug tossing graphs on prediction accuracy in bug assignment. We validate our approach on Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 86.09\% prediction accuracy in bug assignment and significantly reduce tossing path lengths. We show that for our data sets the {NaÃ}¯ve Bayes classifier coupled with productâ€“component features, tossing graphs and incremental learning performs best. Next, we perform an ablative analysis by unilaterally varying classifiers, features, and learning model to show their relative importance of on bug assignment accuracy. Finally, we propose optimization techniques that achieve high prediction accuracy while reducing training and prediction time.},
	pages = {2275--2292},
	number = {10},
	journal = {Journal of Systems and Software},
	author = {Bhattacharya, Pamela and Neamtiu, Iulian and Shelton, Christian R.},
	urldate = {2013-09-13},
	date = {2012},
	%note = {Cited by 0003},
	keywords = {Bug assignment, Bug assignment, bug tossing, bug tossing, empirical studies, empirical studies, machine learning, machine learning}
}

@article{sliwerski_hatari:_2005,
	title = {{HATARI}: raising risk awareness},
	volume = {30},
	issn = {0163-5948},
	url = {http://doi.acm.org/10.1145/1095430.1081725},
	doi = {10.1145/1095430.1081725},
	shorttitle = {{HATARI}},
	abstract = {As a software system evolves, programmers make changes which sometimes lead to problems. The risk of later problems significantly depends on the location of the change. Which are the locations where changes impose the greatest risk? Our {HATARI} prototype relates a version history (such as {CVS}) to a bug database (such as {BUGZILLA}) to detect those locations where changes have been risky in the past. {HATARI} makes this risk visible for developers by annotating source code with color bars. Furthermore, {HATARI} provides views to browse through the most risky locations and to analyze the risk history of a particular location.},
	pages = {107--110},
	number = {5},
	journal = {{SIGSOFT} Softw. Eng. Notes},
	author = {Śliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
	urldate = {2013-08-13},
	date = {2005-09},
	%note = {Cited by 0045},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\AAHR5SMH\\Śliwerski et al. - 2005 - HATARI raising risk awareness.pdf:application/pdf}
}

@inproceedings{kim_automatic_2006,
	title = {Automatic Identification of Bug-Introducing Changes},
	doi = {10.1109/ASE.2006.23},
	abstract = {Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bug-introducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38\% 51\% of false positives and 14\% 15\% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research},
	pages = {81--90},
	booktitle = {21st {IEEE}/{ACM} International Conference on Automated Software Engineering, 2006. {ASE} '06},
	author = {Kim, Sunghun and Zimmermann, T. and Pan, Kai and Whitehead, E.J.},
	date = {2006},
	%note = {Cited by 0093},
	keywords = {annotation graphs, annotation graphs, automatic bug identification, automatic bug identification, bug fixing, bug fixing, Computer bugs, Computer bugs, Computer industry, Computer industry, configuration management, configuration management, Electrical equipment industry, Electrical equipment industry, false negatives, false negatives, false positives, false positives, Fixtures, Fixtures, graph theory, graph theory, History, History, inspection, inspection, nonsemantic source code changes, nonsemantic source code changes, Open source software, Open source software, outlier fix, outlier fix, program debugging, program debugging, project management, project management, Risk analysis, Risk analysis, software bugs, software bugs, Software debugging, Software debugging}
}

@book{han_data_2006,
	edition = {2Âª},
	title = {Data Mining: Concepts and Techniques},
	publisher = {Morgan Kaufmann},
	author = {Han, J and Kamber, M},
	date = {2006},
	%note = {Cited by 16374}
}

@book{_guidelines_2007,
	title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
	abstract = {null},
	date = {2007},
	%note = {Cited by 0631}
}

@inproceedings{lamkanfi_predicting_2013,
	title = {Predicting Reassignments of Bug Reports - An Exploratory Investigation},
	doi = {10.1109/CSMR.2013.42},
	abstract = {Bug tracking systems play an important role in the software development process since they allow users to report bugs they have encountered. Unfortunately, the information provided in the bug reports may contain errors since the reporter is not always acquainted with the technical nature of the software project. It remains a manual process left to the bug triager to correct reports with erroneous information. Consequently, this results in a delay of the resolution time of a bug. In this study, we propose to use data-mining techniques to make early predictions of which particular reported bugs are assigned to the incorrect component. This way, bug triagers are assisted in their task of identifying reported bugs where the "component" field contains an incorrect value. By using open-source cases like Mozilla and Eclipse, we demonstrate the possibility of predicting which particular bug is likely to be reassigned to a different "component".},
	eventtitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {327--330},
	author = {Lamkanfi, A. and Demeyer, S.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {bug report reassignment, bug reports, bug resolution time, bug tracking system, bug triager, data mining, data mining technique, Eclipse, Mining Software Repositories ({MSR}), Mozilla, open-source software, program debugging, public domain software, software development process, software engineering, software project, Textual analysis},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\M4QGTZCV\\articleDetails.html:text/html;IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XUJFBQN6\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\295TFBV6\\Lamkanfi e Demeyer - 2013 - Predicting Reassignments of Bug Reports - An Explo.pdf:application/pdf;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\PH7GKDBM\\Lamkanfi e Demeyer - 2013 - Predicting Reassignments of Bug Reports - An Explo.pdf:application/pdf}
}

@inproceedings{ba_sddirm:_2012,
	title = {{SdDirM}: A dynamic defect prediction model},
	doi = {10.1109/MESA.2012.6275570},
	shorttitle = {{SdDirM}},
	abstract = {Defect prediction and estimation techniques play an important role in software reliability engineering. This paper proposes a dynamic defect prediction model named {SdDirM} (System dynamic based Defect injection and removal Model) to improve the quantitative defect management process. Using {SdDirM}, we can simulate defect introduction and removal processes, and predict and estimate the residual defects in different phases. We describe the modeling process, the validation and the results with the empirical and real project data compared with the other well known models. These experiments show that the managers can use the model to explore and analyze the potential improvements before the practice.},
	eventtitle = {2012 {IEEE}/{ASME} International Conference on Mechatronics and Embedded Systems and Applications ({MESA})},
	pages = {252--256},
	booktitle = {2012 {IEEE}/{ASME} International Conference on Mechatronics and Embedded Systems and Applications ({MESA})},
	author = {Ba, Jie and Wu, Shujian},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {Data models, defect estimation technique, defect introduction and removal process, defect removal efficiency, dynamic defect prediction model, Encoding, Estimation, Predictive models, Productivity, quantitative defect management process, removal process, residual defect, {SdDirM}, software, software defect prediction, Software reliability, software reliability engineering, system dynamic based defect injection and removal model, system dynamics, Testing}
}

@inproceedings{wang_high-dimensional_2009,
	title = {High-Dimensional Software Engineering Data and Feature Selection},
	doi = {10.1109/ICTAI.2009.20},
	abstract = {Software metrics collected during project development play a critical role in software quality assurance. A software practitioner is very keen on learning which software metrics to focus on for software quality prediction. While a concise set of software metrics is often desired, a typical project collects a very large number of metrics. Minimal attention has been devoted to finding the minimum set of software metrics that have the same predictive capability as a larger set of metrics - we strive to answer that question in this paper. We present a comprehensive comparison between seven commonly-used filter-based feature ranking techniques ({FRT}) and our proposed hybrid feature selection ({HFS}) technique. Our case study consists of a very high-dimensional (42 software attributes) software measurement data set obtained from a large telecommunications system. The empirical analysis indicates that {HFS} performs better than {FRT}; however, the Kolmogorov-Smirnov feature ranking technique demonstrates competitive performance. For the telecommunications system, it is found that only 10\% of the software attributes are sufficient for effective software quality prediction.},
	pages = {83--90},
	booktitle = {21st International Conference on Tools with Artificial Intelligence, 2009. {ICTAI} '09},
	author = {Wang, Huanjing and Khoshgoftaar, T.M. and Gao, Kehan and Seliya, N.},
	date = {2009},
	%note = {Cited by 0005},
	keywords = {Artificial intelligence, Artificial intelligence, data mining, data mining, feature ranking, feature ranking, feature ranking techniques, feature ranking techniques, Feature Selection, Feature Selection, Filters, Filters, high-dimensional data, high-dimensional data, high-dimensional software engineering data, high-dimensional software engineering data, hybrid feature selection, hybrid feature selection, Kolmogorov-Smirnov feature ranking technique, Kolmogorov-Smirnov feature ranking technique, large telecommunications system, large telecommunications system, machine learning, machine learning, Power system modeling, Power system modeling, Predictive models, Predictive models, project development, project development, quality prediction, quality prediction, software attributes, software attributes, software engineering, software engineering, Software measurement, Software measurement, Software metrics, Software metrics, software practitioner, software practitioner, software quality, software quality, software quality assurance, software quality assurance}
}

@article{estublier_impact_2005,
	title = {Impact of software engineering research on the practice of software configuration management},
	volume = {14},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/1101815.1101817},
	doi = {10.1145/1101815.1101817},
	abstract = {Software Configuration Management ({SCM}) is an important discipline in professional software development and maintenance. The importance of {SCM} has increased as programs have become larger, more long lasting, and more mission and life critical. This article discusses the evolution of {SCM} technology from the early days of software development to the present, with a particular emphasis on the impact that university and industrial research has had along the way. Based on an analysis of the publication history and evolution in functionality of the available {SCM} systems, we trace the critical ideas in the field from their early inception to their eventual maturation in commercially and freely available {SCM} systems. In doing so, this article creates a detailed record of the critical value of {SCM} research and illustrates how research results have shaped the functionality of today's {SCM} systems.},
	pages = {383--430},
	number = {4},
	journal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Estublier, Jacky and Leblang, David and Hoek, André van der and Conradi, Reidar and Clemm, Geoffrey and Tichy, Walter and Wiborg-Weber, Darcy},
	urldate = {2012-07-11},
	date = {2005-10},
	%note = {Cited by 0129},
	keywords = {data model, data model, process support, process support, research impact, research impact, software configuration management, software configuration management, software engineering, software engineering, Versioning, Versioning, workspace management, workspace management},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\N8WCPMFZ\\Estublier et al. - 2005 - Impact of software engineering research on the pra.pdf:application/pdf}
}

@article{elish_predicting_2008,
	title = {Predicting defect-prone software modules using support vector machines},
	volume = {81},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S016412120700235X},
	doi = {10.1016/j.jss.2007.07.040},
	shorttitle = {Software Process and Product Measurement},
	abstract = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines ({SVM}) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of {SVM} in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four {NASA} datasets. The results indicate that the prediction performance of {SVM} is generally better than, or at least, is competitive against the compared models.},
	pages = {649--660},
	number = {5},
	journal = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Elish, Karim O. and Elish, Mahmoud O.},
	urldate = {2013-07-30},
	date = {2008-05},
	%note = {Cited by 0105},
	keywords = {Defect-prone modules, Predictive models, Software metrics, support vector machines}
}

@article{dejaeger_toward_2013,
	title = {Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers},
	volume = {39},
	issn = {0098-5589},
	doi = {10.1109/TSE.2012.20},
	abstract = {Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network ({BN}) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to {BN} theory, is investigated. The results, both in terms of the {AUC} and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using {BN} classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.},
	pages = {237--257},
	number = {2},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Dejaeger, K. and Verbraken, T. and Baesens, B.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {{AUC}, Bayesian methods, Bayesian network classifiers, bayesian networks, belief networks, {BN} classifiers, {BN} theory, capability maturity model, citing predictive performance, Classification, Comprehensibility, Demsar, faulty software code, feature extraction, Feature Selection, introduced H-measure, learning (artificial intelligence), machine learning, machine learning literature, Markov blanket principle, Markov processes, Measurement, model selection, Naive Bayes classifier, pattern classification, prediction theory, Predictive models, predictive performance, probability distribution, program testing, software, software development, Software fault prediction, software fault prediction models, software fault tolerance, Software testing, statistical analysis, statistical framework},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\HGVTRBFC\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\W6UXWSIV\\Dejaeger et al. - 2013 - Toward Comprehensible Software Fault Prediction Mo.pdf:application/pdf}
}

@article{yuan_predicting_2013,
	title = {Predicting Bugs in Source Code Changes with Incremental Learning Method},
	volume = {8},
	rights = {Copyright \&copy;  {ACADEMY} {PUBLISHER}  - All Rights Reserved. To request permission, please check out {URL}:  http://www.academypublisher.com/copyrightpermission.html .},
	issn = {1796-217X},
	url = {http://www.ojs.academypublisher.com/index.php/jsw/article/view/jsw080716201633},
	doi = {10.4304/jsw.8.7.1620-1633},
	abstract = {Predicting Bugs in Source Code Changes with Incremental Learning Method},
	pages = {1620--1633},
	number = {7},
	journal = {Journal of Software},
	author = {Yuan, Zi and Yu, Lili and Liu, Chao and Zhang, Linghua},
	urldate = {2013-07-20},
	date = {2013-07-01},
	langid = {english},
	%note = {Cited by 0000},
	keywords = {Bug prediction, concept drift, incremental learning, software engineering, source code change},
	file = {9080-23088-1-PB.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\K8NXR7ZS\\9080-23088-1-PB.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\MRGZ7Z2G\\7299.html:text/html}
}

@inproceedings{hindle_automatic_2009,
	title = {Automatic classication of large changes into maintenance categories},
	doi = {10.1109/ICPC.2009.5090025},
	abstract = {Large software systems undergo significant evolution during their lifespan, yet often individual changes are not well documented. In this work, we seek to automatically classify large changes into various categories of maintenance tasks - corrective, adaptive, perfective, feature addition, and non-functional improvement - using machine learning techniques. In a previous paper, we found that many commits could be classified easily and reliably based solely on the manual analysis of the commit metadata and commit messages (i.e., without reference to the source code). Our extension is the automation of classification by training machine learners on features extracted from the commit metadata, such as the word distribution of a commit message, commit author, and modules modified. We validated the results of the learners via 10-fold cross validation, which achieved accuracies consistently above 50\%, indicating good to fair results. We found that the identity of the author of a commit provided much information about the maintenance class of a commit, almost as much as the words of the commit message. This implies that for most large commits, the Source Control System ({SCS}) commit messages plus the commit author identity is enough information to accurately and automatically categorize the nature of the maintenance task.},
	pages = {30--39},
	booktitle = {{IEEE} 17th International Conference on Program Comprehension, 2009. {ICPC} '09},
	author = {Hindle, A. and German, D.M. and Godfrey, M.W. and Holt, R.C.},
	date = {2009},
	%note = {Cited by 0031},
	keywords = {automatic classification, automatic classification, Automatic control, Automatic control, Automation, Automation, commit messages, commit messages, commit metadata, commit metadata, Control systems, Control systems, feature extraction, feature extraction, learning (artificial intelligence), learning (artificial intelligence), machine learning, machine learning, Maintenance, Maintenance, maintenance categories, maintenance categories, maintenance task, maintenance task, Merging, Merging, meta data, meta data, Programming profession, Programming profession, software libraries, software libraries, software maintenance, software maintenance, Software systems, Software systems, source control system, source control system, task analysis, task analysis}
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {0885-6125},
	url = {http://dx.doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2014-09-25},
	year = {2001},
	keywords = {Classification, ensemble, regression},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\HFUW65EX\\Statistics e Breiman - 2001 - Random Forests.pdf:application/pdf}
}

@article{cotroneo_predicting_2013,
	title = {Predicting aging-related bugs using software complexity metrics},
	volume = {70},
	issn = {0166-5316},
	url = {http://www.sciencedirect.com/science/article/pii/S0166531612000946},
	doi = {10.1016/j.peva.2012.09.004},
	shorttitle = {Special Issue on Software Aging and Rejuvenation},
	abstract = {Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs ({ARBs}). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about {ARBs}. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs.},
	pages = {163--178},
	number = {3},
	journal = {Performance Evaluation},
	author = {Cotroneo, Domenico and Natella, Roberto and Pietrantuono, Roberto},
	urldate = {2013-02-28},
	date = {2013-03},
	%note = {Cited by 0000},
	keywords = {Aging-related bugs, Aging-related bugs, Fault prediction, Fault prediction, Software aging, Software aging, Software complexity metrics, Software complexity metrics}
}

@inproceedings{martie_trendy_2012,
	title = {Trendy bugs: Topic trends in the Android bug reports},
	doi = {10.1109/MSR.2012.6224268},
	shorttitle = {Trendy bugs},
	abstract = {Studying vast volumes of bug and issue discussions can give an understanding of what the community has been most concerned about, however the magnitude of documents can overload the analyst. We present an approach to analyze the development of the Android open source project by observing trends in the bug discussions in the Android open source project public issue tracker. This informs us of the features or parts of the project that are more problematic at any given point of time. In turn, this can be used to aid resource allocation (such as time and man power) to parts or features. We support these ideas by presenting the results of issue topic distributions over time using statistical analysis of the bug descriptions and comments for the Android open source project. Furthermore, we show relationships between those time distributions and major development releases of the Android {OS}.},
	eventtitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	pages = {120--123},
	booktitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {Martie, L. and Palepu, V.K. and Sajnani, H. and Lopes, C.},
	date = {2012},
	%note = {Cited by 0004},
	keywords = {Android, Android bug reports, Android open source project public issue tracker, Android {OS}, Androids, bug logs, documents magnitude, Google, Graphics, Humanoid robots, Java, operating system kernels, program debugging, resource allocation, Runtime, Smart phones, statistical analysis, statistical trend analysis, topic distributions, topics, trendy bugs}
}

@article{tsoumakas_multi-label_2007,
	title = {Multi-label classification: An overview},
	volume = {2007},
	shorttitle = {Multi-label classification},
	abstract = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
	pages = {1--13},
	journal = {Int J Data Warehousing and Mining},
	author = {Tsoumakas, Grigorios and Katakis, Ioannis},
	date = {2007},
	%note = {Cited by 0523},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\IE44FBM9\\Tsoumakas e Katakis - 2007 - Multi-label classification An overview.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\EZB62WJK\\summary.html:text/html;Multi-Label Classification\: An Overview | IGI Global:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\W2AGD5VB\\1786.html:text/html}
}

@inproceedings{hindle_what_2008,
	location = {New York, {NY}, {USA}},
	title = {What do large commits tell us?: a taxonomical study of large commits},
	isbn = {978-1-60558-024-1},
	url = {http://doi.acm.org/10.1145/1370750.1370773},
	doi = {10.1145/1370750.1370773},
	series = {{MSR} '08},
	shorttitle = {What do large commits tell us?},
	abstract = {Research in the mining of software repositories has frequently ignored commits that include a large number of files (we call these large commits). The main goal of this paper is to understand the rationale behind large commits, and if there is anything we can learn from them. To address this goal we performed a case study that included the manual classification of large commits of nine open source projects. The contributions include a taxonomy of large commits, which are grouped according to their intention. We contrast large commits against small commits and show that large commits are more perfective while small commits are more corrective. These large commits provide us with a window on the development practices of maintenance teams.},
	pages = {99--108},
	booktitle = {Proceedings of the 2008 international working conference on Mining software repositories},
	publisher = {{ACM}},
	author = {Hindle, Abram and German, Daniel M. and Holt, Ric},
	urldate = {2013-10-12},
	date = {2008},
	%note = {Cited by 0082},
	keywords = {large commits, software evolution, source control system},
	file = {Hindle et al_2008_What do large commits tell us.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\63IQ747N\\Hindle et al_2008_What do large commits tell us.pdf:application/pdf}
}

@inproceedings{hata_extension_2008,
	location = {New York, {NY}, {USA}},
	title = {An extension of fault-prone filtering using precise training and a dynamic threshold},
	isbn = {978-1-60558-024-1},
	url = {http://doi.acm.org/10.1145/1370750.1370772},
	doi = {10.1145/1370750.1370772},
	series = {{MSR} '08},
	abstract = {Fault-prone module detection in source code is important for assurance of software quality. Most previous fault-prone detection approaches have been based on software metrics. Such approaches, however, have difficulties in collecting the metrics and in constructing mathematical models based on the metrics. To mitigate such difficulties, we have proposed a novel approach for detecting fault-prone modules using a spam-filtering technique, named Fault-Prone Filtering. In our approach, fault-prone modules are detected in such a way that the source code modules are considered as text files and are applied to the spam filter directly. In practice, we use the training only errors procedure and apply this procedure to fault-prone. Since no pre-training is required, this procedure can be applied to an actual development field immediately. This paper describes an extension of the training only errors procedures. We introduce a precise unit of training, "modified lines of code," instead of methods. In addition, we introduce the dynamic threshold for classification. The result of the experiment shows that our extension leads to twice the precision with about the same recall, and improves 15\% on the best F1 measurement.},
	pages = {89â€“98},
	booktitle = {Proceedings of the 2008 international working conference on Mining software repositories},
	publisher = {{ACM}},
	author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
	urldate = {2013-07-23},
	date = {2008},
	%note = {Cited by 0006},
	keywords = {fault-prone modules, fault-prone modules, spam filter, spam filter, text mining, text mining}
}

@inproceedings{romano_using_2011,
	title = {Using source code metrics to predict change-prone Java interfaces},
	doi = {10.1109/ICSM.2011.6080797},
	abstract = {Recent empirical studies have investigated the use of source code metrics to predict the change- and defect-proneness of source code files and classes. While results showed strong correlations and good predictive power of these metrics, they do not distinguish between interface, abstract or concrete classes. In particular, interfaces declare contracts that are meant to remain stable during the evolution of a software system while the implementation in concrete classes is more likely to change. This paper aims at investigating to which extent the existing source code metrics can be used for predicting change-prone Java interfaces. We empirically investigate the correlation between metrics and the number of fine-grained source code changes in interfaces of ten Java open-source systems. Then, we evaluate the metrics to calculate models for predicting change-prone Java interfaces. Our results show that the external interface cohesion metric exhibits the strongest correlation with the number of source code changes. This metric also improves the performance of prediction models to classify Java interfaces into change-prone and not change-prone.},
	pages = {303--312},
	booktitle = {2011 27th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Romano, D. and Pinzger, M.},
	date = {2011},
	%note = {Cited by 0015},
	keywords = {change-prone Java interfaces, change-prone Java interfaces, change-proneness, change-proneness, Complexity theory, Complexity theory, Concrete, Concrete, concrete classes, concrete classes, Correlation, Correlation, defect-proneness, defect-proneness, external interface cohesion metric, external interface cohesion metric, fine-grained source code, fine-grained source code, Java, Java, Java open-source systems, Java open-source systems, Measurement, Measurement, prediction models, prediction models, Predictive models, Predictive models, Software metrics, Software metrics, Software reliability, Software reliability, software system, software system, Software systems, Software systems, source code classes, source code classes, source code files, source code files, source code metrics, source code metrics, user interfaces, user interfaces}
}

@inproceedings{asaduzzaman_answering_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Answering questions about unanswered questions of stack overflow},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487109},
	series = {{MSR} '13},
	abstract = {Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.},
	pages = {97â€“100},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Asaduzzaman, Muhammad and Mashiyat, Ahmed Shah and Roy, Chanchal K. and Schneider, Kevin A.},
	urldate = {2013-11-05},
	date = {2013},
	%note = {Cited by 0001}
}

@inproceedings{gousios_ghtorrent:_2012,
	title = {{GHTorrent}: Github's data from a firehose},
	doi = {10.1109/MSR.2012.6224294},
	shorttitle = {{GHTorrent}},
	abstract = {A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, {GitHub} has emerged as a popular project hosting, mirroring and collaboration platform. {GitHub} provides an extensive {REST} {API}, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. {GHTorrent} aims to create a scalable off line mirror of {GitHub}'s event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.},
	eventtitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	pages = {12--21},
	author = {Gousios, G. and Spinellis, D.},
	date = {2012},
	%note = {Cited by 0006},
	keywords = {application program interfaces, collaboration platform, commits, Communities, data acquisition, data curation, dataset, Distributed databases, Electronic mail, events, {GHTorrent}, {GitHub}, Github data, mirroring platform, Open source software, Organizations, Peer to peer computing, project hosting platform, project resources, Protocols, public domain software, repository, {REST} {API}, software engineering, software engineering studies, software repositories, storage management, user actions},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\H2BETII7\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\78VNC55F\\Gousios e Spinellis - 2012 - GHTorrent Github's data from a firehose.pdf:application/pdf}
}

@inproceedings{lewis_does_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Does bug prediction support human developers? findings from a google case study},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486838},
	series = {{ICSE} '13},
	shorttitle = {Does bug prediction support human developers?},
	abstract = {While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code.},
	pages = {372â€“381},
	booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Lewis, Chris and Lin, Zhongpeng and Sadowski, Caitlin and Zhu, Xiaoyan and Ou, Rong and Whitehead Jr., E. James},
	urldate = {2013-06-24},
	date = {2013},
	%note = {Cited by 0000},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2249JG3D\\Lewis et al. - 2013 - Does bug prediction support human developers find.pdf:application/pdf}
}

@article{petersson_capturerecapture_2004,
	title = {Capture–recapture in software inspections after 10 years research––theory, evaluation and application},
	volume = {72},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121203000906},
	doi = {10.1016/S0164-1212(03)00090-6},
	abstract = {Software inspection is a method to detect faults in the early phases of the software life cycle. In order to estimate the number of faults not found, capture–recapture was introduced for software inspections in 1992 to estimate remaining faults after an inspection. Since then, several papers have been written in the area, concerning the basic theory, evaluation of models and application of the method. This paper summarizes the work made in capture–recapture for software inspections during these years. Furthermore, and more importantly, the contribution of the papers are classified as theory, evaluation or application, in order to analyse the performed research as well as to highlight the areas of research that need further work. It is concluded that (1) most of the basic theory is investigated within biostatistics, (2) most software engineering research is performed on evaluation, a majority ending up in recommendation of the Mh–{JK} model, and (3) there is a need for application experiences. In order to support the application, an inspection process is presented with decision points based on capture–recapture estimates.},
	pages = {249--264},
	number = {2},
	journal = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Petersson, Håkan and Thelin, Thomas and Runeson, Per and Wohlin, Claes},
	urldate = {2013-09-12},
	date = {2004-07},
	%note = {Cited by 0064},
	file = {ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\34Q8P6U7\\S0164121203000906.html:text/html}
}

@inproceedings{matter_assigning_2009,
	title = {Assigning bug reports using a vocabulary-based expertise model of developers},
	doi = {10.1109/MSR.2009.5069491},
	abstract = {For popular software systems, the number of daily submitted bug reports is high. Triaging these incoming reports is a time consuming task. Part of the bug triage is the assignment of a report to a developer with the appropriate expertise. In this paper, we present an approach to automatically suggest developers who have the appropriate expertise for handling a bug report. We model developer expertise using the vocabulary found in their source code contributions and compare this vocabulary to the vocabulary of bug reports. We evaluate our approach by comparing the suggested experts to the persons who eventually worked on the bug. Using eight years of Eclipse development as a case study, we achieve 33.6\% top-1 precision and 71.0\% top-10 recall.},
	eventtitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	pages = {131--140},
	author = {Matter, D. and Kuhn, A. and Nierstrasz, O.},
	date = {2009},
	%note = {Cited by 0060},
	keywords = {bug reports, bug triage, Calibration, Counting circuits, developer expertise, Eclipse development, Open source software, program debugging, Prototypes, software development management, software system, Software systems, source code, Vocabulary, vocabulary-based expertise model}
}

@inproceedings{shihab_predicting_2010,
	title = {Predicting Re-opened Bugs: A Case Study on the Eclipse Project},
	doi = {10.1109/WCRE.2010.36},
	shorttitle = {Predicting Re-opened Bugs},
	abstract = {Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on the Eclipse project. We structure our study along 4 dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed on), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). Our case study on the Eclipse Platform 3.0 project shows that the comment and description text, the time it took to fix the bug, and the component the bug was found in are the most important factors in determining whether a bug will be re-opened. Based on these dimensions we create decision trees that predict whether a bug will be re-opened after its closure. Using a combination of our dimensions, we can build explainable prediction models that can achieve 62.9\% precision and 84.5\% recall when predicting whether a bug will be re-opened.},
	pages = {249--258},
	booktitle = {2010 17th Working Conference on Reverse Engineering ({WCRE})},
	author = {Shihab, E. and Ihara, A. and Kamei, Y. and Ibrahim, W.M. and Ohira, M. and Adams, B. and Hassan, A.E. and Matsumoto, K.-i.},
	date = {2010},
	%note = {Cited by 0023},
	keywords = {Accuracy, Accuracy, Computer bugs, Computer bugs, Databases, Databases, data mining, data mining, decision trees, decision trees, Eclipse project, Eclipse project, Predictive models, Predictive models, program debugging, program debugging, Re-opened bugs, Re-opened bugs, software, software, software maintenance, software maintenance, software maintenance resource, software maintenance resource, software quality, software quality, team dimension, team dimension, text description, text description}
}

@inproceedings{chaturvedi_determining_2012,
	title = {Determining Bug severity using machine learning techniques},
	doi = {10.1109/CONSEG.2012.6349519},
	abstract = {Software Bug reporting is an integral part of software development process. Once the Bug is reported on Bug Tracking System, their attributes are analyzed and subsequently assigned to various fixers for their resolution. During the last two decades Machine-Learning Techniques ({MLT}) has been used to create self-improving software. Supervised machine learning technique is widely used for prediction of patterns in various applications but, we have found very few for software repositories. Bug severity, an attribute of a software bug report is the degree of impact that a defect has on the development or operation of a component or system. Bug severity can be classified into different levels based on their impact on the system. In this paper, an attempt has been made to demonstrate the applicability of machine learning algorithms namely {NaiÌˆve} Bayes, k-Nearest Neighbor, {NaiÌˆve} Bayes Multinomial, Support Vector Machine, J48 and {RIPPER} in determining the class of bug severity of bug report data of {NASA} from {PROMISE} repository. The applicability of algorithm in determining the various levels of bug severity for bug repositories has been validated using various performance measures by applying 5-fold cross validation.},
	pages = {1--6},
	booktitle = {2012 {CSI} Sixth International Conference on Software Engineering ({CONSEG})},
	author = {Chaturvedi, K.K. and Singh, V.B.},
	date = {2012},
	%note = {Cited by 0001},
	keywords = {Accuracy, Accuracy, Bayes methods, Bayes methods, bug severity, bug severity, bug tracking system, bug tracking system, Classification algorithms, Classification algorithms, Feature Selection, Feature Selection, J48, J48, k-nearest neighbor, k-nearest neighbor, learning (artificial intelligence), learning (artificial intelligence), machine learning, machine learning, {MLT}, {MLT}, {naiÌˆve} Bayes classifier, {naiÌˆve} Bayes multinomial, naïve Bayes classifier, naïve Bayes multinomial, {NASA}, {NASA}, Niobium, Niobium, pattern classification, pattern classification, program debugging, program debugging, {PROMISE} repository, {PROMISE} repository, {RIPPER} in, {RIPPER} in, self-improving software, self-improving software, software, software, software bug reporting, software bug reporting, software development process, software development process, software engineering, software engineering, software repositories, software repositories, Supervised Classification, Supervised Classification, supervised machine learning technique, supervised machine learning technique, support vector machine, support vector machine, support vector machines, support vector machines, text mining, text mining}
}

@inproceedings{chang_defect_2005,
	title = {A defect estimation approach for sequential inspection using a modified capture-recapture model},
	volume = {1},
	doi = {10.1109/COMPSAC.2005.19},
	abstract = {Defect prediction is an important process in the evaluation of software quality. To accurately predict the rate of software defects can not only facilitate software review decisions, but can also improve software quality. In this paper, we have provided a defect estimation approach, which uses defective data from sequential inspections to increase the accuracy of estimating defects. To demonstrate potential improvements, the results of our approach were compared to those of two other popular estimation approaches, the capture-recapture model and the re-inspection model. By using the proposed approach, software organizations may increase the accuracy of their defect predictions and reduce the effort of subsequent inspections.},
	eventtitle = {Computer Software and Applications Conference, 2005. {COMPSAC} 2005. 29th Annual International},
	pages = {41--46 Vol. 2},
	booktitle = {Computer Software and Applications Conference, 2005. {COMPSAC} 2005. 29th Annual International},
	author = {Chang, Ching-Pao and Lv, Jia-Lyn and Chu, Chih-Ping},
	date = {2005},
	%note = {Cited by 0006},
	keywords = {and re-inspection model, and re-inspection model, Animals, Animals, Biological system modeling, Biological system modeling, capture-recapture model, capture-recapture model, Chromium, Chromium, Computational biology, Computational biology, Costs, Costs, defect estimation, defect estimation, defective data, defective data, defect predictions, defect predictions, inspection, inspection, Maximum likelihood detection, Maximum likelihood detection, Maximum likelihood estimation, Maximum likelihood estimation, Predictive models, Predictive models, program testing, program testing, reinspection model, reinspection model, sequential inspection, sequential inspection, software defect estimation, software defect estimation, software development management, software development management, software organization, software organization, software process improvement, software process improvement, software quality, software quality, software quality evaluation, software quality evaluation}
}

@inproceedings{robbes_mining_2007,
	title = {Mining a Change-Based Software Repository},
	doi = {10.1109/MSR.2007.18},
	abstract = {Although state-of-the-art software repositories based on versioning system information are useful to assess the evolution of a software system, the information they contain is limited in several ways. Versioning systems such as {CVS} or subversion store only snapshots of text files, leading to a loss of information: The exact sequence of changes between two versions is hard to recover. In this paper we present an alternative information repository which stores incremental changes to the system under study, retrieved from the {IDE} used to build the software. We then use this change-based model of system evolution to assess when refactorings happen in two case studies, and compare our findings with refactoring detection approaches on classical versioning system repositories.},
	pages = {15},
	author = {Robbes, R.},
	date = {2007-05},
	%note = {Cited by 0040},
	keywords = {change-based software repository, configuration management, data mining, information repository, software evolution, software maintenance, software prototyping, software refactoring, versioning system information}
}

@inproceedings{khomh_predicting_2011,
	title = {Predicting post-release defects using pre-release field testing results},
	doi = {10.1109/ICSM.2011.6080792},
	abstract = {Field testing is commonly used to detect faults after the in-house (e.g., alpha) testing of an application is completed. In the field testing, the application is instrumented and used under normal conditions. The occurrences of failures are reported. Developers can analyze and fix the reported failures before the application is released to the market. In the current practice, the Mean Time Between Failures ({MTBF}) and the Average usage Time ({AVT}) are metrics that are frequently used to gauge the reliability of the application. However, {MTBF} and {AVT} cannot capture the whole pattern of failure occurrences in the field testing of an application. In this paper, we propose three metrics that capture three additional patterns of failure occurrences: the average length of usage time before the occurrence of the first failure, the spread of failures to the majority of users, and the daily rates of failures. In our case study, we use data derived from the pre-release field testing of 18 versions of a large enterprise software for mobile applications to predict the number of post-release defects for up to two years in advance. We demonstrate that the three metrics complement the traditional {MTBF} and {AVT} metrics. The proposed metrics can predict the number of post-release defects in a shorter time frame than {MTBF} and {AVT}.},
	pages = {253--262},
	booktitle = {2011 27th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Khomh, F. and Chan, B. and Zou, Ying and Sinha, A. and Dietz, D.},
	date = {2011},
	%note = {Cited by 0000},
	keywords = {average usage time, average usage time, {AVT}, {AVT}, failure occurrences, failure occurrences, fault detection, fault detection, fault diagnosis, fault diagnosis, in-house testing, in-house testing, large enterprise software, large enterprise software, mean time between failures, mean time between failures, metrics, metrics, mobile applications, mobile applications, {MTBF}, {MTBF}, post-release defects, post-release defects, prediction, prediction, pre-release field testing results, pre-release field testing results, program testing, program testing, Software reliability, Software reliability, system recovery, system recovery}
}

@article{dabbish_leveraging_2013,
	title = {Leveraging Transparency},
	volume = {30},
	issn = {0740-7459},
	doi = {10.1109/MS.2012.172},
	abstract = {A new generation of development environments takes a radical approach to communication and coordination by fusing social networking functionality with flexible, distributed version control. Through these transparent work environments, people, repositories, development activities, and their histories are immediately and easily visible to all users. Developers quickly acquire the skill to interpret this rich information to find useful resources, connect with people, solve technical problems, and enhance their learning opportunities. This article presents the results of a qualitative study of users of one such environment, {GitHub}. It describes how transparency helps developers on {GitHub} manage their projects, handle dependencies more effectively, reduce communication needs, and figure out what requires their attention. Although transparency is not a silver bullet, the approach shows great promise for enhancing collaboration and coordination. The Web extra at http://www.youtube.com/watch?v={LpGA}2fmAHvM is a video of Joel Spolsky discussing the structure, software, technology, and culture of Stack Exchange.},
	pages = {37--43},
	number = {1},
	journal = {{IEEE} Software},
	author = {Dabbish, L. and Stuart, C. and Tsay, J. and Herbsleb, J.},
	date = {2013-01},
	keywords = {collaboration, coordination, development environments, distributed version control, {GitHub}, group interfaces, {HCI}, information interfaces and representation, Information technology, information technology and systems, Internet, learning opportunities, Organizational aspects, organizational management, organization interfaces, project management, security of data, social networking functionality, social networking (online), Social network services, software engineering, software engineering tools and techniques, software management, transparency leverage, user interfaces},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\Q4PXCTK4\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\BWXE6FSP\\Dabbish et al. - 2013 - Leveraging Transparency.pdf:application/pdf}
}

@article{peterson_github_2013,
	title = {The {GitHub} Open Source Development Process},
	author = {Peterson, Kevin},
	date = {2013},
	file = {The GitHub Open Source Development Process - ResearchGate:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CHBMQ58T\\259217367_The_GitHub_Open_Source_Development_Process.html:text/html}
}

@inproceedings{nagwani_data_2010,
	title = {A Data Mining Model to Predict Software Bug Complexity Using Bug Estimation and Clustering},
	doi = {10.1109/ITC.2010.56},
	abstract = {Software defect(bug) repositories are great source of knowledge. Data mining can be applied on these repositories to explore useful interesting patterns. Complexity of a bug helps the development team to plan future software build and releases. In this paper a prediction model is proposed to predict the bug's complexity. The proposed technique is a three step method. In the first step, fix duration for all the bugs stored in bug repository is calculated and complexity clusters are created based on the calculated bug fix duration. In second step, bug for which complexity is required its estimated fix time is calculated using bug estimation techniques. And in the third step based on the estimated fix time of bug it is mapped to a complexity cluster, which defines the complexity of the bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.},
	eventtitle = {2010 International Conference on Recent Trends in Information, Telecommunication and Computing ({ITC})},
	pages = {13--17},
	booktitle = {2010 International Conference on Recent Trends in Information, Telecommunication and Computing ({ITC})},
	author = {Nagwani, N.K. and Bhansali, A.},
	date = {2010},
	%note = {Cited by 0002},
	keywords = {bug clustering, bug clustering, Bug complexity, Bug complexity, bug estimation, bug estimation, complexity cluster, complexity cluster, Complexity Prediction, Complexity Prediction, Computer bugs, Computer bugs, data mining, data mining, Open source software, Open source software, open source technology, open source technology, pattern clustering, pattern clustering, prediction model, prediction model, Predictive models, Predictive models, program debugging, program debugging, Programming, Programming, project management, project management, software bug complexity, software bug complexity, Software bug repositories, Software bug repositories, software defect repositories, software defect repositories, software development management, software development management, Software testing, Software testing, System testing, System testing, Telecommunication computing, Telecommunication computing}
}

@inproceedings{thummalapenta_guided_2013,
	title = {Guided test generation for web applications},
	pages = {162--171},
	booktitle = {{ICSE}},
	author = {Thummalapenta, Suresh and Lakshmi, K. Vasanta and Sinha, Saurabh and Sinha, Nishant and Chandra, Satish},
	date = {2013},
	%note = {Cited by 0000}
}

@inproceedings{kagdi_mining_2007,
	title = {Mining software repositories for traceability links},
	doi = {10.1109/ICPC.2007.28},
	abstract = {An approach to recover/discover traceability links between software artifacts via the examination of a software system's version history is presented. A heuristic-based approach that uses sequential-pattern mining is applied to the commits in software repositories for uncovering highly frequent co-changing sets of artifacts (e.g., source code and documentation). If different types of files are committed together with high frequency then there is a high probability that they have a traceability link between them. The approach is evaluated on a number of versions of the open source system {KDE}. As a validation step, the discovered links are used to predict similar changes in the newer versions of the same system. The results show highly precision predictions of certain types of traceability links.},
	pages = {145 --154},
	booktitle = {15th {IEEE} International Conference on Program Comprehension, 2007. {ICPC} '07},
	author = {Kagdi, H. and Maletic, J.I. and Sharif, B.},
	date = {2007-06},
	%note = {Cited by 0037},
	keywords = {data mining, data mining, sequential pattern mining, sequential pattern mining, software artifacts, software artifacts, software engineering, software engineering, software repositories, software repositories, software system, software system, traceability links, traceability links}
}

@incollection{tsoumakas_mining_2010,
	title = {Mining Multi-label Data},
	rights = {©2010 Springer Science+Business Media, {LLC}},
	isbn = {978-0-387-09822-7, 978-0-387-09823-4},
	url = {http://link.springer.com/chapter/10.1007/978-0-387-09823-4_34},
	abstract = {A large body of research in supervised learning deals with the analysis of single-label data, where training examples are associated with a single label λ from a set of disjoint labels L. However, training examples in several application domains are often associated with a set of labels Y ⊆ L. Such data are called multi-label. Textual data, such as documents and web pages, are frequently annotated with more than a single label. For example, a news article concerning the reactions of the Christian church to the release of the “Da Vinci Code” film can be labeled as both religion and movies. The categorization of textual data is perhaps the dominant multi-label application.},
	pages = {667--685},
	booktitle = {Data Mining and Knowledge Discovery Handbook},
	publisher = {Springer {US}},
	author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
	editor = {Maimon, Oded and Rokach, Lior},
	urldate = {2013-08-06},
	date = {2010-01-01},
	langid = {english},
	%note = {Cited by 0286},
	keywords = {Artificial Intelligence (incl. Robotics), Database Management, Information Storage and Retrieval, Information Systems and Communication Service, Information Systems Applications (incl.Internet)},
	file = {[9] 2010 Data Mining and Knowledge Discovery Handbook.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\RDIEE442\\[9] 2010 Data Mining and Knowledge Discovery Handbook.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\5MTMJTFR\\978-0-387-09823-4_34.html:text/html}
}

@inproceedings{brunet_developers_2014,
	location = {New York, {NY}, {USA}},
	title = {Do Developers Discuss Design?},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597115},
	doi = {10.1145/2597073.2597115},
	series = {{MSR} 2014},
	abstract = {Design is often raised in the literature as important to attaining various properties and characteristics in a software system. At least for open-source projects, it can be hard to find evidence of ongoing design work in the technical artifacts produced as part of the development. Although developers usually do not produce specific design documents, they do communicate about design in different ways. In this paper, we provide quantitative evidence that developers address design through discussions in commits, issues, and pull requests. To achieve this, we built a discussions' classifier and automatically labeled 102,122 discussions from 77 projects. Based on this data, we make four observations about the projects: i) on average, 25\% of the discussions in a project are about design; ii) on average, 26\% of developers contribute to at least one design discussion; iii) only 1\% of the developers contribute to more than 15\% of the discussions in a project; and iv) these few developers who contribute to a broad range of design discussions are also the top committers in a project.},
	pages = {340--343},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Brunet, João and Murphy, Gail C. and Terra, Ricardo and Figueiredo, Jorge and Serey, Dalton},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Design Discussions, Empirical study, machine learning},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\BN4SGS8S\\Brunet et al. - 2014 - Do Developers Discuss Design.pdf:application/pdf}
}

@inproceedings{aversano_learning_2007,
	location = {New York, {NY}, {USA}},
	title = {Learning from bug-introducing changes to prevent fault prone code},
	isbn = {978-1-59593-722-3},
	url = {http://doi.acm.org/10.1145/1294948.1294954},
	doi = {10.1145/1294948.1294954},
	series = {{IWPSE} '07},
	abstract = {A version control system, such as {CVS}/{SVN}, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes. In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots. The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for {KNN} (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.},
	pages = {19--26},
	publisher = {{ACM}},
	author = {Aversano, Lerina and Cerulo, Luigi and Del Grosso, Concettina},
	urldate = {2013-07-30},
	date = {2007},
	%note = {Cited by 0022},
	keywords = {Bug prediction, mining software repositories, software evolution}
}

@book{kotsiantis_supervised_2007,
	title = {Supervised Machine Learning: A Review of Classification Techniques. Informatica 31:249–268},
	shorttitle = {Supervised Machine Learning},
	abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored. Povzetek: Podan je pregled metod strojnega učenja. 1},
	author = {Kotsiantis, S. B.},
	date = {2007},
	%note = {00000},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\F62QIUCH\\Kotsiantis - 2007 - Supervised Machine Learning A Review of Classific.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\WBJZ5WQE\\summary.html:text/html}
}

@article{hassan_guest_2005,
	title = {Guest Editor's Introduction: Special Issue on Mining Software Repositories},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.70},
	shorttitle = {Guest Editor's Introduction},
	pages = {426 -- 428},
	number = {6},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {Hassan, A.E. and Mockus, A. and Holt, R.C. and Johnson, P.M.},
	date = {2005-06},
	%note = {Cited by 0000}
}

@article{derrac_practical_2011,
	title = {A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms},
	volume = {1},
	issn = {2210-6502},
	url = {http://www.sciencedirect.com/science/article/pii/S2210650211000034},
	doi = {10.1016/j.swevo.2011.02.002},
	abstract = {The interest in nonparametric statistical analysis has grown recently in the field of computational intelligence. In many experimental studies, the lack of the required properties for a proper application of parametric procedures–independence, normality, and homoscedasticity–yields to nonparametric ones the task of performing a rigorous comparison among algorithms.

In this paper, we will discuss the basics and give a survey of a complete set of nonparametric procedures developed to perform both pairwise and multiple comparisons, for multi-problem analysis. The test problems of the {CEC}’2005 special session on real parameter optimization will help to illustrate the use of the tests throughout this tutorial, analyzing the results of a set of well-known evolutionary and swarm intelligence algorithms. This tutorial is concluded with a compilation of considerations and recommendations, which will guide practitioners when using these tests to contrast their experimental results.},
	pages = {3--18},
	number = {1},
	journal = {Swarm and Evolutionary Computation},
	shortjournal = {Swarm and Evolutionary Computation},
	author = {Derrac, Joaquín and García, Salvador and Molina, Daniel and Herrera, Francisco},
	urldate = {2014-08-14},
	date = {2011-03},
	keywords = {Evolutionary algorithms, Multiple comparisons, Nonparametric statistics, Pairwise comparisons, statistical analysis, Swarm intelligence algorithms},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\TH5IFQ47\\Derrac et al. - 2011 - A practical tutorial on the use of nonparametric s.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\AXU6MSDS\\S2210650211000034.html:text/html}
}

@inproceedings{keivanloo_linked_2012,
	title = {A Linked Data platform for mining software repositories},
	doi = {10.1109/MSR.2012.6224296},
	abstract = {The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce {SeCold}, an open and collaborative platform for sharing software datasets. {SeCold} provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the {SeCold} project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the {SeCold} portal and therefore make them an integrated part of the global knowledge domain. The {SeCold} project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.},
	pages = {32 --35},
	booktitle = {Mining Software Repositories ({MSR}), 2012 9th {IEEE} Working Conference on},
	author = {Keivanloo, Iman and Forbes, Christopher and Hmood, Aseel and Erfani, Mostafa and Neal, Christopher and Peristerakis, George and Rilling, Juergen},
	date = {2012-06},
	%note = {Cited by 0007},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\TZFAFT3D\\stamp.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2XUXE2XV\\Keivanloo et al. - 2012 - A Linked Data platform for mining software reposit.pdf:application/pdf}
}

@inproceedings{robles_replicating_2010,
	title = {Replicating {MSR}: A study of the potential replicability of papers published in the Mining Software Repositories proceedings},
	doi = {10.1109/MSR.2010.5463348},
	shorttitle = {Replicating {MSR}},
	abstract = {This paper is the result of reviewing all papers published in the proceedings of the former International Workshop on Mining Software Repositories ({MSR}) (2004-2006) and now Working Conference on {MSR} (2007-2009). We have analyzed the papers that contained any experimental analysis of software projects for their potentiality of being replicated. In this regard, three main issues have been addressed: i) the public availability of the data used as case study, ii) the public availability of the processed dataset used by researchers and iii) the public availability of the tools and scripts. A total number of 171 papers have been analyzed from the six workshops/working conferences up to date. Results show that {MSR} authors use in general publicly available data sources, mainly from free software repositories, but that the amount of publicly available processed datasets is very low. Regarding tools and scripts, for a majority of papers we have not been able to find any tool, even for papers where the authors explicitly state that they have built one. Lessons learned from the experience of reviewing the whole {MSR} literature and some potential solutions to lower the barriers of replicability are finally presented and discussed.},
	pages = {171--180},
	booktitle = {2010 7th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {Robles, G.},
	date = {2010},
	%note = {Cited by 0020},
	keywords = {data mining, data mining, data sources, data sources, mining software repositories, mining software repositories, mining software repositories proceedings, mining software repositories proceedings, {MSR}, {MSR}, {MSR} replication, {MSR} replication, potential replicability, potential replicability, Public datasets, Public datasets, replication, replication, software projects, software projects, tools, tools}
}

@inproceedings{thung_automatic_2012,
	title = {Automatic Defect Categorization},
	doi = {10.1109/WCRE.2012.30},
	abstract = {Defects are prevalent in software systems. In order to understand defects better, industry practitioners often categorize bugs into various types. One common kind of categorization is the {IBM}'s Orthogonal Defect Classification ({ODC}). {ODC} proposes various orthogonal classification of defects based on much information about the defects, such as the symptoms and semantics of the defects, the root cause analysis of the defects, and many more. With these category labels, developers can better perform post-mortem analysis to find out what the common characteristics of the defects that plague a particular software project are. Albeit the benefits of having these categories, for many software systems, these category labels are often missing. To address this problem, we propose a text mining solution that can categorize defects into various types by analyzing both texts from bug reports and code features from bug fixes. To this end, we have manually analyzed the data about 500 defects from three software systems, and classified them according to {ODC}. In addition, we propose a classification-based approach that can automatically classify defects into three super-categories that are comprised of {ODC} categories: control and data flow, structural, and non-functional. Our empirical evaluation shows that the automatic classification approach is able to label defects with an average accuracy of 77.8\% by using the {SVM} multiclass classification algorithm.},
	eventtitle = {2012 19th Working Conference on Reverse Engineering ({WCRE})},
	pages = {205--214},
	author = {Thung, F. and Lo, D. and Jiang, Lingxiao},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {automatic classification approach, automatic defect categorization, bug reports, code features, data mining, defect semantics, {IBM} orthogonal defect classification, industry practitioners, pattern classification, post-mortem analysis, program debugging, program diagnostics, reverse engineering, software projects, software system defects, support vector machines, {SVM} multiclass classification algorithm, text mining solution}
}

@inproceedings{ahsan_automatic_2009,
	title = {Automatic Classification of Software Change Request Using Multi-label Machine Learning Methods},
	doi = {10.1109/SEW.2009.15},
	abstract = {Automatic text classification of the software change request ({CR}) can be used for automating impact analysis, bug triage and effort estimation. In this paper, we focus on the automation of the process for assigning {CRs} to developers and present a solution that is based on automatic text classification of {CRs}. In addition our approach provides the list of source files, which are required to be modified and an estimate for the time required to resolve a given {CR}. To perform experiments, we downloaded the set of resolved {CRs} from the {OSS} project's repository for Mozilla. We labeled each {CR} with multiple labels i.e., the developer name, the list of source files, and the time spent to resolve the {CR}. To train the classifier, our approach applies the Problem Transformation and Algorithm Adaptation methods of multi-label machine learning to the multi-labeled {CR} data. With this approach, we have obtained precision levels up to 71.3\% with 40.1\% recall.},
	eventtitle = {2009 33rd Annual {IEEE} Software Engineering Workshop ({SEW})},
	pages = {79--86},
	booktitle = {2009 33rd Annual {IEEE} Software Engineering Workshop ({SEW})},
	author = {Ahsan, S.N. and Ferzund, J. and Wotawa, F.},
	date = {2009-10},
	keywords = {algorithm adaptation, automatic software change request classification, automatic text classification, bug triage, Indexing, information retrieval, Large scale integration, learning (artificial intelligence), machine learning, Machine learning algorithms, Mozilla, multi-label, multilabel machine learning methods, {OSS} project, pattern classification, problem transformation, semantics, software, software maintenance, text analysis, Time division multiplexing},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\88SGN3Z9\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\73CNFJX3\\Ahsan et al. - 2009 - Automatic Classification of Software Change Reques.pdf:application/pdf}
}

@inproceedings{anbalagan_mining_2009,
	title = {On mining data across software repositories},
	doi = {10.1109/MSR.2009.5069498},
	abstract = {Software repositories provide abundance of valuable information about open source projects. With the increase in the size of the data maintained by the repositories, automated extraction of such data from individual repositories, as well as of linked information across repositories, has become a necessity. In this paper we describe a framework that uses web scraping to automatically mine repositories and link information across repositories. We discuss two implementations of the framework. In the first implementation, we automatically identify and collect security problem reports from project repositories that deploy the Bugzilla bug tracker using related vulnerability information from the National Vulnerability Database. In the second, we collect security problem reports for projects that deploy the Launchpad bug tracker along with related vulnerability information from the National Vulnerability Database. We have evaluated our tool on various releases of Fedora, Ubuntu, Suse, {RedHat}, and Firefox projects. The percentage of security bugs identified using our tool is consistent with that reported by other researchers.},
	eventtitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	pages = {171--174},
	booktitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	author = {Anbalagan, P. and Vouk, M.},
	date = {2009},
	%note = {Cited by 0004},
	keywords = {Bugzilla bug tracker, Computer bugs, Computer Science, Databases, data mining, Data security, Government, information retrieval, Information security, Internet, Launchpad bug tracker, National security, open source projects, Open source software, program debugging, public domain software, software repositories, Web scraping},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\7ACX2E46\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\R3IAWWUA\\Anbalagan e Vouk - 2009 - On mining data across software repositories.pdf:application/pdf}
}

@article{jung_survey_2012,
	title = {A Survey on Mining Software Repositories},
	volume = {E95.D},
	abstract = {This paper presents fundamental concepts, overall process and recent research issues of Mining Software Repositories. The data sources such as source control systems, bug tracking systems or archived communications, data types and techniques used for general {MSR} problems are also presented. Finally, evaluation approaches, opportunities and challenge issues are given.},
	pages = {1384--1406},
	number = {5},
	journal = {{IEICE} Transactions on Information and Systems},
	author = {Jung, Woosung and Lee, Eunjoo and Wu, Chisu},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {analysis, analysis, change, change, evolution, evolution, extraction, extraction, mining, mining, repository, repository, software, software}
}

@inproceedings{jureczko_using_????,
	title = {Using Object-Oriented Design Metrics to Predict Software Defects},
	abstract = {Many object-oriented design metrics have been developed [1,3,8,17,24] to help in predict software defects or evaluate design quality. Since a defect prediction model may give crucial clues about the distribution and location of defects and, thereby, test prioritization, accurate prediction can save costs in the testing process. Considerable research},
	pages = {69--81},
	booktitle = {In Models and Methods of System Dependability. Oficyna Wydawnicza Politechniki Wrocławskiej},
	author = {Jureczko, Marian and Spinellis, Diomidis D.},
	%note = {Cited by 0019},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\5MJ5TI8S\\Jureczko e Spinellis - Using Object-Oriented Design Metrics to Predict So.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\S9ZWB2IM\\summary.html:text/html}
}

@article{song_general_2011,
	title = {A General Software Defect-Proneness Prediction Framework},
	volume = {37},
	issn = {0098-5589},
	doi = {10.1109/TSE.2010.90},
	abstract = {{BACKGROUND} - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. {OBJECTIVE} - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. {METHOD} - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. {RESULTS} - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. {CONCLUSIONS} - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.},
	pages = {356--370},
	number = {3},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Song, Qinbao and Jia, Zihan and Shepperd, M. and Ying, Shi and Liu, Jin},
	date = {2011},
	%note = {Cited by 0025},
	keywords = {Buildings, Buildings, competing learning schemes, competing learning schemes, Data models, Data models, defect predictor, defect predictor, learning (artificial intelligence), learning (artificial intelligence), machine learning, machine learning, Prediction algorithms, Prediction algorithms, Predictive models, Predictive models, scheme evaluation, scheme evaluation, scheme evaluation., scheme evaluation., software, software, software defect prediction, software defect prediction, software defect-proneness prediction, software defect-proneness prediction, software defect proneness prediction framework, software defect proneness prediction framework, software fault tolerance, software fault tolerance, software performance evaluation, software performance evaluation, Training, Training, Training data, Training data},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DRTXSCE6\\abs_all.html:text/html;IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\IJEEZ3NS\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XP9X2C49\\Song et al. - 2011 - A General Software Defect-Proneness Prediction Fra.pdf:application/pdf}
}

@report{dart_spectrum_1990,
	title = {Spectrum of Functionality in Configuration Management Systems},
	url = {http://www.sei.cmu.edu/library/abstracts/reports/90tr011.cfm},
	author = {Dart, Suzan},
	urldate = {2012-07-14},
	date = {1990},
	%note = {Cited by 0069}
}

@inproceedings{sayyad_supporting_2001,
	location = {Washington, {DC}, {USA}},
	title = {Supporting Software Maintenance by Mining Software Update Records},
	isbn = {0-7695-1189-9},
	url = {http://dx.doi.org/10.1109/ICSM.2001.972708},
	doi = {10.1109/ICSM.2001.972708},
	series = {{ICSM} '01},
	abstract = {This paper describes the application of inductive methods to data extracted from both source code and software maintenance records. We would like to extract relations that indicate which files in, a legacy system, are relevant to each other in the context of program maintenance. We call these relations Maintenance Relevance Relations. Such a relation could reveal existing complex interconnections among files in the system, which may in turn be useful in comprehending them. We discuss the methodology we employed to extract and evaluate the relations. We also point out some of the problems we encountered and our solutions for them. Finally, we present some of the results that we have obtained.},
	pages = {22--},
	publisher = {{IEEE} Computer Society},
	author = {Sayyad, Jelber and Lethbridge, C.},
	urldate = {2012-09-06},
	date = {2001},
	%note = {Cited by 0044}
}

@inproceedings{bortis_porchlight:_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {{PorchLight}: a tag-based approach to bug triaging},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486834},
	series = {{ICSE} '13},
	shorttitle = {{PorchLight}},
	abstract = {Bug triaging is an important activity in any software development project. It involves developers working through the set of unassigned bugs, determining for each of the bugs whether it represents a new issue that should receive attention, and, if so, assigning it to a developer and a milestone. Current tools provide only minimal support for bug triaging and especially break down when developers must triage a large number of bug reports, since those reports can only be viewed one-by-one. This paper presents {PorchLight}, a novel tool that uses tags, attached to individual bug reports by queries expressed in a specialized bug query language, to organize bug reports into sets so developers can explore, work with, and ultimately assign bugs effectively in meaningful groups. We describe the challenges in supporting bug triaging, the design decisions upon which {PorchLight} rests, and the technical aspects of the implementation. We conclude with an early evaluation that involved six professional developers who assessed {PorchLight} and its potential for their day-to-day triaging duties.},
	pages = {342--351},
	booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Bortis, Gerald and Hoek, André van der},
	urldate = {2013-08-12},
	date = {2013},
	%note = {Cited by 0000},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\NKT97FTN\\Bortis and Hoek - 2013 - PorchLight a tag-based approach to bug triaging.pdf:application/pdf}
}

@article{hall_systematic_2012,
	title = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
	volume = {38},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.103},
	abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
	pages = {1276--1304},
	number = {6},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Hall, T. and Beecham, S. and Bowes, D. and Gray, D. and Counsell, S.},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {Analytical models, Analytical models, Bayes methods, Bayes methods, Context modeling, Context modeling, contextual information, contextual information, cost reduction, cost reduction, Data models, Data models, fault diagnosis, fault diagnosis, fault prediction models, fault prediction models, fault prediction performance, fault prediction performance, fault prediction study, fault prediction study, Feature Selection, Feature Selection, independent variables, independent variables, logistic regression, logistic regression, methodological information, methodological information, Naive Bayes, Naive Bayes, Predictive models, Predictive models, predictive performance, predictive performance, regression analysis, regression analysis, reliable methodology, reliable methodology, simple modeling techniques, simple modeling techniques, software engineering, software engineering, Software fault prediction, Software fault prediction, software fault tolerance, software fault tolerance, software quality, software quality, Software testing, Software testing, Systematic literature review, Systematic literature review, Systematics, Systematics}
}

@inproceedings{statistics_random_2001,
	title = {Random Forests},
	abstract = {Random forests are a combination of tree predictors  such that each tree depends on the values of a random  vector sampled independently and with the same  distribution for all trees in the forest. The  generalization error for forests converges a.s. to a limit  as the number of trees in the forest becomes large.  The generalization error of a forest of tree classifiers  depends on the strength of the individual trees in the  forest and the correlation between them. Using a  random selection of features to split each node yields  error rates that compare favorably to Adaboost  (Freund and Schapire[1996]), but are more robust with  respect to noise. Internal estimates monitor error,  strength, and correlation and these are used to show  the response to increasing the number of features used  in the splitting. Internal estimates are also used to  measure variable importance. These ideas are also  applicable to regression.  2  1. Random Forests  1.1},
	pages = {5--32},
	booktitle = {Machine Learning},
	author = {Statistics, Leo Breiman and Breiman, Leo},
	date = {2001},
	file = {Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DH2TGXF7\\summary.html:text/html}
}

@inproceedings{kim_predicting_2013,
	location = {New York, {NY}, {USA}},
	title = {Predicting method crashes with bytecode operations},
	isbn = {978-1-4503-1987-4},
	url = {http://doi.acm.org/10.1145/2442754.2442756},
	doi = {10.1145/2442754.2442756},
	series = {{ISEC} '13},
	abstract = {Software monitoring systems have high performance overhead because they typically monitor all processes of the running program. For example, to capture and replay crashes, most current systems monitor all methods; thus yielding a significant performance overhead. Lowering the number of methods being monitored to a smaller subset can dramatically reduce this overhead. We present an approach that can help arrive at such a subset by reliably identifying methods that are the most likely candidates to crash in a future execution of the software. Our approach involves learning patterns from features of methods that previously crashed to classify new methods as crash-prone or non-crash-prone. An evaluation of our approach on two large open source projects, {ASPECTJ} and {ECLIPSE}, shows that we can correctly classify crash-prone methods with an accuracy of 80–86\%. Notably, we found that the classification models can also be used for cross-project prediction with virtually no loss in classification accuracy. In a further experiment, we demonstrate how a monitoring tool, {RECRASH} could take advantage of only monitoring crash-prone methods and thereby, reduce its performance overhead and maintain its ability to perform its intended tasks.},
	pages = {3â€“12},
	booktitle = {Proceedings of the 6th India Software Engineering Conference},
	publisher = {{ACM}},
	author = {Kim, Sunghun and Zimmermann, Thomas and Premraj, Rahul and Bettenburg, Nicolas and Shivaji, Shivkumar},
	urldate = {2013-02-27},
	date = {2013},
	%note = {Cited by 0001}
}

@book{witten_data_2011,
	edition = {3Âª},
	title = {Data Mining Practical Machine Learning Tools and Techniques},
	publisher = {Morgan Kaufmann},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	date = {2011},
	%note = {Cited by 0675}
}

@article{arisholm_systematic_2010,
	title = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
	volume = {83},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121209001605},
	doi = {10.1016/j.jss.2009.06.055},
	shorttitle = {{SI}: Top Scholars},
	abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area ({ROC}), and (iii) our proposed cost-effectiveness measure ({CE}). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases â€“ both in terms of {ROC} area and in terms of {CE}. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.},
	pages = {2--17},
	number = {1},
	journal = {Journal of Systems and Software},
	author = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
	urldate = {2013-07-20},
	date = {2010-01},
	%note = {Cited by 0073},
	keywords = {Cost-effectiveness, Cost-effectiveness, fault prediction models, fault prediction models, Verification, Verification}
}

@article{_systematic_????,
	title = {A systematic study of automated program repair: Fixing 55 out of 105 bugs for \$8 each.},
	volume = {11},
	pages = {3â€“13 process support research impact software configuration management software engineering Versioning workspace management},
	number = {4},
	%note = {Cited by 0029}
}

@inproceedings{padhye_study_2014,
	location = {New York, {NY}, {USA}},
	title = {A Study of External Community Contribution to Open-source Projects on {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597113},
	doi = {10.1145/2597073.2597113},
	series = {{MSR} 2014},
	abstract = {Open-source software projects are primarily driven by community contribution. However, commit access to such projects' software repositories is often strictly controlled. These projects prefer to solicit external participation in the form of patches or pull requests. In this paper, we analyze a set of 89 top-starred {GitHub} projects and their forks in order to explore the nature and distribution of such community contribution. We first classify commits (and developers) into three categories: core, external and mutant, and study the relative sizes of each of these classes through a ring-based visualization. We observe that projects written in mainstream scripting languages such as {JavaScript} and Python tend to include more external participation than projects written in upcoming languages such as Scala. We also visualize the geographic spread of these communities via geocoding. Finally, we classify the types of pull requests submitted based on their labels and observe that bug fixes are more likely to be merged into the main projects as compared to feature enhancements.},
	pages = {332--335},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Padhye, Rohan and Mani, Senthil and Sinha, Vibha Singhal},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {community participation, core committers, external contribution, mining software repositories, open-source software, pull requests},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\6ST9DNZV\\Padhye et al. - 2014 - A Study of External Community Contribution to Open.pdf:application/pdf;ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\VD3RBDF3\\Padhye et al. - 2014 - A Study of External Community Contribution to Open.pdf:application/pdf}
}

@article{alzghoul_data_2012,
	title = {Data stream forecasting for system fault prediction},
	volume = {62},
	issn = {0360-8352},
	url = {http://www.sciencedirect.com/science/article/pii/S0360835211003962},
	doi = {10.1016/j.cie.2011.12.023},
	abstract = {Competition among today’s industrial companies is very high. Therefore, system availability plays an important role and is a critical point for most companies. Detecting failures at an early stage or foreseeing them before they occur is crucial for machinery availability. Data analysis is the most common method for machine health condition monitoring. In this paper we propose a fault-detection system based on data stream prediction, data stream mining, and data stream management system ({DSMS}). Companies that are able to predict and avoid the occurrence of failures have an advantage over their competitors. The literature has shown that data prediction can also reduce the consumption of communication resources in distributed data stream processing. In this paper different data-stream-based linear regression prediction methods have been tested and compared within a newly developed fault detection system. Based on the fault detection system, three {DSM} algorithms outputs are compared to each other and to real data. The three applied and evaluated data stream mining algorithms were: Grid-based classifier, polygon-based method, and one-class support vector machines ({OCSVM}). The results showed that the linear regression method generally achieved good performance in predicting short-term data. (The best achieved performance was with a Mean Absolute Error ({MAE}) around 0.4, representing prediction accuracy of 87.5\%). Not surprisingly, results showed that the classification accuracy was reduced when using the predicted data. However, the fault-detection system was able to attain an acceptable performance of around 89\% classification accuracy when using predicted data.},
	pages = {972--978},
	number = {4},
	journal = {Computers \& Industrial Engineering},
	shortjournal = {Computers \& Industrial Engineering},
	author = {Alzghoul, Ahmad and Löfstrand, Magnus and Backe, Björn},
	urldate = {2013-06-15},
	date = {2012-05},
	%note = {Cited by 0001},
	keywords = {Availability, Data stream management system, Data stream mining, Data stream prediction, Fault detection forecasting, Fault detection system}
}

@book{pressman_engenharia_2011,
	location = {Porto Alegre},
	edition = {7},
	title = {Engenharia de Software: uma abordagem profissional},
	pagetotal = {780},
	publisher = {{AMGH}},
	author = {Pressman, Roger S.},
	date = {2011},
	%note = {Cited by 0003}
}


@inproceedings{xuan_developer_2012,
	title = {Developer prioritization in bug repositories},
	doi = {10.1109/ICSE.2012.6227209},
	abstract = {Developers build all the software artifacts in development. Existing work has studied the social behavior in software repositories. In one of the most important software repositories, a bug repository, developers create and update bug reports to support software development and maintenance. However, no prior work has considered the priorities of developers in bug repositories. In this paper, we address the problem of the developer prioritization, which aims to rank the contributions of developers. We mainly explore two aspects, namely modeling the developer prioritization in a bug repository and assisting predictive tasks with our model. First, we model how to assign the priorities of developers based on a social network technique. Three problems are investigated, including the developer rankings in products, the evolution over time, and the tolerance of noisy comments. Second, we consider leveraging the developer prioritization to improve three predicted tasks in bug repositories, i.e., bug triage, severity identification, and reopened bug prediction. We empirically investigate the performance of our model and its applications in bug repositories of Eclipse and Mozilla. The results indicate that the developer prioritization can provide the knowledge of developer priorities to assist software tasks, especially the task of bug triage.},
	pages = {25--35},
	booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
	author = {Xuan, Jifeng and Jiang, He and Ren, Zhilei and Zou, Weiqin},
	date = {2012},
	%note = {Cited by 0004},
	keywords = {bug reports, bug reports, bug repository, bug repository, bug triage, bug triage, Communities, Communities, Computer bugs, Computer bugs, database management systems, database management systems, developer prioritization, developer prioritization, developer ranking, developer ranking, Eclipse, Eclipse, Internet, Internet, Mozilla, Mozilla, Noise, Noise, Noise measurement, Noise measurement, predictive task, predictive task, program debugging, program debugging, Programming, Programming, reopened bug prediction, reopened bug prediction, severity identification, severity identification, social behavior, social behavior, Social network services, Social network services, social network technique, social network technique, software, software, software development, software development, software evolution, software evolution, software maintenance, software maintenance, software repository, software repository}
}

@thesis{dambros_evolution_2010,
	title = {On the Evolution of Source Code and Software Defects},
	abstract = {Software systems are subject to continuous changes to adapt to new and changing requirements.
This phenomenon, known as software evolution, leads in the long term to software aging: The
size and the complexity of systems increase, while their quality decreases. In this context, it is no
wonder that software maintenance claims the most part of a software system’s cost. The analysis
of software evolution helps practitioners deal with the negative effects of software aging.
With the advent of the Internet and the consequent widespread adoption of distributed de-velopment tools, such as software configuration management and issue tracking systems, a vast
amount of valuable information concerning software evolution has become available. In the
last two decades, researchers have focused on mining and analyzing this data, residing in vari-ous software repositories, to understand software evolution and support maintenance activities.
However, most approaches target a specific maintenance task, and consider only one of the sev-eral facets of software evolution. Such approaches, and the infrastructures that implement them,
cannot be extended to address different maintenance problems.
In this dissertation, we propose an integrated view of software evolution that combines dif-ferent evolutionary aspects. Our thesis is that an integrated and flexible approach supports an
extensible set of software maintenance activities. To this aim, we present a meta-model that in-tegrates two aspects of software evolution: source code and software defects. We implemented
our approach in a framework that, by retrieving information from source code and defect repos-itories, serves as a basis to create analysis techniques and tools. To show the flexibility of our
approach, we extended our meta-model and framework with e-mail information extracted from
development mailing lists.
To validate our thesis, we devised and evaluated, on top of our approach, a number of novel
analysis techniques that achieve two goals:
1. Inferring the causes of problems in a software system. We propose three retrospective
analysis techniques, based on interactive visualizations, to analyze the evolution of source
code, software defects, and their co-evolution. These techniques support various mainte-nance tasks, such as system restructuring, re-documentation, and identification of critical
software components.
2. Predicting the future of a software system. We present four analysis techniques aimed at
anticipating the locations of future defects, and investigating the impact of certain source
code properties on the presence of defects. They support two maintenance tasks: defect
prediction and software quality analysis.
By creating our framework and the mentioned techniques on top of it, we provide evidence
that an integrated view of software evolution, combining source code and software defects in-formation, supports an extensible set of software maintenance tasks.},
	institution = {Università della Svizzera Italiana},
	type = {phdthesis},
	author = {D’Ambros, Marco},
	date = {2010},
	%note = {Cited by 0000},
	file = {dambros-phd-thesis.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8N3P8M3M\\dambros-phd-thesis.pdf:application/pdf;phdthesissmall-101019044526-phpapp01.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\TRGN77TX\\phdthesissmall-101019044526-phpapp01.pdf:application/pdf}
}

@inproceedings{di_lucca_approach_2002,
	title = {An approach to classify software maintenance requests},
	doi = {10.1109/ICSM.2002.1167756},
	abstract = {When a software system critical for an organization exhibits a problem during its operation, it is relevant to fix it in a short period of time, to avoid serious economical losses. The problem is therefore noticed by the organization in charge of the maintenance, and it should be correctly and quickly dispatched to the right maintenance team. We propose to automatically classify incoming maintenance requests (also said tickets), routing them to specialized maintenance teams. The final goal is to develop a router working around the clock, that, without human intervention, dispatches incoming tickets with the lowest misclassification error, measured with respect to a given routing policy. 6000 maintenance tickets from a large, multi-site, software system, spanning about two years of system in-field operation, were used to compare and assess the accuracy of different classification approaches (i.e., Vector Space model, Bayesian model, support vectors, classification trees and k-nearest neighbor classification). The application and the tickets were divided into eight areas and pre-classified by human experts. Preliminary results were encouraging, up to 84\% of the incoming tickets were correctly classified.},
	pages = {93--102},
	booktitle = {International Conference on Software Maintenance, 2002. Proceedings},
	author = {Di Lucca, G.A. and Di Penta, M. and Gradara, S.},
	date = {2002},
	%note = {Cited by 0060},
	keywords = {Application software, Application software, Bayesian methods, Bayesian methods, Bayesian model, Bayesian model, Bayes methods, Bayes methods, Classification, Classification, Classification tree analysis, Classification tree analysis, classification trees, classification trees, Clocks, Clocks, economical loss, economical loss, Extraterrestrial measurements, Extraterrestrial measurements, Humans, Humans, k-nearest neighbor classification, k-nearest neighbor classification, learning (artificial intelligence), learning (artificial intelligence), learning automata, learning automata, machine learning, machine learning, maintenance request classification, maintenance request classification, multi-site software system, multi-site software system, organization, organization, program debugging, program debugging, Routing, Routing, software development management, software development management, software maintenance, software maintenance, software maintenance request classification, software maintenance request classification, Software measurement, Software measurement, Software systems, Software systems, support vector machine, support vector machine, Vector Space model, Vector Space model}
}

@inproceedings{rahman_bugcache_2011,
	location = {New York, {NY}, {USA}},
	title = {{BugCache} for inspections: hit or miss?},
	isbn = {978-1-4503-0443-6},
	url = {http://doi.acm.org/10.1145/2025113.2025157},
	doi = {10.1145/2025113.2025157},
	series = {{ESEC}/{FSE} '11},
	shorttitle = {{BugCache} for inspections},
	abstract = {Inspection is a highly effective but costly technique for quality control. Most companies do not have the resources to inspect all the code; thus accurate defect prediction can help focus available inspection resources. {BugCache} is a simple, elegant, award-winning prediction scheme that "caches" files that are likely to contain defects [12]. In this paper, we evaluate the utility of {BugCache} as a tool for focusing inspection, we examine the assumptions underlying {BugCache} with the aim of improving it, and finally we compare it with a simple, standard bug-prediction technique. We find that {BugCache} is, in fact, useful for focusing inspection effort; but surprisingly, we find that its performance, when used for inspections, is not much better than a naive prediction model – viz., a model that orders files in the system by their count of closed bugs and chooses enough files to capture 20\% of the lines in the system.},
	pages = {322â€“331},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering},
	publisher = {{ACM}},
	author = {Rahman, Foyzur and Posnett, Daryl and Hindle, Abram and Barr, Earl and Devanbu, Premkumar},
	urldate = {2013-08-13},
	date = {2011},
	%note = {Cited by 0009},
	keywords = {empirical software engineering, empirical software engineering, Fault prediction, Fault prediction, inspection, inspection}
}

@article{osullivan_making_2009,
	title = {Making sense of revision-control systems},
	volume = {52},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/1562164.1562183},
	doi = {10.1145/1562164.1562183},
	pages = {56â€“62},
	number = {9},
	journal = {Commun. {ACM}},
	author = {O'Sullivan, Bryan},
	urldate = {2012-05-31},
	date = {2009-09},
	%note = {Cited by 0041}
}

@article{kagdi_survey_2007,
	title = {A survey and taxonomy of approaches for mining software repositories in the context of software evolution},
	volume = {19},
	rights = {Copyright © 2007 John Wiley \& Sons, Ltd.},
	issn = {1532-0618},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.344/abstract},
	doi = {10.1002/smr.344},
	abstract = {A comprehensive literature survey on approaches for mining software repositories ({MSR}) in the context of software evolution is presented. In particular, this survey deals with those investigations that examine multiple versions of software artifacts or other temporal information. A taxonomy is derived from the analysis of this literature and presents the work via four dimensions: the type of software repositories mined (what), the purpose (why), the adopted/invented methodology used (how), and the evaluation method (quality). The taxonomy is demonstrated to be expressive (i.e., capable of representing a wide spectrum of {MSR} investigations) and effective (i.e., facilitates similarities and comparisons of {MSR} investigations). Lastly, a number of open research issues in {MSR} that require further investigation are identified. Copyright © 2007 John Wiley \& Sons, Ltd.},
	pages = {77--131},
	number = {2},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Kagdi, Huzefa and Collard, Michael L. and Maletic, Jonathan I.},
	urldate = {2013-02-01},
	date = {2007},
	langid = {english},
	%note = {Cited by 0181},
	keywords = {mining software repositories, mining software repositories, multi-version analysis, multi-version analysis, software evolution, software evolution},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\AJH37U3E\\Kagdi et al. - 2007 - A survey and taxonomy of approaches for mining sof.pdf:application/pdf}
}

@article{shivaji_reducing_2013,
	title = {Reducing Features to Improve Code Change-Based Bug Prediction},
	volume = {39},
	issn = {0098-5589},
	doi = {10.1109/TSE.2012.43},
	abstract = {Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine ({SVM}) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The {SVM}'s improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.},
	pages = {552--569},
	number = {4},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Shivaji, S. and Whitehead, E.J. and Akella, R. and Kim, Sunghun},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {belief networks, belief networks, buggy F-measure, buggy F-measure, Bug prediction, Bug prediction, classification performance, classification performance, classifier-based bug prediction, classifier-based bug prediction, code change-based bug prediction, code change-based bug prediction, Computer bugs, Computer bugs, feature extraction, feature extraction, Feature Selection, Feature Selection, feature selection technique, feature selection technique, History, History, learning (artificial intelligence), learning (artificial intelligence), machine learned feature reduction, machine learned feature reduction, machine learning, machine learning, machine learning classifier, machine learning classifier, Measurement, Measurement, Naive Bayes classifier, Naive Bayes classifier, pattern classification, pattern classification, program debugging, program debugging, Reliability, Reliability, software, software, software history, software history, software project, software project, source code file, source code file, support vector machine, support vector machine, support vector machines, support vector machines, {SVM} classifier, {SVM} classifier}
}

@inproceedings{li_mining_2011,
	title = {Mining Static Code Metrics for a Robust Prediction of Software Defect-Proneness},
	doi = {10.1109/ESEM.2011.29},
	abstract = {Defect-proneness prediction is affected by multiple aspects including sampling bias, non-metric factors, uncertainty of models etc. These aspects often contribute to prediction uncertainty and result in variance of prediction. This paper proposes two methods of data mining static code metrics to enhance defect-proneness prediction. Given little non-metric or qualitative information extracted from software codes, we first suggest to use a robust unsupervised learning method, shared nearest neighbors ({SNN}) to extract the similarity patterns of the code metrics. These patterns indicate similar characteristics of the components of the same cluster that may result in introduction of similar defects. Using the similarity patterns with code metrics as predictors, defect-proneness prediction may be improved. The second method uses the Occam's windows and Bayesian model averaging to deal with model uncertainty: first, the datasets are used to train and cross-validate multiple learners and then highly qualified models are selected and integrated into a robust prediction. From a study based on 12 datasets from {NASA}, we conclude that our proposed solutions can contribute to a better defect-proneness prediction.},
	pages = {207--214},
	booktitle = {2011 International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Li, Lianfa and Leung, H.},
	date = {2011},
	%note = {Cited by 0004},
	keywords = {Bayesian model, Bayesian model, Bayes methods, Bayes methods, Clustering algorithms, Clustering algorithms, data mining, data mining, defect-proneness, defect-proneness, Measurement, Measurement, mining static code metrics, mining static code metrics, nonmetric information, nonmetric information, Occam windows, Occam windows, Predictive models, Predictive models, qualitative information, qualitative information, Robustness, Robustness, robust prediction, robust prediction, shared nearest neighbors, shared nearest neighbors, {SNN}, {SNN}, software codes, software codes, software defect proneness, software defect proneness, Software metrics, Software metrics, software quality, software quality, Uncertainty, Uncertainty, unsupervised learning, unsupervised learning, unsupervised learning method, unsupervised learning method}
}

@inproceedings{moser_comparative_2008,
	title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
	doi = {10.1145/1368088.1368114},
	abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: {\textgreater}75\% percentage of correctly classified files, a recall of {\textgreater}80\%, and a false positive rate {\textless}30\%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
	eventtitle = {{ACM}/{IEEE} 30th International Conference on Software Engineering, 2008. {ICSE} '08},
	pages = {181--190},
	booktitle = {{ACM}/{IEEE} 30th International Conference on Software Engineering, 2008. {ICSE} '08},
	author = {Moser, R. and Pedrycz, W. and Succi, G.},
	date = {2008},
	%note = {Cited by 0147},
	keywords = {Bayes methods, Bayes methods, change metrics, change metrics, Classification tree analysis, Classification tree analysis, comparative analysis, comparative analysis, Costs, Costs, cost-sensitive classification, cost-sensitive classification, decision tree, decision tree, decision trees, decision trees, defect prediction, defect prediction, Eclipse project, Eclipse project, Java, Java, Java file classification, Java file classification, learning (artificial intelligence), learning (artificial intelligence), logistic regression, logistic regression, Logistics, Logistics, machine learning, machine learning, naive Bayes method, naive Bayes method, pattern classification, pattern classification, Permission, Permission, Predictive models, Predictive models, process related software metrics, process related software metrics, product related software metrics, product related software metrics, program diagnostics, program diagnostics, regression analysis, regression analysis, Resource management, Resource management, software engineering, software engineering, Software metrics, Software metrics, software quality, software quality, static code attribute, static code attribute, Testing, Testing},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\RXANBIZJ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4G9CKRXR\\Moser et al. - 2008 - A comparative analysis of the efficiency of change.pdf:application/pdf}
}

@book{_eclipse_????,
	title = {Eclipse Code Recommenders},
	url = {http://www.eclipse.org/recommenders/documentation/usr.html},
	abstract = {Eclipse is probably best known as a Java {IDE}, but it is more: it is an {IDE} framework, a tools framework, an open source project, a community, an eco-system, and a foundation.},
	urldate = {2012-07-16},
	%note = {Cited by 0000},
	keywords = {code recommenders, collective intelligence, ide 2.0}
}

@online{_c:gnuoutbnetcore30_b.eps_????,
	title = {c:{\textbackslash}gnu{\textbackslash}out{\textbackslash}{BNET}{\textbackslash}core30\_b.eps - a12-aljarah.pdf},
	url = {http://delivery.acm.org/10.1145/2030000/2020402/a12-aljarah.pdf?ip=200.129.173.3&id=2020402&acc=ACTIVE%20SERVICE&key=C2716FEBFA981EF14D39E795746BF179BD02EBA2BB1F340F&CFID=255439464&CFTOKEN=29465794&__acm__=1382376959_3fe6056b1247ae6d9bc0b52237188932},
	urldate = {2013-10-21},
	%note = {Cited by 0000}
}

@inproceedings{lucia_are_2012,
	title = {Are faults localizable?},
	doi = {10.1109/MSR.2012.6224302},
	abstract = {Many fault localization techniques have been proposed to facilitate debugging activities. Most of them attempt to pinpoint the location of faults (i.e., localize faults) based on a set of failing and correct executions and expect debuggers to investigate a certain number of located program elements to find faults. These techniques thus assume that faults are localizable, i.e., only one or a few lines of code that are close to one another are responsible for each fault. However, in reality, are faults localizable? In this work, we investigate hundreds of real faults in several software systems, and find that many faults may not be localizable to a few lines of code and these include faults with high severity level.},
	pages = {74--77},
	booktitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {Lucia and Thung, F. and Lo, D. and Jiang, Lingxiao},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {bug severity, bug severity, Computer bugs, Computer bugs, Debugging, Debugging, debugging activities, debugging activities, Educational institutions, Educational institutions, Fault Locality, Fault Locality, Fault Localization, Fault Localization, fault localization technique, fault localization technique, Java, Java, Manuals, Manuals, program debugging, program debugging, software fault tolerance, software fault tolerance, software system, software system, Software systems, Software systems}
}

@article{malhi_pca-based_2004,
	title = {{PCA}-based feature selection scheme for machine defect classification},
	volume = {53},
	issn = {0018-9456},
	doi = {10.1109/TIM.2004.834070},
	abstract = {The sensitivity of various features that are characteristic of a machine defect may vary considerably under different operating conditions. Hence it is critical to devise a systematic feature selection scheme that provides guidance on choosing the most representative features for defect classification. This paper presents a feature selection scheme based on the principal component analysis ({PCA}) method. The effectiveness of the scheme was verified experimentally on a bearing test bed, using both supervised and unsupervised defect classification approaches. The objective of the study was to identify the severity level of bearing defects, where no a priori knowledge on the defect conditions was available. The proposed scheme has shown to provide more accurate defect classification with fewer feature inputs than using all features initially considered relevant. The result confirms its utility as an effective tool for machine health assessment.},
	pages = {1517--1525},
	number = {6},
	journal = {{IEEE} Transactions on Instrumentation and Measurement},
	author = {Malhi, A. and Gao, R.X.},
	date = {2004},
	%note = {Cited by 0142},
	keywords = {65, 65, bearing defects, bearing defects, Computerized monitoring, Computerized monitoring, Condition monitoring, Condition monitoring, Costs, Costs, decision making, decision making, Defect classification, Defect classification, fault diagnosis, fault diagnosis, Feature Selection, Feature Selection, machine bearings, machine bearings, machine defect classification, machine defect classification, machine health assessment, machine health assessment, Manufacturing processes, Manufacturing processes, mechanical engineering computing, mechanical engineering computing, neural nets, neural nets, neural networks, neural networks, {PCA}, {PCA}, principal component analysis, principal component analysis, production, production, signal classification, signal classification, Testing, Testing, vibrations, vibrations}
}

@inproceedings{vasilescu_historical_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {A historical dataset of software engineering conferences},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org.ez24.periodicos.capes.gov.br/citation.cfm?id=2487085.2487153},
	series = {{MSR} '13},
	abstract = {The Mining Software Repositories community typically focuses on data from software configuration management tools, mailing lists, and bug tracking repositories to uncover interesting and actionable information about the evolution of software systems. However, the techniques employed and the challenges faced when mining are not restricted to these types of repositories. In this paper, we present an atypical dataset of software engineering conferences, containing historical data about the accepted papers and the composition of programme committees for eleven well-established conferences. The dataset (published on Github at https://github.com/tue-mdse/{conferenceMetrics}) can be used, e.g., by conference steering committees or programme committee chairs to assess their selection process and compare against other conferences in the field, or by prospective authors to decide in which conferences to publish.},
	pages = {373--376},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Vasilescu, Bogdan and Serebrenik, Alexander and Mens, Tom},
	urldate = {2013-06-21},
	date = {2013},
	%note = {Cited by 0000},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\76G3NEGT\\Vasilescu et al. - 2013 - A historical dataset of software engineering confe.pdf:application/pdf}
}

@article{catal_unlabelled_2009,
	title = {Unlabelled extra data do not always mean extra performance for semi-supervised fault prediction},
	volume = {26},
	rights = {Â© 2009 The Authors. Journal Compilation Â© 2009 Blackwell Publishing Ltd},
	issn = {1468-0394},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0394.2009.00509.x/abstract},
	doi = {10.1111/j.1468-0394.2009.00509.x},
	abstract = {Abstract: This research focused on investigating and benchmarking several high performance classifiers called J48, random forests, naive Bayes, {KStar} and artificial immune recognition systems for software fault prediction with limited fault data. We also studied a recent semi-supervised classification algorithm called {YATSI} (Yet Another Two Stage Idea) and each classifier has been used in the first stage of {YATSI}. {YATSI} is a meta algorithm which allows different classifiers to be applied in the first stage. Furthermore, we proposed a semi-supervised classification algorithm which applies the artificial immune systems paradigm. Experimental results showed that {YATSI} does not always improve the performance of naive Bayes when unlabelled data are used together with labelled data. According to experiments we performed, the naive Bayes algorithm is the best choice to build a semi-supervised fault prediction model for small data sets and {YATSI} may improve the performance of naive Bayes for large data sets. In addition, the {YATSI} algorithm improved the performance of all the classifiers except naive Bayes on all the data sets.},
	pages = {458â€“471},
	number = {5},
	journal = {Expert Systems},
	author = {Catal, Cagatay and Diri, Banu},
	urldate = {2013-07-10},
	date = {2009},
	langid = {english},
	%note = {Cited by 0007},
	keywords = {artificial immune systems, artificial immune systems, Classification algorithms, Classification algorithms, machine learning, machine learning, Naive Bayes, Naive Bayes, random forests, random forests, semi-supervised classification, semi-supervised classification, Software fault prediction, Software fault prediction, {YATSI}, {YATSI}}
}

@inproceedings{biebler_data_2005,
	title = {Data Mining and Metrics on Data Sets},
	volume = {1},
	doi = {10.1109/CIMCA.2005.1631335},
	abstract = {One needs suitable metrics for topologically based methods of the data analysis. Examples of distances and metrics are given. A general method for the generation of metrics is developed. Its usefulness is demonstrated in support of classification methods for mixed type data},
	pages = {638--641},
	booktitle = {International Conference on Computational Intelligence for Modelling, Control and Automation, 2005 and International Conference on Intelligent Agents, Web Technologies and Internet Commerce},
	author = {Biebler, K.-E. and Wodny, M. and Jager, B.},
	date = {2005},
	%note = {Cited by 0002},
	keywords = {Automation, Automation, Biomedical informatics, Biomedical informatics, Computational intelligence, Computational intelligence, Computational modeling, Computational modeling, Covariance matrix, Covariance matrix, data analysis, data analysis, data classification method, data classification method, data mining, data mining, data set metrics, data set metrics, Euclidean distance, Euclidean distance, pattern classification, pattern classification, Random variables, Random variables, statistical analysis, statistical analysis}
}

@article{ghedini_ralha_multi-agent_2012,
	title = {A multi-agent data mining system for cartel detection in Brazilian government procurement},
	volume = {39},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417412006343},
	doi = {10.1016/j.eswa.2012.04.037},
	abstract = {The main focus of this research project is the problem of extracting useful information from the Brazilian federal procurement process databases used by government auditors in the process of corruption detection and prevention to identify cartel formation among applicants. Extracting useful information to enhance cartel detection is a complex problem from many perspectives due to the large volume of data used to correlate information and the dynamic and diversified strategies companies use to hide their fraudulent operations. To attack the problem of data volume, we have used two data mining model functions, clustering and association rules, and a multi-agent approach to address the dynamic strategies of companies that are involved in cartel formation. To integrate both solutions, we have developed {AGMI}, an agent-mining tool that was validated using real data from the Brazilian Office of the Comptroller General, an institution of government auditing, where several measures are currently used to prevent and fight corruption. Our approach resulted in explicit knowledge discovery because {AGMI} presented many association rules that provided a 90\% correct identification of cartel formation, according to expert assessment. According to auditing specialists, the extracted knowledge could help in the detection, prevention and monitoring of cartels that act in public procurement processes.},
	pages = {11642--11656},
	number = {14},
	journal = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Ghedini Ralha, Célia and Sarmento Silva, Carlos Vinícius},
	urldate = {2012-12-11},
	date = {2012-10-15},
	%note = {Cited by 0000},
	keywords = {{AGMI}, {AGMI}, Brazilian government procurement, Brazilian government procurement, Cartel detection, Cartel detection, Database knowledge discovery, Database knowledge discovery, distributed data mining, distributed data mining, Multi-agent, Multi-agent, Multi-agent data mining system, Multi-agent data mining system}
}

@online{_minerall:_????,
	title = {{MinerAll}: Uma ferramenta para extração e mineração de dados de repositórios de software livre},
	url = {http://www.academia.edu/955453/MinerAll_Uma_ferramenta_para_extracao_e_mineracao_de_dados_de_repositorios_de_software_livre},
	shorttitle = {{MinerAll}},
	abstract = {{MinerAll}: Uma ferramenta para extração e mineração de dados de repositórios de software livre},
	urldate = {2013-05-21},
	%note = {Cited by 0000},
	keywords = {academia, academics, Biology, Chemistry, Computer Science, Earth Sciences, Economics, English, Geography, History, Law, Math, Medicine, Philosophy, Physics, Political Science, Psychology, Religion, research, universities},
	file = {Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\HQ2V9ST2\\MinerAll_Uma_ferramenta_para_extracao_e_mineracao_de_dados_de_repositorios_de_software_livre.html:text/html}
}

@inproceedings{ohira_impact_2012,
	title = {The impact of bug management patterns on bug fixing: A case study of Eclipse projects},
	doi = {10.1109/ICSM.2012.6405281},
	shorttitle = {The impact of bug management patterns on bug fixing},
	abstract = {An efficient bug management process is critical for the success of software projects. Prior work has focused on improving this process, for example, by automating bug triaging, detecting duplicate bugs, and understanding the rationale for re-opening bugs. This paper continues this line of work by exploring the people who are involved in the bug management process. In particular we develop four patterns that distill the different relations between the people involved in the process: the reporter, triager, and fixer of a bug. Through a case study on the Eclipse Platform and {JDT} projects, we demonstrate that these patterns have an impact on the efficiency of the bug management process. For example, we find that using our patterns project personnel can improve their efficiency through better communication about bugs before assigning them.},
	eventtitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
	pages = {264--273},
	booktitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Ohira, M. and Hassan, A.E. and Osawa, N. and Matsumoto, K.},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {bug fixer, bug fixing, bug management pattern, bug management process, bug reopening, bug reporter, bug triager, bug triaging, Computer bugs, conferences, duplicate bugs detection, Eclipse project, Educational institutions, {JDT} project, personnel, program debugging, software maintenance, software project, time measurement},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XCAJIM34\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\MMZXSVGV\\Ohira et al. - 2012 - The impact of bug management patterns on bug fixin.pdf:application/pdf}
}

@inproceedings{sheoran_understanding_2014,
	location = {New York, {NY}, {USA}},
	title = {Understanding "Watchers" on {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597114},
	doi = {10.1145/2597073.2597114},
	series = {{MSR} 2014},
	abstract = {Users on {GitHub} can watch repositories to receive notifications about project activity. This introduces a new type of passive project membership. In this paper, we investigate the behavior of watchers and their contribution to the projects they watch. We find that a subset of project watchers begin contributing to the project and those contributors account for a significant percentage of contributors on the project. As contributors, watchers are more confident and contribute over a longer period of time in a more varied way than other contributors. This is likely attributable to the knowledge gained through project notifications.},
	pages = {336--339},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Sheoran, Jyoti and Blincoe, Kelly and Kalliamvakou, Eirini and Damian, Daniela and Ell, Jordan},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {{GitHub}, Repositories, Software Teams, Watchers},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\33KQZ5RA\\Sheoran et al. - 2014 - Understanding Watchers on GitHub.pdf:application/pdf}
}

@collection{poncelet_data_2007,
	title = {Data Mining Patterns},
	isbn = {9781599041629, 9781599041643},
	url = {http://www.igi-global.com/chapter/metric-methods-data-mining/7558},
	publisher = {{IGI} Global},
	editor = {Poncelet, Pascal and Masseglia, Florent and Teisseire, Maguelonne},
	urldate = {2013-04-09},
	date = {2007-08-31},
	%note = {Cited by 0017},
	file = {Untitled Attachment:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\QQ3TIREX\\Poncelet et al. - 2007 - Data Mining Patterns.html:text/html}
}

@inproceedings{aljarah_selecting_2011,
	location = {New York, {NY}, {USA}},
	title = {Selecting discriminating terms for bug assignment: a formal analysis},
	isbn = {978-1-4503-0709-3},
	url = {http://doi.acm.org/10.1145/2020390.2020402},
	doi = {10.1145/2020390.2020402},
	series = {Promise '11},
	shorttitle = {Selecting discriminating terms for bug assignment},
	abstract = {Background. The bug assignment problem is the problem of triaging new bug reports to the most qualified developer. The qualified developer is the one who has enough knowledge in a specific area that is relevant to the reported bug. In recent years, bug triaging has received a considerable amount of attention from researchers. In previous work, bugs were represented as vectors of terms extracted from the bug reports' description. Once the bugs are represented as vectors in the terms space, traditional machine learning techniques are employed for the bug assignment. Most of the previous algorithms are marred by low accuracy values. Aims. This paper formulates the bug assignment problem as a classification task, and then examines the impact of several term selection approaches on the classification effectiveness. Method. Three variants selection methods that are based on the Log Odds Ratio ({LOR}) score are compared against methods that are based on the Information Gain ({IG}) score and Latent Semantic Analysis ({LSA}). The main difference in the methods that are based on the {LOR} score is in the process of selecting the terms. Results. Term selection techniques that are based on the Log Odds Ratio achieved up to 30\% improvement in the precision and up to 5\% higher in recall compared to other term selection methods such as Latent Semantic Analysis and Information Gain. Conclusions. Experimental results showed that the effectiveness of bug assignment methods is directly affected by the selected terms that are used in the classification methods.},
	pages = {12:1--12:7},
	booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
	publisher = {{ACM}},
	author = {Aljarah, Ibrahim and Banitaan, Shadi and Abufardeh, Sameer and Jin, Wei and Salem, Saeed},
	urldate = {2013-10-21},
	date = {2011},
	%note = {Cited by 0003},
	keywords = {Bug assignment, bug reports, Classification, machine learning}
}

@article{dash_feature_1997,
	title = {Feature selection for classification},
	volume = {1},
	issn = {1088-467X},
	url = {http://www.sciencedirect.com/science/article/pii/S1088467X97000085},
	doi = {10.1016/S1088-467X(97)00008-5},
	abstract = {Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.},
	pages = {131--156},
	number = {1},
	journal = {Intelligent Data Analysis},
	author = {Dash, M. and Liu, H.},
	urldate = {2013-07-24},
	date = {1997},
	%note = {Cited by 1508},
	keywords = {Classification, Classification, Feature Selection, Feature Selection, Framework, Framework}
}

@article{kotsiantis_machine_2006,
	title = {Machine learning: a review of classification and combining techniques},
	volume = {26},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/article/10.1007/s10462-007-9052-3},
	doi = {10.1007/s10462-007-9052-3},
	shorttitle = {Machine learning},
	abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy—ensembles of classifiers.},
	pages = {159--190},
	number = {3},
	journal = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.},
	urldate = {2013-07-05},
	date = {2006-11-01},
	langid = {english},
	%note = {Cited by 0080},
	keywords = {Artificial Intelligence (incl. Robotics), Classifiers, Complexity, Computer Science, general, data mining techniques, Intelligent Data Analysis, Learning Algorithms},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CKRVPQT6\\Kotsiantis et al. - 2006 - Machine learning a review of classification and c.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ST98GZ33\\10.html:text/html}
}

@inproceedings{liu_integrating_1998,
	title = {Integrating Classification and Association Rule Mining},
	abstract = {Classification rule mining aims to discover a small set of rules in the database that forms an accurate classifier. Association rule mining finds all the rules existing in the database that satisfy some minimum support and minimum confidence constraints. For association rule mining, the target of discovery is not pre-determined, while for classification rule mining there is one and only one predetermined target. In this paper, we propose to integrate these two mining techniques. The integration is done by focusing on mining a special subset of association rules, called class association rules ({CARs}). An efficient algorithm is also given for building a classifier based on the set of discovered {CARs}. Experimental results show that the classifier built this way is, in general, more accurate than that produced by the state-of-the-art classification system C4.5. In addition, this integration helps to solve a number of problems that exist in the current classification systems. Introduction ...},
	pages = {80--86},
	author = {Liu, Bing and Hsu, Wynne and Ma, Yiming},
	date = {1998},
	%note = {Cited by 1763}
}

@inproceedings{soetens_studying_2010,
	title = {Studying the Effect of Refactorings: A Complexity Metrics Perspective},
	doi = {10.1109/QUATIC.2010.58},
	shorttitle = {Studying the Effect of Refactorings},
	abstract = {Refactoring is widely recognized as a way to improve the internal structure of a software system in order to ensure its long-term maintainability. Consequently, software projects which adopt refactoring practices should see reductions in the complexity of their code base. We evaluated this assumption on an open source system-namely {PMD}, a Java source code analyzer-and discovered that periods of refactorings did not affect the cyclomatic complexity. This paper investigates this counterintuitive phenomenon through a detailed analysis of the actual source code manipulations applied on the system under study.},
	eventtitle = {Quality of Information and Communications Technology ({QUATIC}), 2010 Seventh International Conference on the},
	pages = {313 --318},
	author = {Soetens, Q.D. and Demeyer, S.},
	date = {2010-10-29},
	%note = {Cited by 0003},
	keywords = {Complexity, complexity metrics perspective, Complexity theory, Couplings, cyclomatic complexity, internal structure, Java, Java source code analyzer, long-term maintainability, Maintenance, Mathematical analysis, Measurement, mining software repositories, open source system, public domain software, Refactoring, refactorings, software evolution, software maintenance, software management, Software metrics, software projects, software system, Software systems, source coding}
}

@article{garcia_advanced_2010,
	title = {Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power},
	volume = {180},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025509005404},
	doi = {10.1016/j.ins.2009.12.010},
	series = {Special Issue on Intelligent Distributed Information Systems},
	shorttitle = {Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining},
	abstract = {Experimental analysis of the performance of a proposed method is a crucial and necessary task in an investigation. In this paper, we focus on the use of nonparametric statistical inference for analyzing the results obtained in an experiment design in the field of computational intelligence. We present a case study which involves a set of techniques in classification tasks and we study a set of nonparametric procedures useful to analyze the behavior of a method with respect to a set of algorithms, such as the framework in which a new proposal is developed.

Particularly, we discuss some basic and advanced nonparametric approaches which improve the results offered by the Friedman test in some circumstances. A set of post hoc procedures for multiple comparisons is presented together with the computation of adjusted p-values. We also perform an experimental analysis for comparing their power, with the objective of detecting the advantages and disadvantages of the statistical tests described. We found that some aspects such as the number of algorithms, number of data sets and differences in performance offered by the control method are very influential in the statistical tests studied. Our final goal is to offer a complete guideline for the use of nonparametric statistical procedures for performing multiple comparisons in experimental studies.},
	pages = {2044--2064},
	number = {10},
	journal = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {García, Salvador and Fernández, Alberto and Luengo, Julián and Herrera, Francisco},
	urldate = {2014-08-14},
	date = {2010-05-15},
	keywords = {Computational intelligence, data mining, Fuzzy classification systems, Genetics-based machine learning, Multiple comparisons procedures, Nonparametric statistics, statistical analysis},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\D5CZ7U4N\\García et al. - 2010 - Advanced nonparametric tests for multiple comparis.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\5VTACHAW\\S0020025509005404.html:text/html}
}

@thesis{shivaji_efficient_2013,
	title = {Efficient Bug Prediction and Fix Suggestions},
	url = {http://escholarship.org/uc/item/47x1t79s#page-1},
	abstract = {Bugs are a well known Achilles' heel of software development. In the last few years, machine learning techniques to combat software bugs have become popular. However, results of these techniques are not good enough for practical adoption. In addition, most techniques do not provide reasons for why a code change is a bug. Furthermore, suggestions to fix the bug would be greatly beneficial. An added bonus would be engaging humans to improve the bug and fix prediction process.In this dissertation, a step-by-step procedure which effectively predicts buggy code changes (Bug Prognosticator), produces bug fix suggestions (Fix Suggester), and utilizes human feedback is presented. Each of these steps can be used independently, but combining them allows more effective management of bugs. These techniques are tested on many open source and a large commercial project. Human feedback was used to understand and improve the performance of the techniques. Feedback was primarily gathered from industry participants in order to assess practical suitability.The Bug Prognosticator explores feature selection techniques and classifiers to improve results of code change bug prediction. The optimized Bug Prognosticator is able to achieve an average 97\% precision and 70\% recall when evaluated on eleven projects, ten open source and one commercial.The Fix Suggester uses the Bug Prognosticator and statistical analysis of keyword term frequencies to suggest unordered fix keywords to a code change predicted to be buggy. The suggestions are validated against actual bug fixes to confirm their utility. The Fix Suggester is able to achieve 46.9\% precision and 38.9\% recall on its predicted fix tokens. This is a reasonable start to the difficult problem of predicting the contents of a bug fix.To improve the efficiency of the Bug Prognosticator and the Fix Suggester, active learning is employed on willing human participants. Developers aid the Bug Prognosticator and the Fix Suggester on code changes that machines find hard to evaluate. The developer's feedback is used to enhance the performance of the Bug Prognosticator and the Fix Suggester. In addition, a user study is performed to gauge the utility of the Fix Suggester.The dissertation concludes with a discussion of future work and challenges faced by the techniques. Given the success of statistical defect prediction techniques, more industrial exposure would benefit researchers and software practitioners.},
	type = {phdthesis},
	author = {Shivaji, Shivkumar},
	urldate = {2013-07-20},
	date = {2013-01-01},
	%note = {Cited by 0000},
	keywords = {bug, bug fix, Bug prediction, Computer Science, machine learning, statistical},
	file = {eScholarship UC item 47x1t79s.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ZMPHBNFI\\eScholarship UC item 47x1t79s.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\JAB8N5AG\\47x1t79s.html:text/html}
}

@inproceedings{denaro_deriving_2002,
	location = {New York, {NY}, {USA}},
	title = {Deriving models of software fault-proneness},
	isbn = {1-58113-556-4},
	url = {http://doi.acm.org/10.1145/568760.568824},
	doi = {10.1145/568760.568824},
	series = {{SEKE} '02},
	abstract = {The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality.By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of homogeneous software products. It also proposes the use of cross-validation for selecting valid models even for small data sets.The early results show that it is possible to build statistical models based on historical data for estimating fault-proneness of software modules before testing, and thus better planning and monitoring the testing activities.},
	pages = {361--368},
	booktitle = {Proceedings of the 14th international conference on Software engineering and knowledge engineering},
	publisher = {{ACM}},
	author = {Denaro, Giovanni and Morasca, Sandro and Pezzè, Mauro},
	urldate = {2013-03-28},
	date = {2002},
	%note = {Cited by 0041},
	keywords = {cross-validation, fault-proneness models, logistic regression, software faultiness, Software metrics, software testing process},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\USR2ZDVG\\Denaro et al. - 2002 - Deriving models of software fault-proneness.pdf:application/pdf}
}

@online{center_for_history_and_new_media_guia_????,
	title = {Guia de Início Rápido},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@inproceedings{aggarwal_co-evolution_2014,
	location = {New York, {NY}, {USA}},
	title = {Co-evolution of Project Documentation and Popularity Within Github},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597120},
	doi = {10.1145/2597073.2597120},
	series = {{MSR} 2014},
	abstract = {Github is a very popular collaborative software-development platform that provides typical source-code management and issue tracking features augmented by strong social-networking features such as following developers and watching projects. These features help ``spread the word'' about individuals and projects, building the reputation of the former and increasing the popularity of the latter. In this paper, we investigate the relation between project popularity and regular, consistent documentation updates. We found strong indicators that consistently popular projects exhibited consistent documentation effort and that this effort tended to attract more documentation collaborators. We also found that frameworks required more documentation effort than libraries to achieve similar adoption success, especially in the initial phase.},
	pages = {360--363},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Aggarwal, Karan and Hindle, Abram and Stroulia, Eleni},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Cross Correlation, Documentation Change, Popularity},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\NC2DCURT\\Aggarwal et al. - 2014 - Co-evolution of Project Documentation and Populari.pdf:application/pdf}
}

@book{_monografia.pdf_????,
	title = {monografia.pdf},
	url = {http://bcc.ime.usp.br/principal/tccs/2012/chico/arquivos/monografia.pdf},
	urldate = {2013-05-21},
	%note = {Cited by 0000}
}

@inproceedings{kobayashi_impactscale:_2011,
	title = {{ImpactScale}: Quantifying change impact to predict faults in large software systems},
	doi = {10.1109/ICSM.2011.6080771},
	shorttitle = {{ImpactScale}},
	abstract = {In software maintenance, both product metrics and process metrics are required to predict faults effectively. However, process metrics cannot be always collected in practical situations. To enable accurate fault prediction without process metrics, we define a new metric, {ImpactScale}. {ImpactScale} is the quantified value of change impact, and the change propagation model for {ImpactScale} is characterized by probabilistic propagation and relation-sensitive propagation. To evaluate {ImpactScale}, we predicted faults in two large enterprise systems using the effort-aware models and Poisson regression. The results showed that adding {ImpactScale} to existing product metrics increased the number of detected faults at 10\% effort ({LOC}) by over 50\%. {ImpactScale} also improved the predicting model using existing product metrics and dependency network measures.},
	pages = {43--52},
	booktitle = {2011 27th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Kobayashi, K. and Matsuo, A. and Inoue, K. and Hayase, Y. and Kamimura, M. and Yoshino, T.},
	date = {2011},
	%note = {Cited by 0001},
	keywords = {change propagation model, change propagation model, Couplings, Couplings, dependency network measures, dependency network measures, effort aware models, effort aware models, {ImpactScale}, {ImpactScale}, Maintenance engineering, Maintenance engineering, Measurement, Measurement, Poisson regression, Poisson regression, Predictive models, Predictive models, Probabilistic logic, Probabilistic logic, probabilistic propagation, probabilistic propagation, probability, probability, process metrics, process metrics, product metrics, product metrics, regression analysis, regression analysis, relation sensitive propagation, relation sensitive propagation, Software fault prediction, Software fault prediction, software fault tolerance, software fault tolerance, software maintenance, software maintenance, Software metrics, Software metrics, Software systems, Software systems, stochastic processes, stochastic processes}
}

@article{laxman_survey_2006,
	title = {A survey of temporal data mining},
	volume = {31},
	issn = {0256-2499, 0973-7677},
	url = {http://link.springer.com/article/10.1007/BF02719780},
	doi = {10.1007/BF02719780},
	abstract = {Data mining is concerned with analysing large volumes of (often unstructured) data to automatically discover interesting regularities or relationships which in turn lead to better understanding of the underlying processes. The field of temporal data mining is concerned with such analysis in the case of ordered data streams with temporal interdependencies. Over the last decade many interesting techniques of temporal data mining were proposed and shown to be useful in many applications. Since temporal data mining brings together techniques from different fields such as statistics, machine learning and databases, the literature is scattered among many different sources. In this article, we present an overview of techniques of temporal data mining. We mainly concentrate on algorithms for pattern discovery in sequential data streams. We also describe some recent results regarding statistical analysis of pattern discovery methods.},
	pages = {173--198},
	number = {2},
	journal = {Sadhana},
	shortjournal = {Sadhana},
	author = {Laxman, Srivatsan and Sastry, P. S.},
	urldate = {2013-07-05},
	date = {2006-04-01},
	langid = {english},
	%note = {Cited by 0156},
	keywords = {Engineering, Engineering, general, general, ordered data streams, ordered data streams, pattern discovery, pattern discovery, Temporal data mining, Temporal data mining, temporal interdependency, temporal interdependency},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\C447VGPT\\Laxman e Sastry - 2006 - A survey of temporal data mining.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\V74TGHQS\\BF02719780.html:text/html}
}

@book{catal_software_????,
	title = {Software Fault Prediction of Unlabeled Program Modules},
	abstract = {Abstract—Software metrics and fault data belonging to a previous software version are used to build the software fault prediction model for the next release of the software. Until now, different classification algorithms have been used to build this kind of models. However, there are cases when previous fault data are not present; and hence, supervised learning approaches cannot be applied. In this study, we propose a fully automated technique which does not require an expert during the prediction process. In addition, it is not required to identify the number of clusters before the clustering phase, as required by K-means clustering method. Software metrics thresholds are used to remove the expert necessity. Our technique first applies X-means clustering method to cluster modules and identifies the best cluster number. After this step, the mean vector of each cluster is checked against the metrics thresholds vector. A cluster is predicted as fault-prone if at least one metric of the mean vector is higher than the threshold value of that metric. In addition to X-means clustering-based method, we made experiments with pure metrics thresholds method, fuzzy clustering, and K-means clustering-based methods. Experiments reveal that unsupervised software fault prediction can be fully automated and effective results can be produced using X-means clustering with software metrics thresholds. Three datasets, collected from Turkish white-goods manufacturer developing embedded controller software, have been used for the validation. Index Terms — Clustering, metrics thresholds, software fault prediction, and X-means clustering.},
	author = {Catal, C. and Sevim, U. and Diri, B.},
	%note = {Cited by 0016},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\Z7VR79PJ\\Catal et al. - Software Fault Prediction of Unlabeled Program Mod.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\MQW7T7H7\\summary.html:text/html}
}

@inproceedings{ivie_metric-based_2007,
	title = {Metric-Based Data Mining Model for Genealogical Record Linkage},
	doi = {10.1109/IRI.2007.4296676},
	abstract = {Genealogical Record Linkage ({GRL}) is the process of determining whether two pedigrees refer to the same base individual. Unlike other record linkage problems, {GRL} datasets are extremely sparse and have several multi-valued attributes. In this paper, we describe a metric-based, data mining approach to {GRL}, and report on its successful application to a large post-blocking, standardized database.},
	pages = {538--543},
	booktitle = {{IEEE} International Conference on Information Reuse and Integration, 2007. {IRI} 2007},
	author = {Ivie, S. and Pixton, B. and Giraud-Carrier, C.},
	date = {2007},
	%note = {Cited by 0002},
	keywords = {biology computing, biology computing, Cancer, Cancer, Computer Science, Computer Science, Couplings, Couplings, Databases, Databases, data mining, data mining, decision trees, decision trees, Diseases, Diseases, genealogical record linkage, genealogical record linkage, genetics, genetics, metric-based data mining, metric-based data mining, multivalued attribute, multivalued attribute, Performance evaluation, Performance evaluation, Testing, Testing}
}

@inproceedings{hassan_mining_2010,
	title = {Mining software engineering data},
	volume = {2},
	doi = {10.1145/1810295.1810451},
	abstract = {Software engineering data (such as code bases, execution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project's status, progress, and evolution. Using well-established data mining techniques, practitioners and researchers have started exploring the potential of this valuable data in order to better manage their projects and to produce higher quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining software engineering data, discusses challenges associated with mining software engineering data, highlights success stories of mining software engineering data, and outlines future research directions. Attendees will acquire the knowledge and skills needed to integrate the mining of software engineering data in their own research or practice. This tutorial builds on several successful offerings at {ICSE} since 2007.},
	pages = {503 --504},
	author = {Hassan, A.E. and Xie, Tao},
	date = {2010-05},
	%note = {Cited by 0020},
	keywords = {data mining, knowledge acquisition, project evolution, project management, project progress, project status, software engineering, software engineering data mining}
}

@inproceedings{zou_towards_2011,
	title = {Towards Training Set Reduction for Bug Triage},
	doi = {10.1109/COMPSAC.2011.80},
	abstract = {Bug triage is an important step in the process of bug fixing. The goal of bug triage is to assign a new-coming bug to the correct potential developer. The existing bug triage approaches are based on machine learning algorithms, which build classifiers from the training sets of bug reports. In practice, these approaches suffer from the large-scale and low-quality training sets. In this paper, we propose the training set reduction with both feature selection and instance selection techniques for bug triage. We combine feature selection with instance selection to improve the accuracy of bug triage. The feature selection algorithm X2-test, instance selection algorithm Iterative Case Filter, and their combinations are studied in this paper. We evaluate the training set reduction on the bug data of Eclipse. For the training set, 70\% words and 50\% bug reports are removed after the training set reduction. The experimental results show that the new and small training sets can provide better accuracy than the original one.},
	eventtitle = {Computer Software and Applications Conference ({COMPSAC}), 2011 {IEEE} 35th Annual},
	pages = {576--581},
	booktitle = {Computer Software and Applications Conference ({COMPSAC}), 2011 {IEEE} 35th Annual},
	author = {Zou, Weiqin and Hu, Yan and Xuan, Jifeng and Jiang, He},
	date = {2011},
	%note = {Cited by 0003},
	keywords = {Accuracy, bug data, bug fixing, bug triage, Computer bugs, Educational institutions, Feature Selection, feature selection algorithm, instance selection, instance selection algorithm, iterative case filter, iterative methods, Machine learning algorithms, program debugging, software, software maintenance, software quality, Text categorization, Training, training set reduction},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8UD8XJ2R\\abs_all.html:text/html;IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ZDINH6WV\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\GCNJB2BH\\Zou et al. - 2011 - Towards Training Set Reduction for Bug Triage.pdf:application/pdf;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\W32G9JGD\\Zou et al. - 2011 - Towards Training Set Reduction for Bug Triage.pdf:application/pdf}
}

@inproceedings{marks_studying_2011,
	location = {New York, {NY}, {USA}},
	title = {Studying the fix-time for bugs in large open source projects},
	isbn = {978-1-4503-0709-3},
	url = {http://doi.acm.org/10.1145/2020390.2020401},
	doi = {10.1145/2020390.2020401},
	series = {Promise '11},
	abstract = {Background: Bug fixing lies at the core of most software maintenance efforts. Most prior studies examine the effort needed to fix a bug (fix-effort). However, the effort needed to fix a bug may not correlate with the calendar time needed to fix it (fix-time). For example, the fix-time for bugs with low fix-effort may be long if they are considered to be of low priority. Aims: We study the fix-time for bugs in large open source projects. Method: We study the fix-time along three dimensions: (1) the location of the bug (e.g., which component), (2) the reporter of the bug, and (3) the description of the bug. Using these three dimensions and their associated attributes, we examine the fix-time for bugs in two large open source projects: Eclipse and Mozilla, using a random forest classifier. Results: We show that we can correctly classify {\textbackslash}textasciitilde65\% of the time the fix-time for bugs in these projects. We perform a sensitivity analysis to identify the most important attributes in each dimension. We find that the time of the filing of a bug and its location are the most important attributes in the Mozilla project for determining the fix-time of a bug. On the other hand, the fix-time in the Eclipse project is highly dependant on the severity of the bug. Surprisingly, the priority of the bug is not an important attribute when determining the fix-time for a bug in both projects. Conclusion: Attributes affecting the fix-time vary between projects and vary over time within the same project.},
	pages = {11:1â€“11:8},
	booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
	publisher = {{ACM}},
	author = {Marks, Lionel and Zou, Ying and Hassan, Ahmed E.},
	urldate = {2013-09-13},
	date = {2011},
	%note = {Cited by 0007},
	keywords = {bug fix-time, bug fix-time, empirical software engineering, empirical software engineering, mining software repositories, mining software repositories}
}

@report{gousios_exploratory_2013,
	title = {An Exploratory Study of the Pull-based Software Development Model},
	url = {http://swerl.tudelft.nl/twiki/pub/Main/TechnicalReports/TUD-SERG-2013-010.pdf},
	institution = {Delft University of Technology},
	author = {Gousios, Georgios and Pinzger, Martin and van Deursen, Arie},
	urldate = {2013-10-10},
	date = {2013},
	%note = {00000}
}

@article{kim_classifying_2008,
	title = {Classifying Software Changes: Clean or Buggy?},
	volume = {34},
	issn = {0098-5589},
	doi = {10.1109/TSE.2007.70773},
	shorttitle = {Classifying Software Changes},
	abstract = {This paper introduces a new technique for predicting latent software bugs, called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent buggy change recall on average. Change classification has several desirable qualities: 1) The prediction granularity is small (a change to a single file), 2) predictions do not require semantic information about the source code, 3) the technique works for a broad array of project types and programming languages, and 4) predictions can be made immediately upon the completion of a change. Contributions of this paper include a description of the change classification approach, techniques for extracting features from the source code and change histories, a characterization of the performance of change classification across 12 open source projects, and an evaluation of the predictive power of different groups of features.},
	pages = {181 --196},
	number = {2},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Kim, Sunghun and Whitehead, E.J. and Zhang, Yi},
	date = {2008-04},
	%note = {Cited by 0132},
	keywords = {and association rules, and association rules, association rule, association rule, change classification, change classification, Classification, Classification, Clustering, Clustering, configuration management, configuration management, data mining, data mining, feature extraction, feature extraction, learning (artificial intelligence), learning (artificial intelligence), machine learning classifier, machine learning classifier, Metrics/Measurement, Metrics/Measurement, open source projects, open source projects, program debugging, program debugging, programming languages, programming languages, software change, software change, software configuration management repository, software configuration management repository, software maintenance, software maintenance, Software metrics, Software metrics, software project, software project}
}

@article{daambros_distributed_2010,
	title = {Distributed and Collaborative Software Evolution Analysis with Churrasco},
	volume = {75},
	issn = {0167-6423},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642309001105},
	doi = {10.1016/j.scico.2009.07.005},
	shorttitle = {Experimental Software and Toolkits ({EST} 3): A special issue of the Workshop on Academic Software Development Tools and Techniques ({WASDeTT} 2008)},
	abstract = {Analyzing the evolution of large and long-lived software systems is a complex problem that requires extensive tool support due to the amount and complexity of the data that needs to be processed. In this paper, we present Churrasco, a tool to support collaborative software evolution analysis through a web interface. After describing the tool and its architecture, we provide a usage scenario of Churrasco on a large open source software system, and we present two collaboration experiments performed with, respectively, 8 and 4 participants.},
	pages = {276--287},
	number = {4},
	journal = {Science of Computer Programming},
	author = {Dâ€™Ambros, Marco and Lanza, Michele},
	urldate = {2013-08-01},
	date = {2010-04},
	%note = {Cited by 0008},
	keywords = {collaboration, collaboration, Software evolution analysis, Software evolution analysis, Visualization, Visualization}
}

@inproceedings{lamkanfi_comparing_2011,
	title = {Comparing Mining Algorithms for Predicting the Severity of a Reported Bug},
	doi = {10.1109/CSMR.2011.31},
	abstract = {A critical item of a bug report is the so-called "severity", i.e. the impact the bug has on the successful execution of the software system. Consequently, tool support for the person reporting the bug in the form of a recommender or verification system is desirable. In previous work we made a first step towards such a tool: we demonstrated that text mining can predict the severity of a given bug report with a reasonable accuracy given a training set of sufficient size. In this paper we report on a follow-up study where we compare four well-known text mining algorithms (namely, Naive Bayes, Naive Bayes Multinomial, K-Nearest Neighbor and Support Vector Machines) with respect to accuracy and training set size. We discovered that for the cases under investigation (two open source systems: Eclipse and {GNOME}) Naive Bayes Multinomial performs superior compared to the other proposed algorithms.},
	pages = {249--258},
	booktitle = {2011 15th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Lamkanfi, A. and Demeyer, S. and Soetens, Q.D. and Verdonck, T.},
	date = {2011},
	%note = {Cited by 0015},
	keywords = {Accuracy, Accuracy, bug report, bug report, bug reports, bug reports, bug severity, bug severity, Bugzilla, Bugzilla, Computer bugs, Computer bugs, data mining, data mining, Eclipse, Eclipse, {GNOME}, {GNOME}, k-nearest neighbor, k-nearest neighbor, mining algorithm, mining algorithm, Naive Bayes, Naive Bayes, naive Bayes multinomial, naive Bayes multinomial, Prediction algorithms, Prediction algorithms, program debugging, program debugging, program verification, program verification, software, software, software system, software system, support vector machines, support vector machines, text analysis, text analysis, text mining, text mining, Training, Training, verification system, verification system}
}

@inproceedings{nguyen_topic-based_2011,
	title = {A topic-based approach for narrowing the search space of buggy files from a bug report},
	doi = {10.1109/ASE.2011.6100062},
	abstract = {Locating buggy code is a time-consuming task in software development. Given a new bug report, developers must search through a large number of files in a project to locate buggy code. We propose {BugScout}, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a bug report. {BugScout} assumes that the textual contents of a bug report and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new bug report. We develop a specialized topic model that represents those technical aspects as topics in the textual contents of bug reports and source files, and correlates bug reports and corresponding buggy files via their shared topics. Our evaluation shows that {BugScout} can recommend buggy files correctly up to 45\% of the cases with a recommended ranked list of 10 files.},
	pages = {263--272},
	booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Al-Kofahi, J. and Nguyen, Hung Viet and Nguyen, T.N.},
	date = {2011},
	%note = {Cited by 0014},
	keywords = {buggy code, buggy code, buggy files, buggy files, bug report, bug report, {BugScout}, {BugScout}, defect localization, defect localization, Prediction algorithms, Prediction algorithms, program debugging, program debugging, search space, search space, Software algorithms, Software algorithms, software development, software development, software engineering, software engineering, Software systems, Software systems, source code, source code, Synchronization, Synchronization, text analysis, text analysis, textual contents, textual contents, topic based approach, topic based approach, Topic Modeling, Topic Modeling, Training, Training, Vectors, Vectors}
}

@inproceedings{gousios_dataset_2014,
	location = {New York, {NY}, {USA}},
	title = {A Dataset for Pull-based Development Research},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597122},
	doi = {10.1145/2597073.2597122},
	series = {{MSR} '14},
	abstract = {Pull requests form a new method for collaborating in distributed software development. To study the pull request distributed development model, we constructed a dataset of almost 900 projects and 350,000 pull requests, including some of the largest users of pull requests on Github. In this paper, we describe how the project selection was done, we analyze the selected features and present a machine learning tool set for the R statistics environment.},
	pages = {368--371},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Gousios, Georgios and Zaidman, Andy},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {distributed software development, empirical software engineering, pull-based development, pull request},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\45PI92PG\\Gousios e Zaidman - 2014 - A Dataset for Pull-based Development Research.pdf:application/pdf}
}

@inproceedings{panjer_predicting_2007,
	title = {Predicting Eclipse Bug Lifetimes},
	doi = {10.1109/MSR.2007.25},
	abstract = {In non-trivial software development projects planning and allocation of resources is an important and difficult task. Estimation of work time to fix a bug is commonly used to support this process. This research explores the viability of using data mining tools to predict the time to fix a bug given only the basic information known at the beginning of a bug's lifetime. To address this question, a historical portion of the Eclipse Bugzilla database is used for modeling and predicting bug lifetimes. A bug history transformation process is described and several data mining models are built and tested. Interesting behaviours derived from the models are documented. The models can correctly predict up to 34.9\% of the bugs into a discretized log scaled lifetime class.},
	pages = {29},
	booktitle = {Mining Software Repositories, 2007. {ICSE} Workshops {MSR} '07. Fourth International Workshop on},
	author = {Panjer, L.D.},
	date = {2007-05},
	%note = {Cited by 0051},
	keywords = {database management systems, database management systems, data mining, data mining, data mining model, data mining model, eclipse bug lifetime, eclipse bug lifetime, eclipse Bugzilla database, eclipse Bugzilla database, program debugging, program debugging, resource allocation, resource allocation, software development project planning, software development project planning, software engineering, software engineering}
}

@inproceedings{tian_information_2012,
	title = {Information Retrieval Based Nearest Neighbor Classification for Fine-Grained Bug Severity Prediction},
	doi = {10.1109/WCRE.2012.31},
	abstract = {Bugs are prevalent in software systems. Some bugs are critical and need to be fixed right away, whereas others are minor and their fixes could be postponed until resources are available. In this work, we propose a new approach leveraging information retrieval, in particular {BM}25-based document similarity function, to automatically predict the severity of bug reports. Our approach automatically analyzes bug reports reported in the past along with their assigned severity labels, and recommends severity labels to newly reported bug reports. Duplicate bug reports are utilized to determine what bug report features, be it textual, ordinal, or categorical, are important. We focus on predicting fine-grained severity labels, namely the different severity labels of Bugzilla including: blocker, critical, major, minor, and trivial. Compared to the existing state-of-the-art study on fine-grained severity prediction, namely the work by Menzies and Marcus, our approach brings significant improvement.},
	pages = {215--224},
	booktitle = {2012 19th Working Conference on Reverse Engineering ({WCRE})},
	author = {Tian, Yuan and Lo, D. and Sun, Chengnian},
	date = {2012},
	%note = {Cited by 0001},
	keywords = {{BM}25 based document similarity function, {BM}25 based document similarity function, bug reports, bug reports, Computer bugs, Computer bugs, Equations, Equations, fine grained bug severity prediction, fine grained bug severity prediction, information retrieval, information retrieval, machine learning, machine learning, nearest neighbor classification, nearest neighbor classification, Prediction algorithms, Prediction algorithms, program debugging, program debugging, Software systems, Software systems}
}

@article{catal_systematic_2009,
	title = {A systematic review of software fault prediction studies},
	volume = {36},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417408007215},
	doi = {10.1016/j.eswa.2008.10.027},
	abstract = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
	pages = {7346--7354},
	number = {4},
	journal = {Expert Systems with Applications},
	author = {Catal, Cagatay and Diri, Banu},
	urldate = {2013-02-27},
	date = {2009-05},
	%note = {Cited by 0111},
	keywords = {Automated fault prediction models, Automated fault prediction models, Expert systems, Expert systems, machine learning, machine learning, Method-level metrics, Method-level metrics, Public datasets, Public datasets}
}

@inproceedings{mcmillan_categorizing_2011,
	title = {Categorizing software applications for maintenance},
	doi = {10.1109/ICSM.2011.6080801},
	abstract = {Software repositories hold applications that are often categorized to improve the effectiveness of various maintenance tasks. Properly categorized applications allow stakeholders to identify requirements related to their applications and predict maintenance problems in software projects. Unfortunately, for different legal and organizational reasons the source code is often not available, thus making it difficult to automatically categorize binary executables of software applications. In this paper, we propose a novel approach in which we use Application Programming Interface ({API}) calls from third-party libraries as attributes for automatic categorization of software applications that use these {API} calls. {API} calls can be extracted from source code and more importantly, from the byte-code of applications, thus making automatic categorization approaches applicable to closed source repositories. We evaluate our approach along with other machine learning algorithms for software categorization on two large Java repositories: an open-source repository containing 3,286 projects and a closed-source one with 745 applications. Our contribution is twofold: not only do we propose a new approach that makes it possible to categorize software projects without any source code using a small number of {API} calls as attributes, but also we carried out the first comprehensive empirical evaluation of automatic categorization approaches.},
	pages = {343--352},
	booktitle = {2011 27th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {{McMillan}, C. and Linares-Vasquez, M. and Poshyvanyk, D. and Grechanik, M.},
	date = {2011},
	%note = {Cited by 0005},
	keywords = {{API} calls, {API} calls, application program interfaces, application program interfaces, application programming interface, application programming interface, automatic categorization, automatic categorization, binary executables, binary executables, byte-code, byte-code, categorizing software applications, categorizing software applications, closed-source, closed-source, closed source repository, closed source repository, closed-source repository, closed-source repository, Companies, Companies, Entropy, Entropy, Java, Java, Java repository, Java repository, learning (artificial intelligence), learning (artificial intelligence), legal reasons, legal reasons, Libraries, Libraries, machine learning, machine learning, Machine learning algorithms, Machine learning algorithms, maintenance tasks, maintenance tasks, open-source, open-source, open-source repository, open-source repository, organizational reasons, organizational reasons, predict maintenance problems, predict maintenance problems, project management, project management, public domain software, public domain software, software, software, software categorization, software categorization, software maintenance, software maintenance, software management, software management, software projects, software projects, software repository, software repository, source code, source code, support vector machines, support vector machines, third-party library, third-party library}
}

@inproceedings{hemmati_msr_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {The {MSR} cookbook: mining a decade of research},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org.ez24.periodicos.capes.gov.br/citation.cfm?id=2487085.2487150},
	series = {{MSR} '13},
	shorttitle = {The {MSR} cookbook},
	abstract = {The Mining Software Repositories ({MSR}) research community has grown significantly since the first {MSR} workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past {MSR} conferences and workshops. To that end, we review all 117 full papers published in the {MSR} proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the {MSR} community can engage in a continuing discussion on our evolving best practices.},
	pages = {343--352},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Hemmati, Hadi and Nadi, Sarah and Baysal, Olga and Kononenko, Oleksii and Wang, Wei and Holmes, Reid and Godfrey, Michael W.},
	urldate = {2013-06-21},
	date = {2013},
	%note = {Cited by 0000},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\D8D4Z2KT\\Hemmati et al. - 2013 - The MSR cookbook mining a decade of research.pdf:application/pdf}
}

@article{demsar_statistical_2006,
	title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
	volume = {7},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1248547.1248548},
	abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced {CD} (critical difference) diagrams.},
	pages = {1--30},
	journal = {The Journal of Machine Learning Research},
	author = {Dem\v{s}ar, Janez},
	urldate = {2014-07-22},
	year = {2006},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2ZKZPZ64\\Demšar - 2006 - Statistical Comparisons of Classifiers over Multip.pdf:application/pdf}
}

@inproceedings{compton_knowledge_1990,
	location = {New York, {NY}, {USA}},
	title = {Knowledge in context: a strategy for expert system maintenance},
	isbn = {0-387-52062-7},
	url = {http://dl.acm.org/citation.cfm?id=89411.89756},
	series = {{AI} '88},
	shorttitle = {Knowledge in context},
	pages = {292â€“306},
	booktitle = {Proceedings of the second Australian joint conference on Artificial intelligence},
	publisher = {Springer-Verlag New York, Inc.},
	author = {Compton, P. and Jansen, R.},
	urldate = {2012-11-08},
	date = {1990},
	%note = {Cited by 0169}
}

@inproceedings{storey_use_2005,
	title = {On the use of visualization to support awareness of human activities in software development},
	isbn = {1595930736},
	url = {http://translate.googleusercontent.com/translate_c?hl=pt-BR&prev=/search%3Fq%3DOn%2Bthe%2BUse%2Bof%2BVisualization%2Bto%2BSupport%2BAwareness%2Bof%2BHuman%2BActivities%2Bin%2BSoftware%2BDevelopment%26hl%3Dpt-BR%26biw%3D1280%26bih%3D699%26prmd%3Dimvns&rurl=translate.google.com.br&sl=en&twu=1&u=http://dl.acm.org/citation.cfm%3Fid%3D1056045&usg=ALkJrhjdhs-gGeulIC2Hp-xBKvTD1XbONg},
	doi = {10.1145/1056018.1056045},
	pages = {193},
	publisher = {{ACM} Press},
	author = {Storey, Margaret-Anne D. and Čubranić, Davor and German, Daniel M.},
	urldate = {2012-07-13},
	date = {2005},
	%note = {Cited by 0115}
}

@inproceedings{bernstein_improving_2007,
	location = {New York, {NY}, {USA}},
	title = {Improving defect prediction using temporal features and non linear models},
	isbn = {978-1-59593-722-3},
	url = {http://doi.acm.org/10.1145/1294948.1294953},
	doi = {10.1145/1294948.1294953},
	series = {{IWPSE} '07},
	abstract = {Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. We also argue that the use of non-linear models, as opposed to traditional regression, is necessary to uncover some of the hidden interrelationships between the features and the defects and maintain the accuracy of the prediction in some cases. Using data obtained from the {CVS} and Bugzilla repositories of the Eclipse project, we extract a number of temporal features, such as the number of revisions and number of reported issues within the last three months. We then use these data to predict both the location of defects (i.e., the classes in which defects will occur) as well as the number of reported bugs in the next month of the project. To that end we use standard tree-based induction algorithms in comparison with the traditional regression. Our non-linear models uncover the hidden relationships between features and defects, and present them in easy to understand form. Results also show that using the temporal features our prediction model can predict whether a source file will have a defect with an accuracy of 99\% (area under {ROC} curve 0.9251) and the number of defects with a mean absolute error of 0.019 (Spearman's correlation of 0.96).},
	pages = {11â€“18},
	booktitle = {Ninth international workshop on Principles of software evolution: in conjunction with the 6th {ESEC}/{FSE} joint meeting},
	publisher = {{ACM}},
	author = {Bernstein, Abraham and Ekanayake, Jayalath and Pinzger, Martin},
	urldate = {2013-07-23},
	date = {2007},
	%note = {Cited by 0039},
	keywords = {decision tree learner, decision tree learner, defect prediction, defect prediction, mining software repository, mining software repository}
}

@article{dick_data_2004,
	title = {Data mining in software metrics databases},
	volume = {145},
	issn = {0165-0114},
	url = {http://www.sciencedirect.com/science/article/pii/S016501140300438X},
	doi = {10.1016/j.fss.2003.10.006},
	shorttitle = {Computational Intelligence in Software Engineering},
	abstract = {We investigate the use of data mining for the analysis of software metric databases, and some of the issues in this application domain. Software metrics are collected at various phases of the software development process, in order to monitor and control the quality of a software product. However, software quality control is complicated by the complex relationship between these metrics and the attributes of a software development process. Data mining has been proposed as a potential technology for supporting and enhancing our understanding of software metrics and their relationship to software quality. In this paper, we use fuzzy clustering to investigate three datasets of software metrics, along with the larger issue of whether supervised or unsupervised learning is more appropriate for software engineering problems. While our findings generally confirm the known linear relationship between metrics and change rates, some interesting behaviors are noted. In addition, our results partly contradict earlier studies that only used correlation analysis to investigate these datasets. These results illustrate how intelligent technologies can augment traditional statistical inference in software quality control.},
	pages = {81--110},
	number = {1},
	journal = {Fuzzy Sets and Systems},
	shortjournal = {Fuzzy Sets and Systems},
	author = {Dick, Scott and Meeks, Aleksandra and Last, Mark and Bunke, Horst and Kandel, Abraham},
	urldate = {2013-01-18},
	date = {2004-07-01},
	%note = {Cited by 0044},
	keywords = {Artificial intelligence, data mining, Fuzzy clustering, machine learning, Software reliability, Software testing},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\GW5V3M32\\Dick et al. - 2004 - Data mining in software metrics databases.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\KA5R2U95\\Dick et al. - 2004 - Data mining in software metrics databases.html:text/html}
}

@article{bishnu_software_2012,
	title = {Software Fault Prediction Using Quad Tree-Based K-Means Clustering Algorithm},
	volume = {24},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2011.163},
	abstract = {Unsupervised techniques like clustering may be used for fault prediction in software modules, more so in those cases where fault labels are not available. In this paper a Quad Tree-based K-Means algorithm has been applied for predicting faults in program modules. The aims of this paper are twofold. First, Quad Trees are applied for finding the initial cluster centers to be input to the A'-Means Algorithm. An input threshold parameter δ governs the number of initial cluster centers and by varying δ the user can generate desired initial cluster centers. The concept of clustering gain has been used to determine the quality of clusters for evaluation of the Quad Tree-based initialization algorithm as compared to other initialization techniques. The clusters obtained by Quad Tree-based algorithm were found to have maximum gain values. Second, the Quad Tree- based algorithm is applied for predicting faults in program modules. The overall error rates of this prediction approach are compared to other existing algorithms and are found to be better in most of the cases.},
	pages = {1146--1150},
	number = {6},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Bishnu, P.S. and Bhattacherjee, V.},
	date = {2012},
	%note = {Cited by 0007},
	keywords = {Algorithm design and analysis, Clustering algorithms, clustering gain, Error analysis, Fault prediction, K-Means clustering, Measurement, pattern clustering, Prediction algorithms, Quad Tree, quad tree-based initialization algorithm, quad tree-based k-means clustering algorithm, quadtrees, software, Software algorithms, Software fault prediction, software fault prediction., software fault tolerance, software modules, unsupervised techniques}
}

@inproceedings{shivaji_reducing_2009,
	location = {Washington, {DC}, {USA}},
	title = {Reducing Features to Improve Bug Prediction},
	isbn = {978-0-7695-3891-4},
	url = {http://dx.doi.org/10.1109/ASE.2009.76},
	doi = {10.1109/ASE.2009.76},
	series = {{ASE} '09},
	abstract = {Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine ({SVM}) classifiers is characterized.},
	pages = {600â€“604},
	booktitle = {Proceedings of the 2009 {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Shivaji, Shivkumar and Jr., E. James Whitehead and Akella, Ram and Kim, Sunghun},
	urldate = {2013-02-20},
	date = {2009},
	%note = {Cited by 0020},
	keywords = {Bug prediction, Bug prediction, Feature Selection, Feature Selection, machine learning, machine learning, Reliability, Reliability}
}

@report{daniela_damian_irwin_kwan_survey_2011,
	title = {A {SURVEY} {OF} {TECHNIQUES} {IN} {SOFTWARE} {REPOSITORY} {MINING}},
	abstract = {Digital records of software-engineering work are left by software developers
during the development process. Source code is usually kept in a software repository, and
software developers use issue-tracking repositories and online project-tracking software,
as well as informal documentation to support their activities. The research discipline
of mining software repositories ({MSR}) uses these extant, digital repositories to gain un-derstanding of the system. {MSR} has not been applied to model-driven development or
model-driven engineering ({MDE}). In particular, model management deserve particular
attention. Model management covers challenges associated with {\textbackslash}maintaining traceabil-ity links among model elements to support model evolution and roundtrip engineering",
{\textbackslash}tracking versions", and {\textbackslash}using models during runtime". These problems can be ad-dressed by investigating the models themselves and their relationship to other artifacts
using {MSR}. The objective of this report is to survey state-of-the-art research in {MSR}
and to discuss how these {MSR} techniques are applicable to the problems faced in {MDE}.
Extracting information about what factors aect model quality, how people interact with
models in the repository, and traceability to other artifacts advance our understanding of
software engineering when {MDE} is used.},
	pages = {15},
	number = {{DCS}-340-{IR}},
	institution = {University of Victoria},
	type = {Technical Report},
	author = {Daniela Damian, Irwin Kwan},
	date = {2011},
	%note = {Cited by 0002}
}

@article{murta_odyssey-scm:_2007,
	title = {Odyssey-{SCM}: An integrated software configuration management infrastructure for {UML} models},
	volume = {65},
	issn = {0167-6423},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642306002073},
	doi = {10.1016/j.scico.2006.05.011},
	shorttitle = {Odyssey-{SCM}},
	abstract = {Model-driven development is becoming a reality. Different {CASE} tool vendors support this paradigm, allowing developers to define high-level models and helping to transform them into refined models or source code. However, current software configuration management tools use a file-based data model that is barely sufficient to manipulate source code. This file-based data model is not adequate to provide versioning capabilities for software modeling environments, which are strongly focused on analysis and architectural design artifacts. The existence of a versioned repository of high-level artifacts integrated with a customized change control process could help in the development and maintenance of such model-based systems. In this work, we introduce Odyssey-{SCM}, an integrated software configuration management infrastructure for {UML} models. This infrastructure is composed of a flexible version control system for fine-grained {UML} model elements, named Odyssey-{VCS}, and two complementary components: a customizable change control system tightly integrated with the version control system, and a traceability link detection tool that uses data mining to discover change traces among versioned {UML} model elements and provides the rationale of change traces, automatically collected from the integrated software configuration management infrastructure.},
	pages = {249--274},
	number = {3},
	journal = {Science of Computer Programming},
	author = {Murta, Leonardo and Oliveira, Hamilton and Dantas, Cristine and Lopes, Luiz Gustavo and Werner, {ClÃ}¡udia},
	urldate = {2012-07-12},
	date = {2007-04},
	%note = {Cited by 0022},
	keywords = {Change control system, Change control system, data mining, data mining, Model-driven development, Model-driven development, software configuration management, software configuration management, Version control system, Version control system}
}

@inproceedings{nguyen_discovering_2012,
	title = {Discovering complete {API} rules with mutation testing},
	doi = {10.1109/MSR.2012.6224275},
	abstract = {Specifications are important for many activities during software construction and maintenance process such as testing, verification, debugging and repairing. Despite their importance, specifications are often missing, informal or incomplete because they are difficult to write manually. Many techniques have been proposed to automatically mine specifications describing method call sequence from execution traces or source code using frequent pattern mining. Unfortunately, a sizeable number of such \#x201C;interesting \#x201D; specifications discovered by frequent pattern mining may not capture the correct use patterns of method calls. Consequently, when used in software testing or verification, these mined specifications lead to many false positive defects, which in turn consume much effort for manual investigation. We present a novel framework for automatically discovering legitimate specifications from execution traces using a mutation testing based approach. Such an approach gives a semantics bearing to the legitimacy of the discovered specifications. We introduce the notion of maximal precision and completeness as the desired forms of discovered specifications, and describe in detail suppression techniques that aid efficient discovery. Preliminary evaluation of this approach on several open source software projects shows that specifications discovered through our approach, compared with those discovered through frequent pattern mining, are much more precise and complete. When used in finding bugs, our specifications also locate defects with significantly fewer false positives and more true positives.},
	pages = {151 --160},
	booktitle = {Mining Software Repositories ({MSR}), 2012 9th {IEEE} Working Conference on},
	author = {Nguyen, Anh Cuong and Khoo, Siau-Cheng},
	date = {2012-06},
	%note = {Cited by 0000}
}

@article{menzies_data_2007,
	title = {Data Mining Static Code Attributes to Learn Defect Predictors},
	volume = {33},
	issn = {0098-5589},
	doi = {10.1109/TSE.2007.256941},
	abstract = {The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of "{McCabes} versus Halstead versus lines of code counts" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected},
	pages = {2--13},
	number = {1},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Menzies, T. and Greenwald, J. and Frank, A.},
	date = {2007},
	%note = {Cited by 0408},
	keywords = {Art, Art, artifical intelligence, artifical intelligence, Artificial intelligence, Artificial intelligence, Bayesian methods, Bayesian methods, data mining, data mining, Data mining detect prediction, Data mining detect prediction, empirical, empirical, Financial management, Financial management, Halstead, Halstead, Learning systems, Learning systems, {McCabe}, {McCabe}, naive Bayes., naive Bayes., software quality, software quality, Software systems, Software systems, Software testing, Software testing, System testing, System testing}
}

@inproceedings{abd-el-hafiz_metrics-based_2012,
	title = {A Metrics-Based Data Mining Approach for Software Clone Detection},
	doi = {10.1109/COMPSAC.2012.14},
	abstract = {The detection of function clones in software systems is valuable for the code adaptation and error checking maintenance activities. This paper presents an efficient metrics-based data mining clone detection approach. First, metrics are collected for all functions in the software system. A data mining algorithm, fractal clustering, is then utilized to partition the software system into a relatively small number of clusters. Each of the resulting clusters encapsulates functions that are within a specific proximity of each other in the metrics space. Finally, clone classes, rather than pairs, are easily extracted from the resulting clusters. For large software systems, the approach is very space efficient and linear in the size of the data set. Evaluation is performed using medium and large open source software systems. In this evaluation, the effect of the chosen metrics on the detection precision is investigated.},
	eventtitle = {Computer Software and Applications Conference ({COMPSAC}), 2012 {IEEE} 36th Annual},
	pages = {35--41},
	booktitle = {Computer Software and Applications Conference ({COMPSAC}), 2012 {IEEE} 36th Annual},
	author = {Abd-El-Hafiz, S.K.},
	date = {2012},
	%note = {Cited by 0001},
	keywords = {clone detection, clone detection, Cloning, Cloning, Clustering, Clustering, Clustering algorithms, Clustering algorithms, code adaptation, code adaptation, Complexity theory, Complexity theory, data mining, data mining, error checking, error checking, fractal clustering, fractal clustering, fractal dimension, fractal dimension, Fractals, Fractals, Measurement, Measurement, metrics based data mining approach, metrics based data mining approach, metrics space, metrics space, open source software systems, open source software systems, software clone detection, software clone detection, Software metrics, Software metrics, Software systems, Software systems},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\9SA4BPUA\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CQGDV3VH\\Abd-El-Hafiz - 2012 - A Metrics-Based Data Mining Approach for Software .pdf:application/pdf}
}

@thesis{ribeiro_monitor_2011,
	location = {Niterói},
	title = {Monitor de Métricas de Software},
	abstract = {A sub-área da engenharia de software chamada gerência de configuração de software é responsável pelo estudo e controle das mudanças que ocorrem durante o desenvolvimento de projetos de software. Gerência de configuração de software estuda o que mudou, onde mudou e quais as consequências destas mudanças no projeto. Por outro lado, métrica de software é a 
medida quantitativa do grau em que um sistema, componente ou processo se encontra em relação a um determinado atributo. Portanto, o objetivo deste trabalho é empregar métricas de software no estudo de gerência de configuração de software. Para tal, desenvolvemos coletores para vinte e duas diferentes métricas básicas, um mecanismo para definir métricas compostas de outras métricas já existentes através de fórmulas e gráficos que permitem analisar tais métricas. Finalmente, fizemos uma experiência inicial da análise de alguns desses indicadores sobre o {IdUFF}, o sistema de gestão acadêmica da Universidade Federal Fluminense.},
	institution = {Universidade Federal Fluminense},
	type = {Trabalho de Conclusão de Curso},
	author = {Ribeiro, Wallace da Silva},
	date = {2011},
	%note = {Cited by 0000}
}

@article{lessmann_benchmarking_2008,
	title = {Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings},
	volume = {34},
	issn = {0098-5589},
	doi = {10.1109/TSE.2008.35},
	shorttitle = {Benchmarking Classification Models for Software Defect Prediction},
	abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the {NASA} Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
	pages = {485--496},
	number = {4},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
	year = {2008},
	keywords = {benchmarking classification models, benchmark testing, code attributes, Complexity measures, data mining, fault-prone modules, Formal methods, metric-based classification, predictive classification models, proprietary data sets, software defect prediction, software quality, Statistical methods, statistical testing, statistical testing procedures, testing efficiency},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\35E2BC8W\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\WV56RXIC\\Lessmann et al. - 2008 - Benchmarking Classification Models for Software De.pdf:application/pdf}
}

@incollection{yu_mining_2008,
	title = {Mining Bug Classifier and Debug Strategy Association Rules for Web-Based Applications},
	rights = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-88191-9, 978-3-540-88192-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-88192-6_40},
	series = {Lecture Notes in Computer Science},
	abstract = {The paper uses data mining approaches to classify bug types and excavate debug strategy association rules for Web-based applications. Chi-square algorithm is used to extract bug features, and {SVM} to model bug classifier achieving more than 70\% predication accuracy on average. Debug strategy association rules accumulate bug fixing knowledge and experiences regarding to typical bug types, and can be applied repeatedly, thus improving the bug fixing efficiency. With 575 training data, three debug strategy association rules are unearthed.},
	pages = {427--434},
	number = {5139},
	booktitle = {Advanced Data Mining and Applications},
	publisher = {Springer Berlin Heidelberg},
	author = {Yu, Lian and Kong, Changzhu and Xu, Lei and Zhao, Jingtao and Zhang, {HuiHui}},
	editor = {Tang, Changjie and Ling, Charles X. and Zhou, Xiaofang and Cercone, Nick J. and Li, Xue},
	urldate = {2013-08-06},
	date = {2008-01-01},
	%note = {Cited by 0002},
	keywords = {Artificial Intelligence (incl. Robotics), association rule, bug classification, bug mining, Chi-square algorithm, Computer Appl. in Administrative Data Processing, Data Mining and Knowledge Discovery, debug strategy, Health Informatics, Information Systems Applications (incl.Internet), software engineering, {SVM}}
}

@inproceedings{tahir_systematic_2012,
	title = {A systematic mapping study on dynamic metrics and software quality},
	doi = {10.1109/ICSM.2012.6405289},
	abstract = {Several important aspects of software product quality can be evaluated using dynamic metrics that effectively capture and reflect the software's true runtime behavior. While the extent of research in this field is still relatively limited, particularly when compared to research on static metrics, the field is growing, given the inherent advantages of dynamic metrics. The aim of this work is to systematically investigate the body of research on dynamic software metrics to identify issues associated with their selection, design and implementation. Mapping studies are being increasingly used in software engineering to characterize an emerging body of research and to identify gaps in the field under investigation. In this study we identified and evaluated 60 works based on a set of defined selection criteria. These studies were further classified and analyzed to identify their relativity to future dynamic metrics research. The classification was based on three different facets: research focus, research type and contribution type. We found a strong body of research related to dynamic coupling and cohesion metrics, with most works also addressing the abstract notion of software complexity. Specific opportunities for future work relate to a much broader range of quality dimensions.},
	eventtitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
	pages = {326--335},
	author = {Tahir, A. and {MacDonell}, S.G.},
	date = {2012},
	%note = {Cited by 0001},
	keywords = {cohesion metrics, conferences, contribution type, dynamic analysis, dynamic coupling, dynamic metrics, dynamic software implementation, dynamic software metrics design, dynamic software metrics selection criteria, Manuals, mapping study, Performance analysis, research focus, research type, software, software complexity, software engineering, Software metrics, software product quality dimensions, software quality, software runtime behavior, systematic mapping}
}

@inproceedings{schur_experimental_2011,
	location = {New York, {NY}, {USA}},
	title = {Experimental specification mining for enterprise applications},
	isbn = {978-1-4503-0443-6},
	url = {http://doi.acm.org/10.1145/2025113.2025169},
	doi = {10.1145/2025113.2025169},
	series = {{ESEC}/{FSE} '11},
	abstract = {Specification mining infers abstractions over a set of program execution traces. Whereas inductive approaches to specification mining rely on a given set of execution traces, experimental approaches systematically generate and execute test cases to infer rich models including uncommon and exceptional behavior. State-of-the-art experimental mining approaches infer low-level models representing the behavior of single classes. This paper proposes an approach for inferring models of built-in processes in enterprise systems based on systematic scenario test generation. The paper motivates the approach, sketches the relevant concepts and challenges, and discusses related work.},
	pages = {388--391},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering},
	publisher = {{ACM}},
	author = {Schur, Matthias},
	urldate = {2013-07-03},
	date = {2011},
	%note = {Cited by 0000},
	keywords = {enterprise applications, model-based testing, specification mining, test case generation},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8747JDIP\\Schur - 2011 - Experimental specification mining for enterprise a.pdf:application/pdf}
}

@book{vembu_label_????,
	title = {Label Ranking Algorithms: A Survey},
	shorttitle = {Label Ranking Algorithms},
	abstract = {Abstract. Label ranking is a complex prediction task where the goal is to map instances to a total order over a finite set of predefined labels. An interesting aspect of this problem is that it subsumes several supervised learning problems such as multiclass prediction, multilabel classification and hierarchical classification. Unsurpisingly, there exists a plethora of label ranking algorithms in the literature due, in part, to this versatile nature of the problem. In this paper, we survey these algorithms. 1},
	author = {Vembu, Shankar and Gärtner, Thomas},
	%note = {00041},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\JZA7BS4N\\Vembu e Gärtner - Label Ranking Algorithms A Survey.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\RT8SME4W\\summary.html:text/html}
}

@inproceedings{xia_comparative_2013,
	title = {A Comparative Study of Supervised Learning Algorithms for Re-opened Bug Prediction},
	doi = {10.1109/CSMR.2013.43},
	abstract = {Bug fixing is a time-consuming and costly job which is performed in the whole life cycle of software development and maintenance. For many systems, bugs are managed in bug management systems such as Bugzilla. Generally, the status of a typical bug report in Bugzilla changes from new to assigned, verified and closed. However, some bugs have to be reopened. Reopened bugs increase the software development and maintenance cost, increase the workload of bug fixers, and might even delay the future delivery of a software. Only a few studies investigate the phenomenon of reopened bug reports. In this paper, we evaluate the effectiveness of various supervised learning algorithms to predict if a bug report would be reopened. We choose 7 state-of-the-art classical supervised learning algorithm in machine learning literature, i.e., {kNN}, {SVM}, Simple Logistic, Bayesian Network, Decision Table, {CART} and {LWL}, and 3 ensemble learning algorithms, i.e., {AdaBoost}, Bagging and Random Forest, and evaluate their performance in predicting reopened bug reports. The experiment results show that among the 10 algorithms, Bagging and Decision Table ({IDTM}) achieve the best performance. They achieve accuracy scores of 92.91\% and 92.80\%, respectively, and reopened bug reports F-Measure scores of 0.735 and 0.732, respectively. These results improve the reopened bug reports F-Measure of the state-of-the-art approaches proposed by Shihab et al. by up to 23.53\%.},
	eventtitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {331--334},
	booktitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Xia, Xin and Lo, D. and Wang, Xinyu and Yang, Xiaohu and Li, Shanping and Sun, Jianling},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {{AdaBoost} algorithm, {AdaBoost} algorithm, bagging algorithm, bagging algorithm, Bayesian network algorithm, Bayesian network algorithm, belief networks, belief networks, bug fixing, bug fixing, bug management system, bug management system, bug reports, bug reports, Bugzilla, Bugzilla, {CART} algorithm, {CART} algorithm, Classification, Classification, classification-and-regression tree, classification-and-regression tree, comparative study, comparative study, decision table algorithm, decision table algorithm, decision tables, decision tables, F-measure, F-measure, k-nearest neighbor, k-nearest neighbor, {kNN} algorithm, {kNN} algorithm, learning (artificial intelligence), learning (artificial intelligence), {LWL} algorithm, {LWL} algorithm, pattern classification, pattern classification, program debugging, program debugging, random forest algorithm, random forest algorithm, reopened bug prediction, reopened bug prediction, reopened reports, reopened reports, simple logistic algorithm, simple logistic algorithm, software development, software development, software maintenance, software maintenance, supervised learning algorithm, supervised learning algorithm, supervised learning algorithms, supervised learning algorithms, support vector machines, support vector machines, {SVM} algorithm, {SVM} algorithm, trees (mathematics), trees (mathematics)},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\6GRX2G2Z\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\9VBAR7FA\\Xia et al. - 2013 - A Comparative Study of Supervised Learning Algorit.pdf:application/pdf}
}

@inproceedings{jureczko_towards_2010,
	location = {New York, {NY}, {USA}},
	title = {Towards identifying software project clusters with regard to defect prediction},
	isbn = {978-1-4503-0404-7},
	url = {http://doi.acm.org/10.1145/1868328.1868342},
	doi = {10.1145/1868328.1868342},
	series = {{PROMISE} '10},
	abstract = {Background: This paper describes an analysis that was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary study perfomed before showed the need for a further in-depth analysis in order to identify project clusters. Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency. Method: Hierarchical and k-means clustering, as well as Kohonen's neural network was used to find groups of similar projects. The obtained clusters were investigated with the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists. Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B -- T=19, p=0.035, r=0.40; 2) cluster proprietary/open - t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding. Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
	pages = {9:1--9:10},
	booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	publisher = {{ACM}},
	author = {Jureczko, Marian and Madeyski, Lech},
	urldate = {2013-06-12},
	date = {2010},
	%note = {Cited by 0016},
	keywords = {Clustering, defect prediction, design metrics, size metrics},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4WZSHWZA\\Jureczko e Madeyski - 2010 - Towards identifying software project clusters with.pdf:application/pdf}
}

@inproceedings{giger_comparing_2011,
	location = {New York, {NY}, {USA}},
	title = {Comparing fine-grained source code changes and code churn for bug prediction},
	isbn = {978-1-4503-0574-7},
	url = {http://doi.acm.org/10.1145/1985441.1985456},
	doi = {10.1145/1985441.1985456},
	series = {{MSR} '11},
	abstract = {A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified ({LM}) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes ({SCC}) for bug prediction. {SCC} captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of {SCC} and {LM}. The results show that {SCC} outperforms {LM} for learning bug prediction models.},
	pages = {83â€“92},
	booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
	urldate = {2013-07-22},
	date = {2011},
	%note = {Cited by 0023},
	keywords = {code churn, code churn, nonlinear regression, nonlinear regression, prediction models, prediction models, software bugs, software bugs, source code changes, source code changes}
}

@inproceedings{gabel_testing_2012,
	location = {New York, {NY}, {USA}},
	title = {Testing mined specifications},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393598},
	doi = {10.1145/2393596.2393598},
	series = {{FSE} '12},
	abstract = {Specifications are necessary for nearly every software engineering task, but they are often missing or incomplete. "Specification mining" is a line of research promising to solve this problem through automated tools that infer specifications directly from existing programs. The standard practice is one of inductive learning: mining tools make observations about software and inductively generalize them into specifications. Inductive reasoning is unsound, however, and existing tools commonly grapple with the problem of inferring "false" specifications, which must be manually checked. In this work, we introduce a new technique for automatically validating mined specifications that lessens this manual burden. Our technique is not based on heuristics; it rather uses a general, semantic definition of a "true" specification. We perform systematic, targeted program transformations to test a mined specification's necessity for overall correctness. If a "violating" program is correct, the specification is false. We have implemented our technique in a prototype tool that validates temporal properties of Java programs, and we demonstrate it to be effective through a large-scale case study on the {DaCapo} benchmarks.},
	pages = {4:1--4:11},
	publisher = {{ACM}},
	author = {Gabel, Mark and Su, Zhendong},
	urldate = {2013-07-03},
	date = {2012},
	%note = {Cited by 0004},
	keywords = {reverse engineering, software tools, specification inference}
}

@inproceedings{jantan_potential_2009,
	title = {Potential Data Mining Classification Techniques for Academic Talent Forecasting},
	doi = {10.1109/ISDA.2009.64},
	abstract = {Classification and prediction are among the major techniques in data mining and widely used in various fields. In this article we present a study on how some talent management problems can be solved using classification and prediction techniques in data mining. By using this approach, the talent performance can be predicted by using past experience knowledge discovered from the existing database. In the experimental phase, we have used selected classification and prediction techniques to propose the appropriate techniques from our training dataset. An example is used to demonstrate the feasibility of the suggested classification techniques using academician performance data. Thus, by using the experiments results, we suggest the potential classification techniques for academic talent forecasting.},
	eventtitle = {Ninth International Conference on Intelligent Systems Design and Applications, 2009. {ISDA} '09},
	pages = {1173--1178},
	booktitle = {Ninth International Conference on Intelligent Systems Design and Applications, 2009. {ISDA} '09},
	author = {Jantan, H. and Hamdan, A.R. and Othman, Z.A.},
	date = {2009-12-30},
	%note = {Cited by 0004},
	keywords = {Academic Talent and Forecasting, academic talent forecasting, Application software, Classification Techniques, Databases, data mining, Electronic mail, Employee rights, Human resource management, Information science, Intelligent systems, knowledge discovery, potential data mining classification techniques, prediction techniques, statistical analysis, Technology forecasting, training dataset},
	file = {IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\5UIC7XBU\\Jantan et al. - 2009 - Potential Data Mining Classification Techniques fo.pdf:application/pdf}
}

@article{spinellis_version_2005,
	title = {Version control systems},
	volume = {22},
	issn = {0740-7459},
	doi = {10.1109/MS.2005.140},
	abstract = {Sane programmers don't write production code without the help of an editor and an interpreter or a compiler, yet the author has seen many software projects limping along without using a version control system. We can explain this contrast if we think in terms of the increased start-up costs and delayed gratification associated with adopting a {VCS}. We humans typically discount the future, and therefore implementing version control in a project appears to be a fight against human nature. It's true that you can't beat the productivity boost that compilers and editors provide, but four decades after punched-card programming in assembly language has gone out of fashion, we must now look elsewhere for our next efficiency gains. And if you or your project isn't using a {VCS}, adopting one might well be the single most important tooling improvement you can undertake.},
	pages = {108 -- 109},
	number = {5},
	journal = {Software, {IEEE}},
	author = {Spinellis, D.},
	date = {2005-10},
	%note = {Cited by 0014},
	keywords = {assembly, assembly, code, code;, configuration, configuration, control, control, Engineering, engineering;, language, language;, management, management;, production, production, Programming, programming;, punched-card, punched-card, software, software, systems, systems;, version, version}
}

@inproceedings{junior_gpu-based_2012,
	location = {Los Alamitos, {CA}, {USA}},
	title = {A {GPU}-based Architecture for Parallel Image-aware Version Control},
	doi = {http://doi.ieeecomputersociety.org/10.1109/CSMR.2012.28},
	abstract = {Version control is considered a vital component for supporting professional software development and has been widely used for textual artifacts, like source code. However, binary artifacts have received small attention when compared to the former. This fact can impose huge restrictions for projects in the game and media industry, which use large amount of binary data, such as images, videos, graphics, 3D models, and animations, together with source code. For these kinds of artifacts, existing strategies, such as storing the file as a whole for each commit or performing conventional binary delta, consume significant storage space with duplicate data, and even worse, lose vital semantic information. As a response to this problem, this paper introduces an infrastructure to support version control of image artifacts. Due to the amount of data that must be processed, we implemented our proposal using a {GPU} architecture, allowing a massively parallel approach for version control. The proposed architecture provides speedup over 55 X if compared to the same implementation in {CPU}.},
	pages = {191--200},
	publisher = {{IEEE} Computer Society},
	author = {Junior, Jose Ricardo da Silva and Pacheco, Toni and Clua, Esteban and Murta, Leonardo},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {cuda, cuda, gpu, gpu, images, images, interaction, interaction, revision control, revision control}
}

@inproceedings{bettenburg_mining_2011,
	location = {New York, {NY}, {USA}},
	title = {Mining development repositories to study the impact of collaboration on software systems},
	isbn = {978-1-4503-0443-6},
	url = {http://doi.acm.org/10.1145/2025113.2025165},
	doi = {10.1145/2025113.2025165},
	series = {{ESEC}/{FSE} '11},
	abstract = {Software development is a largely collaborative effort, of which the actual encoding of program logic in source code is a relatively small part. Yet, little is known about the impact of collaboration between stakeholders on software quality. We hypothesize that the collaboration between stakeholders during software development has a non-negligible impact on the software system. Information about collaborative activities can be recovered from traces of their communication, which are recorded in the repositories used for the development of the software system. This thesis contributes the following: 1) to make this information accessible for practitioners and researchers, we present approaches to distill communication information from development repositories, and empirically validate our proposed extractors. 2) By linking back the extracted communication data to the parts of the software system under discussion, we are able to empirically study the impact of communication, as a proxy to collaboration between stakeholders, on a software system. Through case studies on a broad spectrum of open-source software projects, we demonstrate the important role of social interactions between stakeholders with respect to the evolution of a software system.},
	pages = {376--379},
	publisher = {{ACM}},
	author = {Bettenburg, Nicolas},
	urldate = {2012-07-09},
	date = {2011},
	%note = {Cited by 0000},
	keywords = {collaboration, empirical studies, socio-technical congruence, software repositories, unstructured data}
}

@article{menzies_defect_2010,
	title = {Defect prediction from static code features: current results, limitations, new approaches},
	volume = {17},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/article/10.1007/s10515-010-0069-5},
	doi = {10.1007/s10515-010-0069-5},
	shorttitle = {Defect prediction from static code features},
	abstract = {Building quality software is expensive and software quality assurance ({QA}) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control {QA} resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve ({AUC}) of the probability of false alarms and probability of detection “{AUC}(pd, pf)”; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize “{AUC}(effort, pd)” find the smallest set of modules that contain the most errors. {WHICH} is a meta-learner framework that can be quickly customized to different goals. When customized to {AUC}(effort, pd), {WHICH} out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. {WHICH}), tuning a learner to specific local business goals can be a simple task.},
	pages = {375--407},
	number = {4},
	journal = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ayşe},
	urldate = {2013-07-23},
	date = {2010-12-01},
	langid = {english},
	%note = {Cited by 0062},
	keywords = {Artificial Intelligence (incl. Robotics), defect prediction, Software Engineering/Programming and Operating Systems, Static code features, {WHICH}},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CAMEUT93\\Menzies et al. - 2010 - Defect prediction from static code features curre.pdf:application/pdf;Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\GTVNJVJH\\Menzies et al. - 2010 - Defect prediction from static code features curre.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\6JXVG6MU\\s10515-010-0069-5.html:text/html;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\F46XGBZ3\\10.html:text/html}
}

@inproceedings{park_distributed_2002,
	title = {Distributed Data Mining: Algorithms, Systems, and Applications},
	shorttitle = {Distributed Data Mining},
	abstract = {This paper presents a brief overview of the {DDM} algorithms, systems, applications, and the emerging research directions. The structure of the paper is organized as follows. We first present the related research of {DDM} and illustrate data distribution scenarios. Then {DDM} algorithms are reviewed. Subsequently, the architectural issues in {DDM} systems and future directions are discussed},
	pages = {341--358},
	author = {Park, Byung-Hoon and Kargupta, Hillol},
	date = {2002},
	%note = {Cited by 0199}
}

@online{_preference_????,
	title = {Preference Learning},
	url = {http://www.mathematik.uni-marburg.de/%7Eeyke/publications/ki-preferences.pdf},
	urldate = {2014-05-23},
	%note = {00000},
	file = {D\:\\TEX\\Schlagwort\\ki-preferences.DVI - ki-preferences.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\K4C8SNQX\\ki-preferences.pdf:application/pdf}
}

@inproceedings{bachmann_missing_2010,
	location = {New York, {NY}, {USA}},
	title = {The missing links: bugs and bug-fix commits},
	isbn = {978-1-60558-791-2},
	url = {http://doi.acm.org/10.1145/1882291.1882308},
	doi = {10.1145/1882291.1882308},
	series = {{FSE} '10},
	shorttitle = {The missing links},
	abstract = {Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories and models of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a diffcult task, requiring an expert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache {HTTP} web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.},
	pages = {97--106},
	booktitle = {Proceedings of the eighteenth {ACM} {SIGSOFT} international symposium on Foundations of software engineering},
	publisher = {{ACM}},
	author = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
	urldate = {2013-09-13},
	date = {2010},
	%note = {Cited by 0047},
	keywords = {apache, bias, case study, manual annotation, tool}
}

@article{catal_fault_2013,
	title = {A fault detection strategy for software projects},
	volume = {20},
	issn = {1330-3651},
	url = {http://hrcak.srce.hr/index.php?show=clanak&id_clanak_jezik=143476},
	abstract = {The existing software fault prediction models require metrics and fault data belonging to previous software versions or similar software projects. However, there are cases when previous fault data are not present, such as a software...},
	pages = {1, 1--7, 7},
	number = {1},
	journal = {Tehnički vjesnik},
	author = {Catal, Cagatay and Diri, Banu},
	urldate = {2013-07-17},
	date = {2013-02-22},
	langid = {english},
	%note = {Cited by 0000},
	keywords = {detection strategies, detection strategies, fault, fault, metrics thresholds, metrics thresholds, prediction strategy, prediction strategy, Software fault prediction, Software fault prediction, Software metrics, Software metrics, software quality, software quality},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\NJ29JUBU\\Catal e Diri - 2013 - A fault detection strategy for software projects.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\T2HIZSZ7\\index.html:text/html}
}

@article{taghi_m._khoshgoftaar_predicting_1990,
	edition = {2},
	title = {Predicting Software Development Errors Using Software Complexity Metrics},
	abstract = {arious  software metrics have been developed for the 
purpose of evaluating certain characteristics of the computer program- 
ming process. Attempts to use these software metric data have met 
with questionable success in  program development scenarios. Simi- 
larly, there are many different models of software reliability. Each of 
these models fundamentally studies the computer program as a black 
box. The intrinsic assumption underlying each of the models is that the 
black box has one or more parameters that are uniquely determined 
for that particular box. Complexity metrics do provide substantial in- 
formation on the distinguishing differences among the software sys- 
tems whose reliability is being modeled and may be used in the deter- 
mination of initial parameter estimates. Many of the existing complexity 
metrics measure common aspects of program complexity and thus have 
shared variance. In this paper, we develop predictive models that in- 
corporate a functional relationship of program error measures with 
software complexity metrics and metrics based on factor analysis of 
empirical data. Specific techniques for assessing regession models are 
presented for analyzing these models},
	pages = {253--261},
	journal = {{JOURNAL} {ON} {SELECTED} {AREAS} {IN} {COMMUNICATIONS}},
	author = {Taghi M. Khoshgoftaar and John C. Munson},
	date = {1990},
	%note = {Cited by 0222}
}

@inproceedings{bettenburg_deciphering_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Deciphering the story of software development through frequent pattern mining},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486960},
	series = {{ICSE} '13},
	abstract = {Software teams record their work progress in task repositories which often require them to encode their activities in a set of edits to field values in a form-based user interface. When others read the tasks, they must decode the schema used to write the activities down. We interviewed four software teams and found out how they used the task repository fields to record their work activities. However, we also found that they had trouble interpreting task revisions that encoded for multiple activities at the same time. To assist engineers in decoding tasks, we developed a scalable method based on frequent pattern mining to identify patterns of frequently co-edited fields that each represent a conceptual work activity. We applied our method to our two years of our interviewee's task repositories and were able to abstract 83,000 field changes into just 27 patterns that cover 95\% of the task revisions. We used the 27 patterns to render the teams' tasks in web-based English newsfeeds and evaluated them with the product teams. The team agreed with most of our patterns and English interpretations, but outlined a number of improvements that we will incorporate into future work.},
	pages = {1197â€“1200},
	booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Bettenburg, Nicolas and Begel, Andrew},
	urldate = {2013-06-24},
	date = {2013},
	%note = {Cited by 0000}
}

@inproceedings{van_der_meulen_correlations_2007,
	title = {Correlations between Internal Software Metrics and Software Dependability in a Large Population of Small C/C++ Programs},
	doi = {10.1109/ISSRE.2007.12},
	abstract = {Software metrics are often supposed to give valuable information for the development of software. In this paper we focus on several common internal metrics: Lines of Code, number of comments, Halstead Volume and {McCabe}'s Cyclomatic Complexity. We try to find relations between these internal software metrics and metrics of software dependability: Probability of Failure on Demand and number of defects. The research is done using 59 specifications from a programming competition—The Online Judge–on the internet. Each specification provides us between 111 and 11,495programs for our analysis; the total number of programs used is 71,917. We excluded those programs that consist of a look-up table. The results for the Online Judge programs are: (1) there is a very strong correlation between Lines of Code and Hal- stead Volume; (2) there is an even stronger correlation between Lines of Code and {McCabe}'s Cyclomatic Complexity; (3) none of the internal software metrics makes it possible to discern correct programs from incorrect ones; (4) given a specification, there is no correlation between any of the internal software metrics and the software dependability metrics.},
	eventtitle = {The 18th {IEEE} International Symposium on Software Reliability, 2007. {ISSRE} '07},
	pages = {203--208},
	author = {van der Meulen, M.J.P. and Revilla, M.A.},
	date = {2007},
	%note = {Cited by 0015},
	keywords = {C++ language, C++ program, correlation method, correlation methods, C programs, cyclomatic complexity, decision making, Halstead volume, internal software metrics, Internet, Java, look-up table, Mathematics, Phase frequency detector, Predictive models, probability, Reliability engineering, software dependability, Software metrics, Software reliability, Table lookup}
}

@inproceedings{risi_metric_2013,
	title = {Metric Attitude},
	doi = {10.1109/CSMR.2013.59},
	abstract = {We present Metric Attitude, an Eclipse Rich Client Platform application, for the reverse engineering of object-oriented software systems. The approach graphically represents a suite of object-oriented design metrics and "traditional" code-size metrics. To assess the validity of Metric Attitude and its underlying approach, we have conducted a case study on the framework Eclipse 3.5.},
	pages = {405--408},
	booktitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Risi, M. and Scanniello, G. and Tortora, G.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {code-size metrics, code-size metrics, Eclipse 3.5 framework, Eclipse 3.5 framework, Eclipse rich client platform application, Eclipse rich client platform application, metric attitude, metric attitude, object-oriented design metrics, object-oriented design metrics, object-oriented methods, object-oriented methods, object-oriented software systems, object-oriented software systems, Program Comprehension, Program Comprehension, reverse engineering, reverse engineering, software maintenance, software maintenance, Software metrics, Software metrics, Software Visualization Tool, Software Visualization Tool}
}

@inproceedings{hooimeijer_modeling_2007,
	location = {New York, {NY}, {USA}},
	title = {Modeling bug report quality},
	isbn = {978-1-59593-882-4},
	url = {http://doi.acm.org/10.1145/1321631.1321639},
	doi = {10.1145/1321631.1321639},
	series = {{ASE} '07},
	abstract = {Software developers spend a significant portion of their resources handling user-submitted bug reports. For software that is widely deployed, the number of bug reports typically outstrips the resources available to triage them. As a result, some reports may be dealt with too slowly or not at all. We present a descriptive model of bug report quality based on a statistical analysis of surface features of over 27,000 publicly available bug reports for the Mozilla Firefox project. The model predicts whether a bug report is triaged within a given amount of time. Our analysis of this model has implications for bug reporting systems and suggests features that should be emphasized when composing bug reports. We evaluate our model empirically based on its hypothetical performance as an automatic filter of incoming bug reports. Our results show that our model performs significantly better than chance in terms of precision and recall. In addition, we show that our modelcan reduce the overall cost of software maintenance in a setting where the average cost of addressing a bug report is more than 2\% of the cost of ignoring an important bug report.},
	pages = {34â€“43},
	booktitle = {Proceedings of the twenty-second {IEEE}/{ACM} international conference on Automated software engineering},
	publisher = {{ACM}},
	author = {Hooimeijer, Pieter and Weimer, Westley},
	urldate = {2013-09-18},
	date = {2007},
	%note = {Cited by 0113},
	keywords = {bug report triage, bug report triage, information retrieval, information retrieval, issue tracking, issue tracking, statistical model, statistical model}
}

@article{saeys_review_2007,
	title = {A review of feature selection techniques in bioinformatics},
	volume = {23},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/23/19/2507},
	doi = {10.1093/bioinformatics/btm344},
	abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques.
In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.
Contact: yvan.saeys@psb.ugent.be
Supplementary information: http://bioinformatics.psb.ugent.be/supplementary\_data/yvsae/fsreview},
	pages = {2507--2517},
	number = {19},
	journal = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Saeys, Yvan and Inza, Iñaki and Larrañaga, Pedro},
	urldate = {2013-07-23},
	date = {2007-01-10},
	langid = {english},
	%note = {Cited by 0994},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\35PVQ5RK\\Saeys et al. - 2007 - A review of feature selection techniques in bioinf.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\EJBJIAPS\\2507.html:text/html}
}

@inproceedings{anvik_who_2006,
	location = {New York, {NY}, {USA}},
	title = {Who Should Fix This Bug?},
	isbn = {1-59593-375-1},
	url = {http://doi.acm.org/10.1145/1134285.1134336},
	doi = {10.1145/1134285.1134336},
	series = {{ICSE} '06},
	abstract = {Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57\% and 64\% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.},
	pages = {361--370},
	booktitle = {Proceedings of the 28th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
	urldate = {2014-06-30},
	date = {2006},
	%note = {00444},
	keywords = {bug report assignment, bug triage, issue tracking, machine learning, problem tracking},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\SKQANU62\\Anvik et al. - 2006 - Who Should Fix This Bug.pdf:application/pdf}
}

@article{li_supervised_2006,
	title = {A supervised clustering and classification algorithm for mining data with mixed variables},
	volume = {36},
	issn = {1083-4427},
	doi = {10.1109/TSMCA.2005.853501},
	abstract = {This paper presents a data mining algorithm based on supervised clustering to learn data patterns and use these patterns for data classification. This algorithm enables a scalable incremental learning of patterns from data with both numeric and nominal variables. Two different methods of combining numeric and nominal variables in calculating the distance between clusters are investigated. In one method, separate distance measures are calculated for numeric and nominal variables, respectively, and are then combined into an overall distance measure. In another method, nominal variables are converted into numeric variables, and then a distance measure is calculated using all variables. We analyze the computational complexity, and thus, the scalability, of the algorithm, and test its performance on a number of data sets from various application domains. The prediction accuracy and reliability of the algorithm are analyzed, tested, and compared with those of several other data mining algorithms.},
	pages = {396--406},
	number = {2},
	journal = {{IEEE} Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans},
	author = {Li, Xiangyang and Ye, Nong},
	date = {2006},
	%note = {Cited by 0035},
	keywords = {Algorithm design and analysis, Algorithm design and analysis, Application software, Application software, Classification, Classification, Classification algorithms, Classification algorithms, Clustering, Clustering, Clustering algorithms, Clustering algorithms, computational complexity, computational complexity, computer intrusion detection, computer intrusion detection, data classification algorithm, data classification algorithm, data mining, data mining, data mining algorithm, data mining algorithm, data pattern learning, data pattern learning, dissimilarity measures, dissimilarity measures, Intrusion detection, Intrusion detection, learning (artificial intelligence), learning (artificial intelligence), Military computing, Military computing, mixed variables, mixed variables, nominal variables conversion, nominal variables conversion, numeric variables, numeric variables, Partitioning algorithms, Partitioning algorithms, Scalability, Scalability, scalable incremental learning, scalable incremental learning, separate distance measures, separate distance measures, supervised clustering algorithm, supervised clustering algorithm, Testing, Testing},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\V496M5WD\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\W8USE9NM\\Li e Ye - 2006 - A supervised clustering and classification algorit.pdf:application/pdf}
}

@inproceedings{zimmermann_predicting_2007,
	location = {Washington, {DC}, {USA}},
	title = {Predicting Defects for Eclipse},
	isbn = {0-7695-2954-2},
	url = {http://dx.doi.org/10.1109/PROMISE.2007.10},
	doi = {10.1109/PROMISE.2007.10},
	series = {{PROMISE} '07},
	abstract = {We have mapped defects from the bug database of Eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the Eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
	pages = {9--},
	publisher = {{IEEE} Computer Society},
	author = {Zimmermann, Thomas and Premraj, Rahul and Zeller, Andreas},
	urldate = {2013-02-14},
	date = {2007},
	%note = {Cited by 0254}
}

@article{ghezzi_framework_2013,
	title = {A framework for semi-automated software evolution analysis composition},
	volume = {20},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/article/10.1007/s10515-013-0125-z},
	doi = {10.1007/s10515-013-0125-z},
	abstract = {Software evolution data stored in repositories such as version control, bug and issue tracking, or mailing lists is crucial to better understand a software system and assess its quality. A myriad of analyses exploiting such data have been proposed throughout the years. However, easy and straight forward synergies between these analyses rarely exist. To tackle this problem we have investigated the concept of Software Analysis as a Service and devised {SOFAS}, a distributed and collaborative software evolution analysis platform. Software analyses are offered as services that can be accessed, composed into workflows, and executed over the Internet. This paper presents our framework for composing these analyses into workflows, consisting of a custom-made modeling language and a composition infrastructure for the service offerings. The framework exploits the {RESTful} nature of our analysis service architecture and comes with a service composer to enable semi-automated service compositions by a user. We validate our framework by showcasing two different approaches built on top of it that support different stakeholders in gaining a deeper insight into a project history and evolution. As a result, our framework has shown its applicability to deliver diverse, complex analyses across system and tool boundaries.},
	pages = {463--496},
	number = {3},
	journal = {Automated Software Engineering},
	shortjournal = {Autom Softw Eng},
	author = {Ghezzi, Giacomo and Gall, Harald C.},
	urldate = {2013-07-04},
	date = {2013-09-01},
	langid = {english},
	%note = {Cited by 0001},
	keywords = {Artificial Intelligence (incl. Robotics), Artificial Intelligence (incl. Robotics), Software Engineering/Programming and Operating Systems, Software Engineering/Programming and Operating Systems},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\QWNXSPU4\\Ghezzi and Gall - 2013 - A framework for semi-automated software evolution .pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\C79WG3MP\\Ghezzi and Gall - 2013 - A framework for semi-automated software evolution .html:text/html}
}

@inproceedings{xie_mining_2007,
	title = {Mining Software Engineering Data},
	doi = {10.1109/ICSECOMPANION.2007.50},
	abstract = {Software engineering data (such as code bases, exe- cution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a {projectÃ}‚Â¿s status, progress, and evolution. Using well- established data mining techniques, practitioners and re- searchers can explore the potential of this valuable data in order to better manage their projects and to produce higher-quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining Soft- ware Engineering ({SE}) data, discusses challenges associ- ated with mining {SE} data, highlights {SE} data mining suc- cess stories, and outlines future research directions. Partic- ipants will acquire knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice.},
	pages = {172 --173},
	booktitle = {Software Engineering - Companion, 2007. {ICSE} 2007 Companion. 29th International Conference on},
	author = {Xie, Tao and Pei, Jian and Hassan, Ahmed E.},
	date = {2007-05},
	%note = {Cited by 0028}
}

@inproceedings{bissyande_empirical_2013,
	title = {Empirical Evaluation of Bug Linking},
	doi = {10.1109/CSMR.2013.19},
	abstract = {To collect software bugs found by users, development teams often set up bug trackers using systems such as Bugzilla. Developers would then fix some of the bugs and commit corresponding code changes into version control systems such as svn or git. Unfortunately, the links between bug reports and code changes are missing for many software projects as the bug tracking and version control systems are often maintained separately. Yet, linking bug reports to fix commits is important as it could shed light into the nature of bug fixing processes and expose patterns in software management. Bug linking solutions, such as {ReLink}, have been proposed. The demonstration of their effectiveness however faces a number of issues, including a reliability issue with their ground truth datasets as well as the extent of their measurements. We propose in this study a benchmark for evaluating bug linking solutions. This benchmark includes a dataset of about 12,000 bug links from 10 programs. These true links between bug reports and their fixes have been provided during bug fixing processes. We designed a number of research questions, to assess both quantitatively and qualitatively the effectiveness of a bug linking tool. Finally, we apply this benchmark on {ReLink} to report the strengths and limitations of this bug linking tool.},
	pages = {89--98},
	booktitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Bissyande, T.F. and Thung, F. and Wang, Shaowei and Lo, D. and Jiang, Lingxiao and Reveillere, L.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {benchmark, benchmark, Bug Linking, Bug Linking, bug linking empirical evaluation, bug linking empirical evaluation, bug linking tool, bug linking tool, bug tracking system, bug tracking system, Bugzilla system, Bugzilla system, code change, code change, empirical evaluation, empirical evaluation, ground truth dataset, ground truth dataset, missing links, missing links, program debugging, program debugging, {ReLink}, {ReLink}, {ReLink} solution, {ReLink} solution, software bug collection, software bug collection, software engineering, software engineering, software management, software management, software project, software project, Version control system, Version control system}
}

@inproceedings{chen_predicting_2014,
	location = {New York, {NY}, {USA}},
	title = {Predicting the Number of Forks for Open Source Software Project},
	isbn = {978-1-4503-2965-1},
	url = {http://doi.acm.org/10.1145/2627508.2627515},
	doi = {10.1145/2627508.2627515},
	series = {{EAST} 2014},
	abstract = {{GitHub} is successful open source software platform which attract many developers. In {GitHub}, developers are allowed to fork repositories and copy repositories without asking for permission, which make contribution to projects much easier than it has ever been. It is significant to predict the number of forks for open source software projects. The prediction can help {GitHub} to recommend popular projects, and guide developers to find projects which are likely to succeed and worthy of their contribution.   In this paper, we use stepwise regression and design a model to predict the number of forks for open source software projects. Then we collect datasets of 1,000 repositories through {GitHub}’s {APIs}. We use datasets of 700 repositories to compute the weight of attributes and realize the model. Then we use other 300 repositories to verify the prediction accuracy of our model. Advantages of our model include: (1) Some attributes used in our model are new. This is because {GitHub} is different from traditional open source software platforms and has some new features. These new features are used to build our model. (2) Our model uses project information within t month after its creation, and predicts the number of forks in the month T (t {\textless} T). It allows users to set the combination of time parameters and satisfy their own needs. (3) Our model predicts the exact number of forks, rather than the range of the number of forks (4) Experiments show that our model has high prediction accuracy. For example, we use project information with 3 months to prediction the number of forks in month 6 after its creation. The correlation coefficient is as high as 0.992, and the median number of absolute difference between prediction value and actual value is only 1.8. It shows that the predicted number of forks is very close to the actual number of forks. Our model also has high prediction accuracy when we set other time parameters.},
	pages = {40--47},
	booktitle = {Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies},
	publisher = {{ACM}},
	author = {Chen, Fangwei and Li, Lei and Jiang, Jing and Zhang, Li},
	urldate = {2014-06-11},
	date = {2014},
	%note = {00000},
	keywords = {Fork, Open source software},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4XF3KB66\\Chen et al. - 2014 - Predicting the Number of Forks for Open Source Sof.pdf:application/pdf}
}

@article{radjenovic_software_2013,
	title = {Software fault prediction metrics: A systematic literature review},
	volume = {55},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584913000426},
	doi = {10.1016/j.infsof.2013.02.009},
	shorttitle = {Software fault prediction metrics},
	abstract = {Context 
Software metrics may be used in fault prediction models to improve software quality by predicting fault location. 
Objective 
This paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics’ selection and performance. 
Method 
This systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties. 
Results 
Object-oriented metrics (49\%) were used nearly twice as often compared to traditional source code metrics (27\%) or process metrics (24\%). Chidamber and Kemerer’s ({CK}) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics. 
Conclusion 
More studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.},
	pages = {1397--1418},
	number = {8},
	journal = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Radjenović, Danijel and Heričko, Marjan and Torkar, Richard and Živkovič, Aleš},
	urldate = {2013-06-11},
	date = {2013-08},
	%note = {Cited by 0000},
	keywords = {Software fault prediction, Software metric, Systematic literature review},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8UH3WM4Z\\Radjenović et al. - 2013 - Software fault prediction metrics A systematic li.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\EHFN97SR\\Radjenović et al. - 2013 - Software fault prediction metrics A systematic li.html:text/html}
}

@inproceedings{mockus_predictors_2005,
	location = {New York, {NY}, {USA}},
	title = {Predictors of customer perceived software quality},
	isbn = {1-58113-963-2},
	url = {http://doi.acm.org/10.1145/1062455.1062506},
	doi = {10.1145/1062455.1062506},
	series = {{ICSE} '05},
	abstract = {Predicting software quality as perceived by a customer may allow an organization to adjust deployment to meet the quality expectations of its customers, to allocate the appropriate amount of maintenance resources, and to direct quality improvement efforts to maximize the return on investment. However, customer perceived quality may be affected not simply by the software content and the development process, but also by a number of other factors including deployment issues, amount of usage, software platform, and hardware configurations. We predict customer perceived quality as measured by various service interactions, including software defect reports, requests for assistance, and field technician dispatches using the afore mentioned and other factors for a large telecommunications software system. We employ the non-intrusive data gathering technique of using existing data captured in automated project monitoring and tracking systems as well as customer support and tracking systems. We find that the effects of deployment schedule, hardware configurations, and software platform can increase the probability of observing a software failure by more than 20 times. Furthermore, we find that the factors affect all quality measures in a similar fashion. Our approach can be applied at other organizations, and we suggest methods to independently validate and replicate our results.},
	pages = {225â€“233},
	booktitle = {Proceedings of the 27th international conference on Software engineering},
	publisher = {{ACM}},
	author = {Mockus, Audris and Zhang, Ping and Li, Paul Luo},
	urldate = {2013-04-02},
	date = {2005},
	%note = {Cited by 0072},
	keywords = {metrics, metrics, modeling, modeling, quality, quality}
}

@thesis{ribeiro_ostra:_2012,
	location = {Rio de Janeiro},
	title = {Ostra: um estudo do histórico da qualidade do software através  de regras de associação de métricas.},
	abstract = {O ciclo de vida de um software pode ser dividido em três fases: definição, 
desenvolvimento e manutenção. Dessas três, a manutenção consome cerca de 60\% do tempo e 
90\% do custo do projeto. Porém, os atributos de qualidade  flexibilidade e manutenibilidade, 
que avaliam a capacidade de manutenções evolutivas e corretivas, são definidos desde a fase 
de desenvolvimento. Consequentemente, deve-se monitorar os atributos de qualidade desde a 
fase de desenvolvimento para garantir a qualidade do produto. Dessa forma, para controlar a 
qualidade do software e não apenas reagir à sua variação, deve-se entender sobre quais fatores 
influenciam os atributos de qualidade e como eles variam entre si. Neste trabalho, é proposta a 
abordagem Ostra, que permite a análise do histórico de um software, armazenado no sistema 
de controle de versão, através da variação de métricas de software. Os objetivos da Ostra são: 
(1)  fornecer informações ao processo de tomada de decisões,  (2) monitorar a qualidade do 
software e  (3)  buscar padrões entre métricas  presentes em um ou vários projetos.  Para 
alcançar esses objetivos, a Ostra se baseia em três disciplinas: métricas de software, gerência 
de configuração e mineração de dados.  Para apresentar as informações encontradas, são 
utilizados:  gráficos de controle, histogramas, regras de associação e uma tabela de 
comportamentos. Para avaliar a abordagem,  foram  utilizados 16 projetos em  três 
experimentos  distintos, com os quais  foi possível  obter indícios  de que  os  objetivos  da 
abordagem são alcançados  para projetos de diferentes tamanhos e finalidades.  As 
contribuições deste trabalho são: (1) uma abordagem que considera as alterações realizadas ao 
longo da evolução do software como elemento básico de análise, as quais são descritas como 
a variação de métricas, (2) experimentos  avaliando as questões de pesquisa, (3) uma 
infraestrutura para futuras pesquisas sobre mineração de repositórios de software e (4) uma 
base de dados com a medição de cerca de 30 métricas sobre 150 projetos.},
	pagetotal = {175},
	institution = {Universidade Federal Fluminense},
	type = {Dissertação},
	author = {Ribeiro, Daniel Drumond Castellani},
	date = {2012},
	%note = {Cited by 0000}
}

@article{lim_comparison_2000,
	title = {A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-Three Old and New Classification Algorithms},
	volume = {40},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1023/A%3A1007608224229},
	doi = {10.1023/A:1007608224229},
	abstract = {Twenty-two decision tree, nine statistical, and two neural network algorithms are compared on thirty-two datasets in terms of classification accuracy, training time, and (in the case of trees) number of leaves. Classification accuracy is measured by mean error rate and mean rank of error rate. Both criteria place a statistical, spline-based, algorithm called {POLYCLSSS} at the top, although it is not statistically significantly different from twenty other algorithms. Another statistical algorithm, logistic regression, is second with respect to the two accuracy criteria. The most accurate decision tree algorithm is {QUEST} with linear splits, which ranks fourth and fifth, respectively. Although spline-based statistical algorithms tend to have good accuracy, they also require relatively long training times. {POLYCLASS}, for example, is third last in terms of median training time. It often requires hours of training compared to seconds for other algorithms. The {QUEST} and logistic regression algorithms are substantially faster. Among decision tree algorithms with univariate splits, C4.5, {IND}-{CART}, and {QUEST} have the best combinations of error rate and speed. But C4.5 tends to produce trees with twice as many leaves as those from {IND}-{CART} and {QUEST}.},
	pages = {203--228},
	number = {3},
	journal = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Lim, Tjen-Sien and Loh, Wei-Yin and Shih, Yu-Shan},
	urldate = {2013-07-16},
	date = {2000-09-01},
	langid = {english},
	%note = {Cited by 0767},
	keywords = {Artificial Intelligence (incl. Robotics), Artificial Intelligence (incl. Robotics), Automation and Robotics, Automation and Robotics, classification tree, classification tree, Computer Science, Computer Science, general, decision tree, decision tree, general, neural net, neural net, statistical classifier, statistical classifier},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\UQ7USBR2\\Lim et al. - 2000 - A Comparison of Prediction Accuracy, Complexity, a.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\VIBSRKK2\\Lim et al. - 2000 - A Comparison of Prediction Accuracy, Complexity, a.html:text/html}
}

@article{aha_instance-based_1991,
	title = {Instance-Based Learning Algorithms},
	volume = {6},
	issn = {0885-6125},
	url = {http://dx.doi.org/10.1023/A:1022689900470},
	doi = {10.1023/A:1022689900470},
	abstract = {Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-world databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.},
	pages = {37--66},
	number = {1},
	journal = {Machine Learning},
	author = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
	urldate = {2014-09-25},
	year = {1991},
	keywords = {incremental learning, instance-based concept descriptions, learning theory, Noise, similarity, Supervised concept learning},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\FECPC3NA\\Aha et al. - 1991 - Instance-based learning algorithms.pdf:application/pdf}
}

@inproceedings{finlay_mining_2011,
	title = {Mining Software Metrics from Jazz},
	doi = {10.1109/SERA.2011.40},
	abstract = {In this paper, we describe the extraction of source code metrics from the Jazz repository and the application of data mining techniques to identify the most useful of those metrics for predicting the success or failure of an attempt to construct a working instance of the software product. We present results from a systematic study using the J48 classification method. The results indicate that only a relatively small number of the available software metrics that we considered have any significance for predicting the outcome of a build. These significant metrics are discussed and implication of the results discussed, particularly the relative difficulty of being able to predict failed build attempts.},
	pages = {39 --45},
	booktitle = {2011 9th International Conference on Software Engineering Research, Management and Applications ({SERA})},
	author = {Finlay, J. and Connor, A.M. and Pears, R.},
	date = {2011-08},
	%note = {Cited by 0002},
	keywords = {Accuracy, Accuracy, Classification tree analysis, Classification tree analysis, data mining, data mining, J48 classification method, J48 classification method, Jazz, Jazz, Programming, Programming, software, software, Software metrics, Software metrics, software repositories, software repositories, source code metrics, source code metrics}
}

@inproceedings{peters_better_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Better cross company defect prediction},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487161},
	series = {{MSR} '13},
	abstract = {How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within- company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64\% more useful predictors than both within- company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.},
	pages = {409â€“418},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Peters, Fayola and Menzies, Tim and Marcus, Andrian},
	urldate = {2013-08-05},
	date = {2013},
	%note = {Cited by 0002}
}

@inproceedings{canfora_supporting_2006,
	location = {New York, {NY}, {USA}},
	title = {Supporting change request assignment in open source development},
	isbn = {1-59593-108-2},
	url = {http://doi.acm.org/10.1145/1141277.1141693},
	doi = {10.1145/1141277.1141693},
	series = {{SAC} '06},
	abstract = {Software repositories, such as {CVS} and Bugzilla, provide a huge amount of data regarding, respectively, source code and change request history. In this paper we propose a study on how change requests have been assigned to developers involved in an open source project and a method to suggest the set of best candidate developers to resolve a new change request. The method is based on the hypothesis that, given a new change request, developers that have resolved similar change requests in the past are the best candidates to resolve the new one. The suggestion can be useful for project managers in order to choose the best candidate to resolve a particular change request and/or to construct a competence database of developers working on software projects. We use the textual description of change requests stored in software repositories to index developers as documents in an information retrieval system. An Information Retrieval method is then applied to retrieve the candidate developers using the textual description of a new change request as a query.Case and evaluation study of the analysis and the methods introduced in this paper has been conducted on two large open source projects, Mozilla and {KDE}.},
	pages = {1767â€“1772},
	booktitle = {Proceedings of the 2006 {ACM} symposium on Applied computing},
	publisher = {{ACM}},
	author = {Canfora, Gerardo and Cerulo, Luigi},
	urldate = {2013-10-21},
	date = {2006},
	%note = {Cited by 0067},
	keywords = {information retrieval, information retrieval, maintenance task assignment, maintenance task assignment, mining software repositories, mining software repositories}
}

@article{cubranic_hipikat:_2005,
	title = {Hipikat: A Project Memory for Software Development},
	volume = {31},
	pages = {446--465},
	number = {6},
	author = {Cubranic, Davor and Murphy, Gail C. and Singer, Janice and Booth, Kellogg S.},
	date = {2005},
	%note = {Cited by 0193}
}

@inproceedings{kim_defect_2012,
	location = {New York, {NY}, {USA}},
	title = {Defect, defect, defect: defect prediction 2.0},
	isbn = {978-1-4503-1241-7},
	url = {http://doi.acm.org/10.1145/2365324.2365325},
	doi = {10.1145/2365324.2365325},
	series = {{PROMISE} '12},
	shorttitle = {Defect, defect, defect},
	abstract = {Defect prediction has been a very active research area in software engineering [6–8, 11, 13, 16, 19, 20]. In 1971, Akiyama proposed one of the earliest defect prediction models using Lines of Code ({LOC}) [1]: "Defect = 4.86 + 0.018LOC." Since then, many effective new defect prediction models and metrics have been proposed. For the prediction models, typical machine learners and regression algorithms such as Naive Bayes, Decision Tree, and Linear Regression are widely used. On the other hand, Kim et al. proposed a cache-based prediction model using bug occurrence properties [9]. Hassan proposed a change entropy model to effectively predict defects [6]. Recently, Bettenburg et al. proposed Multivariate Adaptive Regression Splines to improve defect prediction models by learning from local and global properties together [4]. Besides {LOC}, many new effective metrics for defect prediction have been proposed. Among them, source code metrics and change history metrics are widely used and yield reasonable defect prediction accuracy. For example, Basili et al. [3] used Chidamber and Kemerer metrics, and Ohlsson et al. [14] used {McCabe}'s cyclomatic complexity for defect prediction. Moser et al. [12] used the number of revisions, authors, and past fixes, and age of a file as defect predictors. Recently, micro interaction metrics ({MIMs}) [10] and source code quality measures [15] for effective defect prediction are proposed. However, there is much room to improve for defect prediction 2.0. First of all, understanding the actual causes of defects is necessary. Without understanding them, we may reach to nonsensical conclusions from defect prediction results [18]. Many effective prediction models have been proposed, but successful application cases in practice are scarcely reported. To be more attractive for developers in practice, it is desirable to predict defects in finer granularity levels such as the code line or even keyword level. Note that static bug finders such as {FindBugs} [2] can identify potential bugs in the line level, and many developers find them useful in practice. Dealing with noise in defect data has become an important issue. Bird et al. identified there is non-neglectable noise in defect data [5]. This noise may yield poor and/or meaningless defect prediction results. Cross-prediction is highly desirable: for new projects or projects with limited training data, it is necessary to learn a prediction model using sufficient training data from other projects, and to apply the model to those projects. However, Zimmermann et al. [21] identified cross-project prediction is a challenging problem. Turhan et al. [17] analyzed Cross-Company ({CC}) and Within-Company ({WC}) data for defect prediction, and confirmed that it is challenging to reuse {CC} data directly to predict defects in other companies' software. Overall, defect prediction is a very interesting and promising research area. However, there are still many research challenges and problems to be addressed. Hopefully, this discussion calls new solutions and ideas to address these challenges.},
	pages = {1â€“2},
	booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering},
	publisher = {{ACM}},
	author = {Kim, Sunghun},
	urldate = {2013-07-20},
	date = {2012},
	%note = {Cited by 0001}
}

@article{kitchenham_systematic_2009,
	title = {Systematic literature reviews in software engineering – A systematic literature review},
	volume = {51},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584908001390},
	doi = {10.1016/j.infsof.2008.09.009},
	shorttitle = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
	abstract = {Background 
In 2004 the concept of evidence-based software engineering ({EBSE}) was introduced at the {ICSE}04 conference. 
Aims 
This study assesses the impact of systematic literature reviews ({SLRs}) which are the recommended {EBSE} method for aggregating evidence. 
Method 
We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. 
Results 
Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven {SLRs} addressed cost estimation. The quality of {SLRs} was fair with only three scoring less than 2 out of 4. 
Conclusions 
Currently, the topic areas covered by {SLRs} are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation {SLRs} demonstrate the potential value of {EBSE} for synthesising evidence and making it available to practitioners.},
	pages = {7--15},
	number = {1},
	journal = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Kitchenham, Barbara and Pearl Brereton, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
	urldate = {2013-07-05},
	date = {2009-01},
	%note = {Cited by 0256},
	keywords = {Cost estimation, Evidence-based software engineering, Systematic literature review, Systematic review quality, Tertiary study},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2W6PNTEA\\Kitchenham et al. - 2009 - Systematic literature reviews in software engineer.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\GF3DEWVU\\S0950584908001390.html:text/html}
}

@inproceedings{guzman_sentiment_2014,
	location = {New York, {NY}, {USA}},
	title = {Sentiment Analysis of Commit Comments in {GitHub}: An Empirical Study},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597118},
	doi = {10.1145/2597073.2597118},
	series = {{MSR} 2014},
	shorttitle = {Sentiment Analysis of Commit Comments in {GitHub}},
	abstract = {Emotions have a high impact in productivity, task quality, creativity, group rapport and job satisfaction. In this work we use lexical sentiment analysis to study emotions expressed in commit comments of different open source projects and analyze their relationship with different factors such as used programming language, time and day of the week in which the commit was made, team distribution and project approval. Our results show that projects developed in Java tend to have more negative commit comments, and that projects that have more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, we found that commit comments written on Mondays tend to a more negative emotion. While our results need to be confirmed by a more representative sample they are an initial step into the study of emotions and related factors in open source projects.},
	pages = {352--355},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Guzman, Emitza and Azócar, David and Li, Yang},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Human Factors in Software Engineering, Sentiment Analysis},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DCPKXAQN\\Guzman et al. - 2014 - Sentiment Analysis of Commit Comments in GitHub A.pdf:application/pdf}
}

@inproceedings{jamshidi_framework_2013,
	title = {A Framework for Classifying and Comparing Architecture-centric Software Evolution Research},
	doi = {10.1109/CSMR.2013.39},
	abstract = {Context: Software systems are increasingly required to operate in an open world, characterized by continuous changes in the environment and in the prescribed requirements. Architecture-centric software evolution ({ACSE}) is considered as an approach to support software adaptation at a controllable level of abstraction in order to survive in the uncertain environment. This requires evolution in system structure and behavior that can be modeled, analyzed and evolved in a formal fashion. Existing research and practices comprise a wide spectrum of evolution-centric approaches in terms of formalisms, methods, processes and frameworks to tackle {ACSE} as well as empirical studies to consolidate existing research. However, there is no unified framework providing systematic insight into classification and comparison of state-of-the-art in {ACSE} research. Objective: We present a taxonomic scheme for a classification and comparison of existing {ACSE} research approaches, leading to a reflection on areas of future research. Method: We performed a systematic literature review ({SLR}), resulting in 4138 papers searched and 60 peer-reviewed papers considered for data collection. We populated the taxonomic scheme based on a quantitative and qualitative extraction of data items from the included studies. Results: We identified five main classification categories: (i) type of evolution, (ii) type of specification, (iii) type of architectural reasoning, (iv) runtime issues, and (v) tool support. The selected studies are compared based on their claims and supporting evidences through the scheme. Conclusion: The classification scheme provides a critical view of different aspects to be considered when addressing specific {ACSE} problems. Besides, the consolidation of the {ACSE} evidences reflects current trends and the needs for future research directions.},
	eventtitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {305--314},
	booktitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Jamshidi, P. and Ghafari, M. and Ahmad, A. and Pahl, C.},
	date = {2013},
	%note = {Cited by 0004},
	keywords = {abstraction level, abstraction level, {ACSE}, {ACSE}, architectural reasoning type classification category, architectural reasoning type classification category, architecture-centric software evolution, architecture-centric software evolution, classification scheme, classification scheme, comparison scheme, comparison scheme, data collection, data collection, Evidence-Based and Empirical Study, Evidence-Based and Empirical Study, evolution-centric approach, evolution-centric approach, evolution type classification category, evolution type classification category, pattern classification, pattern classification, qualitative data extraction, qualitative data extraction, quantitative data extraction, quantitative data extraction, runtime issue classification category, runtime issue classification category, {SLR}, {SLR}, software adaptation, software adaptation, software architecture, software architecture, software system, software system, specification type classification category, specification type classification category, Systematic literature review, Systematic literature review, taxonomic scheme, taxonomic scheme, tool support classification category, tool support classification category},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\PPI8PWBN\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4QFCGM2A\\Jamshidi et al. - 2013 - A Framework for Classifying and Comparing Architec.pdf:application/pdf}
}

@inproceedings{bettenburg_think_2012,
	title = {Think locally, act globally: Improving defect and effort prediction models},
	doi = {10.1109/MSR.2012.6224300},
	shorttitle = {Think locally, act globally},
	abstract = {Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.},
	eventtitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	pages = {60--69},
	booktitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {Bettenburg, Nicolas and Nagappan, M. and Hassan, A.E.},
	date = {2012},
	%note = {Cited by 0014},
	keywords = {Adaptation models, Biological system modeling, data handling, Data models, effort prediction models, fine-grained subsets, learning (artificial intelligence), machine learning models, Measurement, models, prediction model defect, Predictive models, regression analysis, software, software engineering, Software metrics, splitting datasets, statistical regression models, techniques},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\9GKX4HDN\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XZX2WSNW\\Bettenburg et al. - 2012 - Think locally, act globally Improving defect and .pdf:application/pdf}
}

@incollection{talia_weka4ws:_2005,
	title = {Weka4WS: A {WSRF}-Enabled Weka Toolkit for Distributed Data Mining on Grids},
	volume = {3721},
	isbn = {978-3-540-29244-9},
	url = {http://www.springerlink.com/content/f34573122k388046/abstract/},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Weka4WS},
	pages = {309--320},
	booktitle = {Knowledge Discovery in Databases: {PKDD} 2005},
	publisher = {Springer Berlin / Heidelberg},
	author = {Talia, Domenico and Trunfio, Paolo and Verta, Oreste},
	editor = {Jorge, Alípio and Torgo, Luís and Brazdil, Pavel and Camacho, Rui and Gama, João},
	urldate = {2012-11-09},
	date = {2005},
	%note = {Cited by 0088},
	keywords = {Computer Science},
	file = {SpringerLink Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\B3IZM56R\\Talia et al. - 2005 - Weka4WS A WSRF-Enabled Weka Toolkit for Distribut.html:text/html}
}

@online{_preference_????-1,
	title = {Preference Learning: An Introduction},
	url = {http://www.mathematik.uni-marburg.de/%7Eeyke/publications/intro.pdf},
	urldate = {2014-05-23},
	%note = {00002},
	file = {intro.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8ZHT7Q9E\\intro.pdf:application/pdf}
}

@online{_scp13.pdf_2013,
	title = {{SCP}13.pdf},
	url = {http://www.win.tue.nl/mdse/conferences/SCP13.pdf},
	urldate = {2013-07-08},
	date = {2013-07-08},
	%note = {Cited by 0000}
}

@inproceedings{thung_empirical_2012,
	title = {An Empirical Study of Bugs in Machine Learning Systems},
	doi = {10.1109/ISSRE.2012.22},
	abstract = {Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and {OpenNLP}, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6\% of the bugs belong to the algorithm/method category, 15.6\% of the bugs belong to the non-functional category, and 13\% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.},
	pages = {271--280},
	booktitle = {2012 {IEEE} 23rd International Symposium on Software Reliability Engineering ({ISSRE})},
	author = {Thung, F. and Wang, Shaowei and Lo, D. and Jiang, Lingxiao},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {algorithm-intensive code, algorithm-intensive code, algorithm-intensive nature, algorithm-intensive nature, Apache Mahout, Apache Mahout, bug categories, bug categories, bug databases, bug databases, code repositories, code repositories, data mining, data mining, information retrieval, information retrieval, Internet advertising systems, Internet advertising systems, learning (artificial intelligence), learning (artificial intelligence), Lucene, Lucene, machine learning systems, machine learning systems, natural language processing, natural language processing, natural language processing code, natural language processing code, {OpenNLP}, {OpenNLP}, program debugging, program debugging, recommendation systems, recommendation systems, Search engines, Search engines, software engineering tasks, software engineering tasks, Software mining, Software mining, Software reliability, Software reliability, system reliability, system reliability}
}

@inproceedings{richardson_introducing_2008,
	title = {Introducing data mining techniques and software engineering to high school science students},
	doi = {10.1109/FIE.2008.4720456},
	abstract = {This paper describes the activities of a Computer Science doctoral student and a Secondary Education masters student in the design, development, and implementation of a lesson for a high school science class. The graduate students, called Fellows, worked in secondary classrooms in the Cincinnati Public School District as a part of Project {STEP} at the University of Cincinnati, which is funded by the National Science Foundation {GK}-12 Program. The Fellows formed partnerships with secondary math and science teachers to generate new lessons, activities, and resources to enhance the {STEM} skills of high school students. Additionally, the Fellows used their engineering expertise to bring authentic, inquiry-based learning experiences into the classroom and introduced engineering concepts to underserved student populations. This paper highlights a lesson that integrates data mining and software engineering into a physical science lesson that focuses on the periodic table. Included in the paper are techniques used by the Fellows, reactions and feedback from the students, and observations and reflections by the Fellows regarding aspects of the activity that had the most impact on student learning.},
	pages = {F2D--1 --F2D--6},
	author = {Richardson, B.D. and Davis, K.C. and Beach, M.D.},
	date = {2008-10},
	%note = {Cited by 0002},
	keywords = {Computer Science doctoral student, computer science education, data mining, data mining techniques, high school science students, periodic table, physical science lesson, Secondary Education masters student, software engineering, teaching}
}

@inproceedings{gousios_exploratory_2014,
	location = {New York, {NY}, {USA}},
	title = {An Exploratory Study of the Pull-based Software Development Model},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568260},
	doi = {10.1145/2568225.2568260},
	series = {{ICSE}},
	abstract = {The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workflow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, first on the {GHTorrent} corpus and then on a carefully selected sample of 291 projects. We find that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and find that technical ones are only a small minority.},
	pages = {345--355},
	booktitle = {Proceedings of the 36th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
	urldate = {2014-06-06},
	year = {2014},
	keywords = {distributed software development, empirical software engineering, pull-based development, pull request},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\BBVJM7WD\\Gousios et al. - 2014 - An Exploratory Study of the Pull-based Software De.pdf:application/pdf}
}

@inproceedings{joshi_local_2007,
	title = {Local and Global Recency Weighting Approach to Bug Prediction},
	doi = {10.1109/MSR.2007.17},
	abstract = {Finding and fixing software bugs is a challenging maintenance task, and a significant amount of effort is invested by software development companies on this issue. In this paper, we use the Eclipse project's recorded software bug history to predict occurrence of future bugs. The history contains information on when bugs have been reported and subsequently fixed.},
	pages = {33--33},
	booktitle = {Fourth International Workshop on Mining Software Repositories, 2007. {ICSE} Workshops {MSR} '07},
	author = {Joshi, H. and Zhang, Chuanlei and Ramaswamy, S. and Bayrak, C.},
	date = {2007},
	%note = {Cited by 0013},
	keywords = {Computer bugs, Computer bugs, Computer Science, Computer Science, Eclipse project, Eclipse project, Financial management, Financial management, History, History, local/global recency weighting approach, local/global recency weighting approach, Predictive models, Predictive models, program debugging, program debugging, Programming, Programming, project management, project management, Quality management, Quality management, Software debugging, Software debugging, software development, software development, software development management, software development management, software maintenance, software maintenance, software project bug prediction, software project bug prediction}
}

@article{talia_weka4ws_2008,
	title = {The Weka4WS framework for distributed data mining in service-oriented Grids},
	volume = {20},
	rights = {Copyright © 2008 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {http://onlinelibrary.wiley.com.ez12.periodicos.capes.gov.br/doi/10.1002/cpe.1311/abstract},
	doi = {10.1002/cpe.1311},
	abstract = {The service-oriented architecture paradigm can be exploited for the implementation of data and knowledge-based applications in distributed environments. The Web services resource framework ({WSRF}) has recently emerged as the standard for the implementation of Grid services and applications. {WSRF} can be exploited for developing high-level services for distributed data mining applications. This paper describes Weka4WS, a framework that extends the widely used open source Weka toolkit to support distributed data mining on {WSRF}-enabled Grids. Weka4WS adopts the {WSRF} technology for running remote data mining algorithms and managing distributed computations. The Weka4WS user interface supports the execution of both local and remote data mining tasks. On every computing node, a {WSRF}-compliant Web service is used to expose all the data mining algorithms provided by the Weka library. The paper describes the design and implementation of Weka4WS using the {WSRF} libraries and services provided by Globus Toolkit 4. A performance analysis of Weka4WS for executing distributed data mining tasks in different network scenarios is presented. Copyright © 2008 John Wiley \& Sons, Ltd.},
	pages = {1933--1951},
	number = {16},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Talia, Domenico and Trunfio, Paolo and Verta, Oreste},
	urldate = {2012-11-09},
	date = {2008},
	langid = {english},
	%note = {Cited by 0023},
	keywords = {distributed data mining, Grid computing, Web services resource framework},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\BBC3R34B\\Talia et al. - 2008 - The Weka4WS framework for distributed data mining .pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\WQKSE8UV\\Talia et al. - 2008 - The Weka4WS framework for distributed data mining .html:text/html}
}

@book{pohlert_pairwise_2014,
	title = {The Pairwise Multiple Comparison of Mean Ranks Package ({PMCMR})},
	author = {Pohlert, Thorsten},
	date = {2014},
	note = {R package},
	file = {PMCMR.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\27GTM473\\PMCMR.pdf:application/pdf}
}

@inproceedings{kim_dealing_2011,
	location = {New York, {NY}, {USA}},
	title = {Dealing with noise in defect prediction},
	isbn = {978-1-4503-0445-0},
	url = {http://doi.acm.org/10.1145/1985793.1985859},
	doi = {10.1145/1985793.1985859},
	series = {{ICSE} '11},
	abstract = {Many software defect prediction models have been built using historical defect data obtained by mining software repositories ({MSR}). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding {FP} (false positive) or {FN} (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20\%-35\% of both {FP} and {FN} noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved.},
	pages = {481â€“490},
	booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
	urldate = {2013-07-20},
	date = {2011},
	%note = {Cited by 0023},
	keywords = {buggy changes, buggy changes, buggy files, buggy files, data quality, data quality, defect prediction, defect prediction, noise resistance, noise resistance}
}

@article{hullermeier_label_2008,
	title = {Label ranking by learning pairwise preferences},
	volume = {172},
	issn = {0004-3702},
	url = {http://www.sciencedirect.com/science/article/pii/S000437020800101X},
	doi = {10.1016/j.artint.2008.08.002},
	abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison ({RPC}), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (weighted) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare {RPC} to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that {RPC} is superior in terms of computational efficiency, and at least competitive in terms of accuracy.},
	pages = {1897--1916},
	number = {16},
	journal = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Hüllermeier, Eyke and Fürnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
	urldate = {2014-05-23},
	date = {2008-11},
	%note = {00201},
	keywords = {Constraint classification, Pairwise classification, Preference learning, Ranking},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CXM3ZRZK\\Hüllermeier et al. - 2008 - Label ranking by learning pairwise preferences.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\J76H6AHB\\S000437020800101X.html:text/html}
}

@inproceedings{giger_can_2012,
	title = {Can we predict types of code changes? An empirical analysis},
	doi = {10.1109/MSR.2012.6224284},
	shorttitle = {Can we predict types of code changes?},
	abstract = {There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes ({SCC}) capture such detailed code changes and their semantics on the statement level. These {SCC} can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of {SCC}. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of {SCC} types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.},
	eventtitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	pages = {217--226},
	author = {Giger, E. and Pinzger, M. and Gall, H.C.},
	date = {2012},
	%note = {Cited by 0004},
	keywords = {Artificial neural networks, Azureus 3 project, change-prone file, change-proneness, Computational modeling, condition change, Correlation, Eclipse platform, fine-grained source code change, interface modification, learning (artificial intelligence), machine learning, Measurement, neural nets, neural network model, object-oriented methods, Object-oriented metrics, Object oriented modeling, prediction model, Predictive models, {SCC}, semantics, social network centrality measures, software maintenance, Software metrics, software quality, software system, source file, statement change, statement level, static source code dependency graph}
}

@inproceedings{kotsiantis_supervised_2007-1,
	location = {Amsterdam, The Netherlands, The Netherlands},
	title = {Supervised Machine Learning: A Review of Classification Techniques},
	isbn = {978-1-58603-780-2},
	url = {http://dl.acm.org/citation.cfm?id=1566770.1566773},
	shorttitle = {Supervised Machine Learning},
	abstract = {The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
	pages = {3--24},
	publisher = {{IOS} Press},
	author = {Kotsiantis, S. B.},
	urldate = {2013-07-05},
	date = {2007},
	%note = {Cited by 0524},
	keywords = {Classifiers, data mining, Intelligent Data Analysis, Learning Algorithms}
}

@article{bansiya_hierarchical_2002,
	title = {A Hierarchical Model for Object-Oriented Design Quality Assessment},
	author = {Bansiya, Jagdish and Davis, Carl G.},
	date = {2002},
	%note = {Cited by 0428}
}

@inproceedings{dambros_effective_2011,
	title = {Effective mining of software repositories},
	doi = {10.1109/ICSM.2011.6080839},
	abstract = {With the advent of open-source, the Internet, and the consequent widespread adoption of distributed development tools, such as software configuration management and issue tracking systems, a vast amount of valuable information concerning software development and evolution has become available. Mining Software Repositories ({MSR})-a very active and interest-growing research field-deals with retrieving and analyzing this data. Empirical analyses of software repositories allow researchers to validate assumptions previously based only on intuitions, as well as finding novel theories. In turn, these theories about the software development phenomenon have been translated into concrete approaches and tools that support software developers and managers in their daily tasks. In this tutorial, we provide an overview of the state of the art of {MSR}. In particular, we describe what software repositories are, what in turn Mining Software Repositories is, what techniques are available to researchers and practitioners, and finally, what the limitations of {MSR} are nowadays, and how to fix them.},
	pages = {598--598},
	booktitle = {2011 27th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {D'Ambros, M. and Robbes, R.},
	date = {2011},
	%note = {Cited by 0000},
	keywords = {configuration management, configuration management, data mining, data mining, distributed development tool, distributed development tool, Internet, Internet, issue tracking system, issue tracking system, mining software repositories, mining software repositories, {MSR}, {MSR}, software configuration management, software configuration management, software development, software development, software evolution, software evolution, software maintenance, software maintenance}
}

@article{williams_automatic_2005,
	title = {Automatic mining of source code repositories to improve bug finding techniques},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.63},
	abstract = {We describe a method to use the source code change history of a software project to drive and help to refine the search for bugs. Based on the data retrieved from the source code repository, we implement a static source code checker that searches for a commonly fixed bug and uses information automatically mined from the source code repository to refine its results. By applying our tool, we have identified a total of 178 warnings that are likely bugs in the Apache Web server source code and a total of 546 warnings that are likely bugs in Wine, an open-source implementation of the Windows {API}. We show that our technique is more effective than the same static analysis that does not use historical data from the source code repository.},
	pages = {466 -- 480},
	number = {6},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {Williams, C.C. and Hollingsworth, J.K.},
	date = {2005-06},
	%note = {Cited by 0127},
	keywords = {Apache Web server, Apache Web server, application program interfaces, application program interfaces, automatic mining, automatic mining, bug finding technique, bug finding technique, Computer bugs, Computer bugs, configuration control, configuration control, configuration management, configuration management, data mining, data mining, data retrieval, data retrieval, Debugging, Debugging, debugging aids, debugging aids, debugging aids., debugging aids., Detectors, Detectors, file servers, file servers, historical data, historical data, History, History, Index Terms- Testing tools, Index Terms- Testing tools, information retrieval, information retrieval, inspection, inspection, Internet, Internet, open-source implementation, open-source implementation, Open source software, Open source software, program debugging, program debugging, program diagnostics, program diagnostics, Programming profession, Programming profession, program testing, program testing, public domain software, public domain software, software project, software project, source code repository, source code repository, static analysis, static analysis, static source code checker, static source code checker, testing tools, testing tools, version control, version control, Web server, Web server, Windows {API}, Windows {API}}
}

@book{_data_????,
	title = {Data Mining and Knowledge Discovery Handbook},
	url = {http://www.springer.com/computer/database+management+%26+information+retrieval/book/978-0-387-09822-7},
	abstract = {Knowledge Discovery demonstrates intelligent computing at its best, and is the most desirable and interesting end-product of Information Technology. To be able to discover and to extract knowledge from data is a task that many ...},
	urldate = {2013-08-06},
	%note = {Cited by 0301},
	keywords = {Database Management, Database Management, Data Mining and Knowledge Discovery Handbook, Data Mining and Knowledge Discovery Handbook, Information Storage and Retrieval, Information Storage and Retrieval}
}

@inproceedings{tsay_influence_2014,
	location = {New York, {NY}, {USA}},
	title = {Influence of Social and Technical Factors for Evaluating Contribution in {GitHub}},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568315},
	doi = {10.1145/2568225.2568315},
	series = {{ICSE}},
	abstract = {Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as {GitHub} make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in {GitHub} that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in {GitHub}. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects.},
	pages = {356--366},
	booktitle = {Proceedings of the 36th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James},
	urldate = {2014-06-02},
	year = {2014},
	%note = {00000},
	keywords = {contribution, {GitHub}, open source, signaling theory, social computing, social media, transparency},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\KB26G4WW\\Tsay et al. - 2014 - Influence of Social and Technical Factors for Eval.pdf:application/pdf}
}

@inproceedings{nagappan_mining_2006,
	location = {New York, {NY}, {USA}},
	title = {Mining metrics to predict component failures},
	isbn = {1-59593-375-1},
	url = {http://doi.acm.org/10.1145/1134285.1134349},
	doi = {10.1145/1134285.1134349},
	series = {{ICSE} '06},
	abstract = {What is it that makes software fail? In an empirical study of the post-release defect history of five Microsoft software systems, we found that failure-prone software entities are statistically correlated with code complexity measures. However, there is no single set of complexity metrics that could act as a universally best defect predictor. Using principal component analysis on the code metrics, we built regression models that accurately predict the likelihood of post-release defects for new entities. The approach can easily be generalized to arbitrary projects; in particular, predictors obtained from one project can also be significant for new, similar projects.},
	pages = {452â€“461},
	booktitle = {Proceedings of the 28th international conference on Software engineering},
	publisher = {{ACM}},
	author = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
	urldate = {2012-12-14},
	date = {2006},
	%note = {Cited by 0378},
	keywords = {bug database, bug database, complexity metrics, complexity metrics, Empirical study, Empirical study, principal component analysis, principal component analysis, regression model, regression model}
}

@inproceedings{riaz_systematic_2009,
	location = {Washington, {DC}, {USA}},
	title = {A systematic review of software maintainability prediction and metrics},
	isbn = {978-1-4244-4842-5},
	url = {http://dx.doi.org/10.1109/ESEM.2009.5314233},
	doi = {10.1109/ESEM.2009.5314233},
	series = {{ESEM} '09},
	abstract = {This paper presents the results of a systematic review conducted to collect evidence on software maintainability prediction and metrics. The study was targeted at the software quality attribute of maintainability as opposed to the process of software maintenance. The evidence was gathered from the selected studies against a set of meaningful and focused questions. 710 studies were initially retrieved; however of these only 15 studies were selected; their quality was assessed; data extraction was performed; and data was synthesized against the research questions. Our results suggest that there is little evidence on the effectiveness of software maintainability prediction techniques and models.},
	pages = {367â€“377},
	booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
	publisher = {{IEEE} Computer Society},
	author = {Riaz, Mehwish and Mendes, Emilia and Tempero, Ewan},
	urldate = {2013-07-09},
	date = {2009},
	%note = {Cited by 0053}
}

@inproceedings{poncin_process_2011,
	title = {Process Mining Software Repositories},
	doi = {10.1109/CSMR.2011.5},
	abstract = {Software developers' activities are in general recorded in software repositories such as version control systems, bug trackers and mail archives. While abundant information is usually present in such repositories, successful information extraction is often challenged by the necessity to simultaneously analyze different repositories and to combine the information obtained. We propose to apply process mining techniques, originally developed for business process analysis, to address this challenge. However, in order for process mining to become applicable, different software repositories should be combined, and â€œrelatedâ€ software development events should be matched: e.g., mails sent about a file, modifications of the file and bug reports that can be traced back to it. The combination and matching of events has been implemented in {FRASR} (Framework for Analyzing Software Repositories), augmenting the process mining framework {ProM}. {FRASR} has been successfully applied in a series of case studies addressing such aspects of the development process as roles of different developers and the way bug reports are handled.},
	pages = {5--14},
	booktitle = {2011 15th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Poncin, W. and Serebrenik, A. and van den Brand, M.},
	date = {2011},
	%note = {Cited by 0015},
	keywords = {bug trackers, bug trackers, business process analysis, business process analysis, Computer bugs, Computer bugs, Control systems, Control systems, data mining, data mining, Framework for Analyzing Software Repositories, Framework for Analyzing Software Repositories, {FRASR}, {FRASR}, information extraction, information extraction, mail archives, mail archives, Postal services, Postal services, Process control, Process control, process mining, process mining, process mining framework, process mining framework, {PROM}, {PROM}, software, software, software development events, software development events, software engineering, software engineering, software repositories, software repositories, version control systems, version control systems}
}

@inproceedings{shin_does_2009,
	title = {Does calling structure information improve the accuracy of fault prediction?},
	doi = {10.1109/MSR.2009.5069481},
	abstract = {Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of an industrial software system, we investigate the effectiveness of adding information about calling structure to fault prediction models. The addition of calling structure information to a model based solely on non-calling structure code attributes provided noticeable improvement in prediction accuracy, but only marginally improved the best model based on history and non-calling structure code attributes. The best model based on history and non-calling structure code attributes outperformed the best model based on calling and non-calling structure code attributes.},
	pages = {61--70},
	booktitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	author = {Shin, Yonghee and Bell, R. and Ostrand, T. and Weyuker, E.},
	date = {2009},
	%note = {Cited by 0018},
	keywords = {Accuracy, Accuracy, calling structure information, calling structure information, Computer industry, Computer industry, Computer Science, Computer Science, Costs, Costs, fault diagnosis, fault diagnosis, fault prediction accuracy, fault prediction accuracy, History, History, history information, history information, industrial software system, industrial software system, Information analysis, Information analysis, noncalling structure code attribute, noncalling structure code attribute, Performance analysis, Performance analysis, Predictive models, Predictive models, software fault tolerance, software fault tolerance, software houses, software houses, Software systems, Software systems}
}

@inproceedings{tamrawi_fuzzy_2011,
	location = {New York, {NY}, {USA}},
	title = {Fuzzy set and cache-based approach for bug triaging},
	isbn = {978-1-4503-0443-6},
	url = {http://doi.acm.org/10.1145/2025113.2025163},
	doi = {10.1145/2025113.2025163},
	series = {{ESEC}/{FSE} '11},
	abstract = {Bug triaging aims to assign a bug to the most appropriate fixer. That task is crucial in reducing time and efforts in a bug fixing process. In this paper, we propose Bugzie, a novel approach for automatic bug triaging based on fuzzy set and cache-based modeling of the bug-fixing expertise of developers. Bugzie considers a software system to have multiple technical aspects, each of which is associated with technical terms. For each technical term, it uses a fuzzy set to represent the developers who are capable/competent of fixing the bugs relevant to the corresponding aspect. The fixing correlation of a developer toward a technical term is represented by his/her membership score toward the corresponding fuzzy set. The score is calculated based on the bug reports that (s)he has fixed, and is updated as the newly fixed bug reports are available. For a new bug report, Bugzie combines the fuzzy sets corresponding to its terms and ranks the developers based on their membership scores toward that combined fuzzy set to find the most capable fixers. Our empirical results show that Bugzie achieves significantly higher accuracy and time efficiency than existing state-of-the-art approaches.},
	pages = {365â€“375},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering},
	publisher = {{ACM}},
	author = {Tamrawi, Ahmed and Nguyen, Tung Thanh and Al-Kofahi, Jafar M. and Nguyen, Tien N.},
	urldate = {2013-10-21},
	date = {2011},
	%note = {Cited by 0015},
	keywords = {bug triaging, bug triaging, developers' expertise, developers' expertise, fuzzy set, fuzzy set}
}

@article{basili_validation_1996,
	title = {A validation of object-oriented design metrics as quality indicators},
	volume = {22},
	issn = {0098-5589},
	doi = {10.1109/32.544352},
	abstract = {This paper presents the results of a study in which we empirically investigated the suite of object-oriented ({OO}) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known {OO} analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these {OO} metrics are discussed. Several of Chidamber and Kemerer's {OO} metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than â€œtraditionalâ€ code metrics, which can only be collected at a later phase of the software development processes},
	pages = {751--761},
	number = {10},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Basili, V.R. and Briand, L.C. and Melo, W.L.},
	date = {1996},
	%note = {Cited by 1204},
	keywords = {C++ programming language, C++ programming language, C language, C language, class maintenance changes, class maintenance changes, Computer languages, Computer languages, Costs, Costs, data set, data set, Design methodology, Design methodology, fault-prone classes, fault-prone classes, information management systems, information management systems, information systems, information systems, metric validation, metric validation, object oriented analysis, object oriented analysis, object-oriented design metrics, object-oriented design metrics, object-oriented languages, object-oriented languages, object-oriented methods, object-oriented methods, Object oriented modeling, Object oriented modeling, Predictive models, Predictive models, Programming, Programming, Resource management, Resource management, sequential life cycle model, sequential life cycle model, software development, software development, software maintenance, software maintenance, Software metrics, Software metrics, software quality, software quality, software quality indicators, software quality indicators, Software systems, Software systems, System testing, System testing}
}

@article{cavalcanti_challenges_2014,
	title = {Challenges and opportunities for software change request repositories: a systematic mapping study},
	volume = {26},
	rights = {Copyright © 2013 John Wiley \& Sons, Ltd.},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.1639/abstract},
	doi = {10.1002/smr.1639},
	shorttitle = {Challenges and opportunities for software change request repositories},
	abstract = {Software maintenance starts as soon as the first artifacts are delivered and is essential for the success of the software. However, keeping maintenance activities and their related artifacts on track comes at a high cost. In this respect, change request ({CR}) repositories are fundamental in software maintenance. They facilitate the management of {CRs} and are also the central point to coordinate activities and communication among stakeholders. However, the benefits of {CR} repositories do not come without issues, and commonly occurring ones should be dealt with, such as the following: duplicate {CRs}, the large number of {CRs} to assign, or poorly described {CRs}. Such issues have led researchers to an increased interest in investigating {CR} repositories, by considering different aspects of software development and {CR} management. In this paper, we performed a systematic mapping study to characterize this research field. We analyzed 142 studies, which we classified in two ways. First, we classified the studies into different topics and grouped them into two dimensions: challenges and opportunities. Second, the challenge topics were classified in accordance with an existing taxonomy for information retrieval models. In addition, we investigated tools and services for {CR} management, to understand whether and how they addressed the topics identified. Copyright © 2013 John Wiley \& Sons, Ltd.},
	pages = {620--653},
	number = {7},
	journal = {Journal of Software: Evolution and Process},
	shortjournal = {J. Softw. Evol. and Proc.},
	author = {Cavalcanti, Yguaratã Cerqueira and others},
	urldate = {2014-10-02},
	year = {2014},
	langid = {english},
	keywords = {bug report, bug tracking, change request repository, software evolution, software maintenance, software quality assurance},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\96MF9NI6\\Cavalcanti et al. - 2014 - Challenges and opportunities for software change r.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\W77MRPTJ\\abstract.html:text/html}
}

@inproceedings{gousios_ghtorent_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {The {GHTorent} dataset and tool suite},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487132},
	series = {{MSR}},
	abstract = {During the last few years, {GitHub} has emerged as a popular project hosting, mirroring and collaboration platform. {GitHub} provides an extensive {REST} {API}, which enables researchers to retrieve high-quality, interconnected data. The {GHTorent} project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.},
	pages = {233--236},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE}},
	author = {Gousios, Georgios},
	urldate = {2013-10-17},
	date = {2013},
	year = {2013},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2JTRMBD6\\.zotero-ft-cache:application/pdf;ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\7V7Q2PR9\\Gousios - 2013 - The GHTorent Dataset and Tool Suite.pdf:application/pdf}
}

@inproceedings{buse_analytics_2010,
	location = {New York, {NY}, {USA}},
	title = {Analytics for software development},
	isbn = {978-1-4503-0427-6},
	url = {http://doi.acm.org/10.1145/1882362.1882379},
	doi = {10.1145/1882362.1882379},
	series = {{FoSER} '10},
	abstract = {Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.},
	pages = {77â€“80},
	booktitle = {Proceedings of the {FSE}/{SDP} workshop on Future of software engineering research},
	publisher = {{ACM}},
	author = {Buse, Raymond P.L. and Zimmermann, Thomas},
	urldate = {2013-03-28},
	date = {2010},
	%note = {Cited by 0008},
	keywords = {analytics, analytics, project management, project management}
}

@inproceedings{dantas_mining_2007,
	location = {João Pessoa, Brazil},
	title = {Mining Change Traces from Versioned {UML} Repositories},
	abstract = {As software evolves, analysis and design models should be modi-
fied, correspondingly. In this scenario, one of the main problems is to detect 
which elements should be changed due to a given change. This paper presents 
an approach that applies data mining over a versioned {UML} repository in or-
der to detect change traces among model elements at different abstraction lev-
els. These traces are presented to the software engineer together with context 
information obtained from {SCM} systems and used within {CASE} tools. We also 
present the results of a retrieval performance evaluation of our approach, 
which are promising.},
	eventtitle = {Brazilian Symposium on Software Engineering ({SBES})},
	pages = {236--252 p},
	author = {Dantas, Cristine and Murta, Leonardo and Werner, Cláudia},
	date = {2007},
	%note = {Cited by 0006}
}

@inproceedings{lamkanfi_filtering_2012,
	title = {Filtering Bug Reports for Fix-Time Analysis},
	doi = {10.1109/CSMR.2012.47},
	abstract = {Several studies have experimented with data mining algorithms to predict the fix-time of reported bugs. Unfortunately, the fix-times as reported in typical open-source cases are heavily skewed with a significant amount of reports registering fix-times less than a few minutes. Consequently, we propose to include an additional filtering step to improve the quality of the underlying data in order to gain better results. Using a small-scale replication of a previously published bug fix-time prediction experiment, we show that the additional filtering of reported bugs indeed improves the outcome of the results.},
	eventtitle = {2012 16th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {379--384},
	author = {Lamkanfi, A. and Demeyer, S.},
	date = {2012},
	%note = {Cited by 0003},
	keywords = {Accuracy, bug fix-time prediction experiment, bug report filtering, bug reports, Computer bugs, data analysis, data mining, data mining algorithm, data quality, Fires, fix-time, fix-time analysis, Open source software, preprocessing, program debugging, software engineering, Software systems}
}

@inproceedings{hassan_software_2010,
	location = {New York, {NY}, {USA}},
	title = {Software intelligence: the future of mining software engineering data},
	isbn = {978-1-4503-0427-6},
	url = {http://doi.acm.org/10.1145/1882362.1882397},
	doi = {10.1145/1882362.1882397},
	series = {{FoSER} '10},
	shorttitle = {Software intelligence},
	abstract = {Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence ({SI}) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name {SI} as an inspiration from the Business Intelligence ({BI}) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, {SI} offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. {SI} should support decision-making processes throughout the lifetime of a software system not just during its development phase. The vision of {SI} has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories ({MSR}) field show great promise and provide strong support for realizing {SI} in the near future. This position paper summarizes the state of practice and research of {SI}, and lays out future research directions for mining software engineering data to enable {SI}.},
	pages = {161â€“166},
	booktitle = {Proceedings of the {FSE}/{SDP} workshop on Future of software engineering research},
	publisher = {{ACM}},
	author = {Hassan, Ahmed E. and Xie, Tao},
	urldate = {2013-04-09},
	date = {2010},
	%note = {Cited by 0020},
	keywords = {mining software engineering data, mining software engineering data, mining software repositories, mining software repositories, software intelligence, software intelligence}
}

@article{baldi_assessing_2000,
	title = {Assessing the accuracy of prediction algorithms for classification: an overview},
	volume = {16},
	issn = {1367-4803, 1460-2059},
	url = {http://bioinformatics.oxfordjournals.org/content/16/5/412},
	doi = {10.1093/bioinformatics/16.5.412},
	shorttitle = {Assessing the accuracy of prediction algorithms for classification},
	abstract = {We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, and correlation coefficients, and to information theoretic measures such as relative entropy and mutual information. We briefly discuss the advantages and disadvantages of each approach. For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient. We observe and prove several results relating sensitivity and specificity of optimal systems. While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction.
Contact: pfbaldi@ics.uci.edu},
	pages = {412--424},
	number = {5},
	journal = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Baldi, Pierre and Brunak, Søren and Chauvin, Yves and Andersen, Claus A. F. and Nielsen, Henrik},
	urldate = {2013-07-16},
	date = {2000-01-05},
	langid = {english},
	%note = {Cited by 0983},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\BSUUFXT2\\Baldi et al. - 2000 - Assessing the accuracy of prediction algorithms fo.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\6TI69HWC\\412.html:text/html}
}

@inproceedings{antoniol_is_2008,
	location = {New York, {NY}, {USA}},
	title = {Is it a bug or an enhancement?: a text-based approach to classify change requests},
	url = {http://doi.acm.org/10.1145/1463788.1463819},
	doi = {10.1145/1463788.1463819},
	series = {{CASCON} '08},
	shorttitle = {Is it a bug or an enhancement?},
	abstract = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as "bug" for lack of a better classification support or of knowledge about the possible kinds. This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities. We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and {JBoss} indicate that issues can be classified with between 77\% and 82\% of correct decisions.},
	pages = {23:304--23:318},
	booktitle = {Proceedings of the 2008 conference of the center for advanced studies on collaborative research: meeting of minds},
	publisher = {{ACM}},
	author = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Guéhéneuc, Yann-Gaël},
	urldate = {2013-08-03},
	date = {2008},
	%note = {Cited by 0072},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\3XIB7MU4\\Antoniol et al. - 2008 - Is it a bug or an enhancement a text-based appro.pdf:application/pdf}
}

@article{mockus_predicting_2000,
	title = {Predicting risk of software changes},
	volume = {5},
	rights = {Copyright Â© 2000 Lucent Technologies Inc.},
	issn = {1538-7305},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/bltj.2229/abstract},
	doi = {10.1002/bltj.2229},
	abstract = {Reducing the number of software failures is one of the most challenging problems of software production. We assume that software development proceeds as a series of changes and model the probability that a change to software will cause a failure. We use predictors based on the properties of a change itself. Such predictors include size in lines of code added, deleted, and unmodified; diffusion of the change and its component subchanges, as reflected in the number of files, modules, and subsystems touched, or changed; several measures of developer experience; and the type of change and its subchanges (fault fixes or new code). The model is built on historic information and is used to predict the risk of new changes. In this paper we apply the model to 5ESSÂ® software updates and find that change diffusion and developer experience are essential to predicting failures. The predictive model is implemented as a Web-based tool to allow timely prediction of change quality. The ability to predict the quality of change enables us to make appropriate decisions regarding inspection, testing, and delivery. Historic information on software changes is recorded in many commercial software projects, suggesting that our results can be easily and widely applied in practice.},
	pages = {169â€“180},
	number = {2},
	journal = {Bell Labs Technical Journal},
	author = {Mockus, Audris and Weiss, David M.},
	urldate = {2013-03-14},
	date = {2000},
	langid = {english},
	%note = {Cited by 0148}
}

@inproceedings{estublier_software_2000,
	location = {New York, {NY}, {USA}},
	title = {Software configuration management: a roadmap},
	isbn = {1-58113-253-0},
	url = {http://doi.acm.org/10.1145/336512.336576},
	doi = {10.1145/336512.336576},
	series = {{ICSE} '00},
	shorttitle = {Software configuration management},
	pages = {279--289},
	booktitle = {Proceedings of the Conference on The Future of Software Engineering},
	publisher = {{ACM}},
	author = {Estublier, Jacky},
	urldate = {2012-05-30},
	date = {2000},
	%note = {Cited by 0220},
	keywords = {architecture, concurrent engineering, federation, interoperability, process support, software configuration management, version control},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\RX5XC4SU\\Estublier - 2000 - Software configuration management a roadmap.pdf:application/pdf;estublier2000.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\T88EPJ45\\estublier2000.pdf:application/pdf}
}

@article{gonzalez-barahona_are_2011,
	title = {Are Developers Fixing Their Own Bugs?: Tracing Bug-Fixing and Bug-Seeding Committers},
	volume = {3},
	issn = {1942-3926},
	url = {http://dx.doi.org/10.4018/jossp.2011040102},
	doi = {10.4018/jossp.2011040102},
	shorttitle = {Are Developers Fixing Their Own Bugs?},
	abstract = {The process of fixing software bugs plays a key role in the maintenance activities of a software project. Ideally, code ownership and responsibility should be enforced among developers working on the same artifacts, so that those introducing buggy code could also contribute to its fix. However, especially in {FLOSS} projects, this mechanism is not clearly understood: in particular, it is not known whether those contributors fixing a bug are the same introducing and seeding it in the first place. This paper analyzes the comm-central {FLOSS} project, which hosts part of the Thunderbird, {SeaMonkey}, Lightning extensions and Sunbird projects from the Mozilla community. The analysis is focused at the level of lines of code and it uses the information stored in the source code management system. The results of this study show that in 80\% of the cases, the bug-fixing activity involves source code modified by at most two developers. It also emerges that the developers fixing the bug are only responsible for 3.5\% of the previous modifications to the lines affected; this implies that the other developers making changes to those lines could have made that fix. In most of the cases the bug fixing process in comm-central is not carried out by the same developers than those who seeded the buggy code.},
	pages = {23â€“42},
	number = {2},
	journal = {Int. J. Open Source Softw. Process.},
	author = {Gonzalez-Barahona, Jesus M.. and Izquierdo-Cortazar, Daniel and Capiluppi, Andrea},
	urldate = {2013-10-16},
	date = {2011},
	%note = {Cited by 0002},
	keywords = {Bug-Fixing, Bug-Fixing, buggy code, buggy code, Bug-Seeding, Bug-Seeding, {FLOSS} Projects, {FLOSS} Projects, Open Source Software {OSS}, Open Source Software {OSS}}
}

@article{ngai_application_2011,
	title = {The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature},
	volume = {50},
	issn = {0167-9236},
	url = {http://www.sciencedirect.com/science/article/pii/S0167923610001302},
	doi = {10.1016/j.dss.2010.08.006},
	shorttitle = {On quantitative methods for detection of financial fraud},
	abstract = {This paper presents a review of — and classification scheme for — the literature on the application of data mining techniques for the detection of financial fraud. Although financial fraud detection ({FFD}) is an emerging topic of great importance, a comprehensive literature review of the subject has yet to be carried out. This paper thus represents the first systematic, identifiable and comprehensive academic literature review of the data mining techniques that have been applied to {FFD}. 49 journal articles on the subject published between 1997 and 2008 was analyzed and classified into four categories of financial fraud (bank fraud, insurance fraud, securities and commodities fraud, and other related financial fraud) and six classes of data mining techniques (classification, regression, clustering, prediction, outlier detection, and visualization). The findings of this review clearly show that data mining techniques have been applied most extensively to the detection of insurance fraud, although corporate fraud and credit card fraud have also attracted a great deal of attention in recent years. In contrast, we find a distinct lack of research on mortgage fraud, money laundering, and securities and commodities fraud. The main data mining techniques used for {FFD} are logistic models, neural networks, the Bayesian belief network, and decision trees, all of which provide primary solutions to the problems inherent in the detection and classification of fraudulent data. This paper also addresses the gaps between {FFD} and the needs of the industry to encourage additional research on neglected topics, and concludes with several suggestions for further {FFD} research.},
	pages = {559--569},
	number = {3},
	journal = {Decision Support Systems},
	shortjournal = {Decision Support Systems},
	author = {Ngai, E.W.T. and Hu, Yong and Wong, Y.H. and Chen, Yijun and Sun, Xin},
	urldate = {2013-07-05},
	date = {2011-02},
	%note = {Cited by 0043},
	keywords = {Business intelligence, data mining, Financial fraud, Fraud detection, Literature review},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\SAWGVCN3\\Ngai et al. - 2011 - The application of data mining techniques in finan.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4A6CZING\\Ngai et al. - 2011 - The application of data mining techniques in finan.html:text/html}
}

@article{hindle_automated_2013,
	title = {Automated topic naming},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-012-9209-9},
	doi = {10.1007/s10664-012-9209-9},
	abstract = {Software repositories provide a deluge of software artifacts to analyze. Researchers have attempted to summarize, categorize, and relate these artifacts by using semi-unsupervised machine-learning algorithms, such as Latent Dirichlet Allocation ({LDA}). {LDA} is used for concept and topic analysis to suggest candidate word-lists or topics that describe and relate software artifacts. However, these word-lists and topics are difficult to interpret in the absence of meaningful summary labels. Current attempts to interpret topics assume manual labelling and do not use domain-specific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: automated labelled topic extraction. Topics are extracted using {LDA} from commit-log comments recovered from source control systems. These topics are given labels from a generalizable cross-project taxonomy, consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on three large-scale Relational Database Management System ({RDBMS}) projects: {MySQL}, {PostgreSQL} and {MaxDB}. The case studies show that labelled topic extraction can produce appropriate, context-sensitive labels that are relevant to these projects, and provide fresh insight into their evolving software development activities.},
	pages = {1125--1155},
	number = {6},
	journal = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Hindle, Abram and Ernst, Neil A. and Godfrey, Michael W. and Mylopoulos, John},
	urldate = {2013-10-17},
	date = {2013-12-01},
	langid = {english},
	%note = {Cited by 0000},
	keywords = {Latent Dirichlet allocation, Programming Languages, Compilers, Interpreters, Repository Mining, Software Engineering/Programming and Operating Systems, software maintenance, topic models}
}
@article{hall_weka_2009,
  abstract = {More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.},
  added-at = {2010-05-11T22:12:16.000+0200},
  address = {New York, NY, USA},
  author = {Hall, Mark and others},
  biburl = {http://www.bibsonomy.org/bibtex/2e14c89c0481b3d49ae7c3c9e8dc6b67a/ans},
  doi = {10.1145/1656274.1656278},
  interhash = {7503381e49f45cde7ecf66f715bd37c7},
  intrahash = {e14c89c0481b3d49ae7c3c9e8dc6b67a},
  issn = {1931-0145},
  journal = {SIGKDD Explor. Newsl.},
  keywords = {data master mining toVerify tools weka},
  number = 1,
  pages = {10--18},
  publisher = {ACM},
  timestamp = {2010-05-11T22:12:16.000+0200},
  title = {The WEKA data mining software: an update},
  url = {http://dx.doi.org/10.1145/1656274.1656278},
  volume = 11,
  year = 2009,
  url = {http://doi.acm.org/10.1145/1656274.1656278},
  urldate = {2012-11-09},
  date = {2009-11},
  file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\D6ZVWD8B\\Hall et al. - 2009 - The WEKA data mining software an update.pdf:application/pdf}
}

@article{daambros_evaluating_2012,
	title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
	volume = {17},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-011-9173-9},
	doi = {10.1007/s10664-011-9173-9},
	shorttitle = {Evaluating defect prediction approaches},
	abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
	pages = {531--577},
	number = {4},
	journal = {Empirical Software Engineering},
	author = {Dâ€™Ambros, Marco and Lanza, Michele and Robbes, Romain},
	urldate = {2013-07-30},
	date = {2012-08},
	langid = {english},
	%note = {Cited by 0031},
	keywords = {change metrics, change metrics, Compilers, defect prediction, defect prediction, Interpreters, programming languages, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, Software Engineering/Programming and Operating Systems, source code metrics, source code metrics}
}

@article{livshits_dynamine:_2005,
	title = {{DynaMine}: finding common error patterns by mining software revision histories},
	volume = {30},
	issn = {0163-5948},
	url = {http://doi.acm.org/10.1145/1095430.1081754},
	doi = {10.1145/1095430.1081754},
	shorttitle = {{DynaMine}},
	abstract = {A great deal of attention has lately been given to addressing software bugs such as errors in operating system drivers or security bugs. However, there are many other lesser known errors specific to individual applications or {APIs} and these violations of application-specific coding rules are responsible for a multitude of errors. In this paper we propose {DynaMine}, a tool that analyzes source code check-ins to find highly correlated method calls as well as common bug fixes in order to automatically discover application-specific coding patterns. Potential patterns discovered through mining are passed to a dynamic analysis tool for validation; finally, the results of dynamic analysis are presented to the user.The combination of revision history mining and dynamic analysis techniques leveraged in {DynaMine} proves effective for both discovering new application-specific patterns and for finding errors when applied to very large applications with many man-years of development and debugging effort behind them. We have analyzed Eclipse and {jEdit}, two widely-used, mature, highly extensible applications consisting of more than 3,600,000 lines of code combined. By mining revision histories, we have discovered 56 previously unknown, highly application-specific patterns. Out of these, 21 were dynamically confirmed as very likely valid patterns and a total of 263 pattern violations were found.},
	pages = {296â€“305},
	number = {5},
	journal = {{SIGSOFT} Softw. Eng. Notes},
	author = {Livshits, Benjamin and Zimmermann, Thomas},
	urldate = {2013-02-27},
	date = {2005-09},
	%note = {Cited by 0219},
	keywords = {coding patterns, coding patterns, data mining, data mining, dynamic analysis, dynamic analysis, error patterns, error patterns, one-line check-ins, one-line check-ins, revision histories, revision histories, software bugs, software bugs}
}

@inproceedings{pham_creating_2013,
	title = {Creating a shared understanding of testing culture on a social coding site},
	doi = {10.1109/ICSE.2013.6606557},
	abstract = {Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as {GitHub} provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of {GitHub} to investigate how the increased transparency found on {GitHub} influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of {GitHub}. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on {GitHub} may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects.},
	eventtitle = {2013 35th International Conference on Software Engineering ({ICSE})},
	pages = {112--121},
	booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
	author = {Pham, R. and Singer, L. and Liskin, O. and Figueira Filho, F. and Schneider, K.},
	date = {2013-05},
	keywords = {collaborative environment, Encoding, {GitHub}, groupware, Guidelines, Interviews, Media, online questionnaire, program testing, social coding site, social networking (online), social transparency, Sociology, software, software development projects, software quality, sustainable testing culture, Testing},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\MCC74F7V\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\6BDGGGGF\\Pham et al. - 2013 - Creating a shared understanding of testing culture.pdf:application/pdf}
}

@inproceedings{yadav_predicting_2013,
	title = {Predicting Design Quality of Object-Oriented Software using {UML} diagrams},
	doi = {10.1109/IAdCC.2013.6514442},
	abstract = {Assessment of Object Oriented Software Design Quality has been an important issue among researchers in Software Engineering discipline. In this paper, we propose an approach for determining the design quality of Object Oriented Software System. The approach makes use of a set of {UML} diagrams created during the design phase of the development process. Design metrics are fetched from the {UML} diagrams using a parser developed by us and design quality is assessed using a Hierarchical Model of Software Design Quality. To validate the design quality, we compute the product quality for the same software that corresponds to the {UML} design diagrams using available tools {METRIC} 1.3.4, {JHAWK} and Team In a Box. The objective is to establish a correspondence between design quality and product quality of Object Oriented Software. For this purpose, we have chosen priory known three software of Low, Medium and High quality. This is a work under progress, though; the substantial task has already been completed.},
	pages = {1462--1467},
	booktitle = {Advance Computing Conference ({IACC}), 2013 {IEEE} 3rd International},
	author = {Yadav, V. and Singh, R.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {design metrics, design metrics, grammars, grammars, Hierarchical Model, Hierarchical Model, hierarchical model of software design quality, hierarchical model of software design quality, {JHAWK} tool, {JHAWK} tool, {METRIC} 1.3.4 tool, {METRIC} 1.3.4 tool, object-oriented programming, object-oriented programming, object oriented software design quality assessment, object oriented software design quality assessment, Object Oriented Systems, Object Oriented Systems, parser, parser, product quality, product quality, Software Design Quality, Software Design Quality, software engineering, software engineering, Software metrics, Software metrics, Software Product Quality, Software Product Quality, software quality, software quality, software tools, software tools, Team In a Box tool, Team In a Box tool, {UML} design diagram, {UML} design diagram, {UML} Diagrams, {UML} Diagrams, Unified Modeling Language, Unified Modeling Language}
}

@article{strate_literature_2013,
	title = {A Literature Review of Research in Software Defect Reporting},
	volume = {62},
	issn = {0018-9529},
	doi = {10.1109/TR.2013.2259204},
	abstract = {In 2002, the National Institute of Standards and Technology ({NIST}) estimated that software defects cost the U.S. economy in the area of \$60 billion a year. It is well known that identifying and tracking these defects efficiently has a measurable impact on software reliability. In this work, we evaluate 104 academic papers on defect reporting published since the {NIST} report to 2012 to identify the most important advancements in improving software reliability though the efficient identification and tracking of software defects. We categorize the research into the areas of automatic defect detection, automatic defect fixing, attributes of defect reports, quality of defect reports, and triage of defect reports. We then summarize the most important work being done in each area. Finally, we provide conclusions on the current state of the literature, suggest tools and lessons learned from the research for practice, and comment on open research problems.},
	pages = {444--454},
	number = {2},
	journal = {{IEEE} Transactions on Reliability},
	author = {Strate, J.D. and Laplante, P.A.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {automatic defect detection, automatic defect detection, automatic defect fixing, automatic defect fixing, defect report attribute, defect report attribute, defect report quality, defect report quality, National Institute of Standards and Technology, National Institute of Standards and Technology, {NIST}, {NIST}, program testing, program testing, software defect identification, software defect identification, software defect reporting, software defect reporting, software defect tracking, software defect tracking, software fault tolerance, software fault tolerance, software quality, software quality, software quality assurance, software quality assurance, Software reliability, Software reliability, software test automation, software test automation, {US} economy, {US} economy}
}

@inproceedings{budimac_towards_2012,
	title = {Towards the Better Software Metrics Tool},
	doi = {10.1109/CSMR.2012.64},
	abstract = {Project "Towards the better software metrics tool" is a bilateral project between Slovenia and Serbia. It was supported by Ministries of Sciences of both countries. The main goal of this project was to detect the main difficulties in application of software metrics in practice and to deal with them by development of a more useful software metrics tool. Additional goal is establishment of origin for extension of this tool to support metric based software evolution. Motivation, goals and current results of the project were presented in several international conferences.},
	pages = {491--494},
	booktitle = {2012 16th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Budimac, Z. and Rakic, G. and {HeriÄ}ko, M. and Gerlec, C.},
	date = {2012},
	%note = {Cited by 0001},
	keywords = {Java, Java, Prototypes, Prototypes, software, software, software evolution, software evolution, software maintenance, software maintenance, Software metrics, Software metrics, software metrics tool, software metrics tool, software tools, software tools, Syntactics, Syntactics, syntax tree, syntax tree}
}

@inproceedings{giger_predicting_2010,
	location = {New York, {NY}, {USA}},
	title = {Predicting the fix time of bugs},
	isbn = {978-1-60558-974-9},
	url = {http://doi.acm.org/10.1145/1808920.1808933},
	doi = {10.1145/1808920.1808933},
	series = {{RSSE} '10},
	abstract = {Two important questions concerning the coordination of development effort are which bugs to fix first and how long it takes to fix them. In this paper we investigate empirically the relationships between bug report attributes and the time to fix. The objective is to compute prediction models that can be used to recommend whether a new bug should and will be fixed fast or will take more time for resolution. We examine in detail if attributes of a bug report can be used to build such a recommender system. We use decision tree analysis to compute and 10-fold cross validation to test prediction models. We explore prediction models in a series of empirical studies with bug report data of six systems of the three open source projects Eclipse, Mozilla, and Gnome. Results show that our models perform significantly better than random classification. For example, fast fixed Eclipse Platform bugs were classified correctly with a precision of 0.654 and a recall of 0.692. We also show that the inclusion of postsubmission bug report data of up to one month can further improve prediction models.},
	pages = {52â€“56},
	booktitle = {Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering},
	publisher = {{ACM}},
	author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald},
	urldate = {2013-09-12},
	date = {2010},
	%note = {Cited by 0032}
}

@inproceedings{tian_improved_2012,
	title = {Improved Duplicate Bug Report Identification},
	doi = {10.1109/CSMR.2012.48},
	abstract = {Bugs are prevalent in software systems. To improve the reliability of software systems, developers often allow end users to provide feedback on bugs that they encounter. Users could perform this by sending a bug report in a bug report management system like Bugzilla. This process however is uncoordinated and distributed, which means that many users could submit bug reports reporting the same problem. These are referred to as duplicate bug reports. The existence of many duplicate bug reports may cause much unnecessary manual efforts as often a triager would need to manually tag bug reports as being duplicates. Recently, there have been a number of studies that investigate duplicate bug report problem which in effect answer the following question: given a new bug report, retrieve k other similar bug reports. This, however, still requires substantive manual effort which could be reduced further. Jalbert and Weimer are the first to introduce the direct detection of duplicate bug reports, it answers the question: given a new bug report, classify if it as a duplicate bug report or not. In this paper, we extend Jalbert and Weimer's work by improving the accuracy of automated duplicate bug report identification. We experiments with bug reports from Mozilla bug tracking system which were reported between February 2005 to October 2005, and find that we could improve the accuracy of the previous approach by about 160\%.},
	pages = {385--390},
	booktitle = {2012 16th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Tian, Yuan and Sun, Chengnian and Lo, D.},
	date = {2012},
	%note = {Cited by 0006},
	keywords = {bug report management system, bug report management system, Bugzilla, Bugzilla, Computer bugs, Computer bugs, duplicate bug report identification, duplicate bug report identification, Duplicate bug reports, Duplicate bug reports, feedback, feedback, Frequency measurement, Frequency measurement, Harmonic analysis, Harmonic analysis, machine learning, machine learning, Mozilla bug tracking system, Mozilla bug tracking system, program debugging, program debugging, relative similarity, relative similarity, Reliability, Reliability, Software reliability, Software reliability, Software systems, Software systems, Training, Training, Training data, Training data}
}

@inproceedings{schroter_predicting_2006,
	location = {New York, {NY}, {USA}},
	title = {Predicting component failures at design time},
	isbn = {1-59593-218-6},
	url = {http://doi.acm.org/10.1145/1159733.1159739},
	doi = {10.1145/1159733.1159739},
	series = {{ISESE} '06},
	abstract = {How do design decisions impact the quality of the resulting software? In an empirical study of 52 {ECLIPSE} plug-ins, we found that the software design as well as past failure history, can be used to build models which accurately predict failure-prone components in new programs. Our prediction only requires usage relationships between components, which are typically defined in the design phase; thus, designers can easily explore and assess design alternatives in terms of predicted quality. In the {ECLIPSE} study, 90\% of the 5\% most failure-prone components, as predicted by our model from design data, turned out to actually produce failures later; a random guess would have predicted only 33\%.},
	pages = {18--27},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} international symposium on Empirical software engineering},
	publisher = {{ACM}},
	author = {Schröter, Adrian and Zimmermann, Thomas and Zeller, Andreas},
	urldate = {2013-02-21},
	date = {2006},
	%note = {Cited by 0101},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\F5MH2Z6D\\Schröter et al. - 2006 - Predicting component failures at design time.pdf:application/pdf}
}

@inproceedings{dit_dataset_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {A dataset from change history to support evaluation of software maintenance tasks},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org.ez24.periodicos.capes.gov.br/citation.cfm?id=2487085.2487114},
	series = {{MSR} '13},
	abstract = {Approaches that support software maintenance need to be evaluated and compared against existing ones, in order to demonstrate their usefulness in practice. However, oftentimes the lack of well-established sets of benchmarks leads to situations where these approaches are evaluated using different datasets, which results in biased comparisons. In this data paper we describe and make publicly available a set of benchmarks from six Java applications, which can be used in the evaluation of various software engineering ({SE}) tasks, such as feature location and impact analysis. These datasets consist of textual description of change requests, the locations in the source code where they were implemented, and execution traces. Four of the benchmarks were already used in several {SE} research papers, and two of them are new. In addition, we describe in detail the methodology used for generating these benchmarks and provide a suite of tools in order to encourage other researchers to validate our datasets and generate new benchmarks for other subject software systems. Our online appendix: http://www.cs.wm.edu/semeru/data/msr13/},
	pages = {131--134},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Dit, Bogdan and Holtzhauer, Andrew and Poshyvanyk, Denys and Kagdi, Huzefa},
	urldate = {2013-06-21},
	date = {2013},
	%note = {Cited by 0001},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\A9DEVE53\\Dit et al. - 2013 - A dataset from change history to support evaluatio.pdf:application/pdf}
}

@inproceedings{rodriguez_software_2012,
	title = {On software engineering repositories and their open problems},
	doi = {10.1109/RAISE.2012.6227971},
	abstract = {In the last decade, a large number of software repositories have been created for different purposes. In this paper we present a survey of the publicly available repositories and classify the most common ones as well as discussing the problems faced by researchers when applying machine learning or statistical techniques to them.},
	eventtitle = {2012 First International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering ({RAISE})},
	pages = {52--56},
	booktitle = {2012 First International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering ({RAISE})},
	author = {Rodriguez, D. and Herraiz, I. and Harrison, R.},
	date = {2012},
	%note = {Cited by 0005},
	keywords = {data mining, data quality, Estimation, information resources, learning (artificial intelligence), machine learning, Measurement, open problems, Open source software, preprocessing software engineering data, publicly available repositories, quality, Software algorithms, software engineering, software engineering repositories, statistical analysis, statistical techniques},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\AKJQM3HR\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DC55ZMN4\\Rodriguez et al. - 2012 - On software engineering repositories and their ope.pdf:application/pdf}
}

@article{kim_which_2011,
	title = {Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts},
	volume = {37},
	issn = {0098-5589},
	doi = {10.1109/TSE.2011.20},
	shorttitle = {Which Crashes Should I Fix First?},
	abstract = {Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these \#x201C;top crashes \#x201D; thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.},
	pages = {430 --447},
	number = {3},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Kim, Dongsun and Wang, Xinming and Kim, Sunghun and Zeller, A. and Cheung, S.C. and Park, Sooyong},
	date = {2011-06},
	%note = {Cited by 0015},
	keywords = {Computer bugs, Computer bugs, crash reports, crash reports, data mining., data mining., Debugging, Debugging, feature extraction, feature extraction, Firefox crash report databases, Firefox crash report databases, Fires, Fires, machine learning, machine learning, program debugging, program debugging, social network analysis, social network analysis, software, software, software failures, software failures, software maintenance, software maintenance, software quality, software quality, Software systems, Software systems, system recovery, system recovery, Testing, Testing, Thunderbird crash report databases, Thunderbird crash report databases, Top crash, Top crash, Training, Training}
}

@inproceedings{jongsawat_developing_2012,
	title = {Developing a Bayesian Network Model Based on a State and Transition Model for Software Defect Detection},
	doi = {10.1109/SNPD.2012.41},
	abstract = {This paper describes a Bayesian Network model-to diagnose the causes-effect of software defect detection in the process of software testing. The aim is to use the {BN} model to identify defective software modules for efficient software test in order to improve the quality of a software system. It can also be used as a decision tool to assist software developers to determine defect priority levels for each phase of a software development project. The {BN} tool can provide a cause-effect relationship between the software defects found in each phase and other factors affecting software defect detection in software testing. First, we build a State and Transition Model that is used to provide a simple framework for integrating knowledge about software defect detection and various factors. Second, we convert the State and Transition Model into a Bayesian Network model. Third, the probabilities for the {BN} model are determined through the knowledge of software experts and previous software development projects or phases. Last, we observe the interactions among the variables and allow for prediction of effects of external manipulation. We believe that both {STM} and {BN} models can be used as very practical tools for predicting software defects and reliability in varying software development lifecycles.},
	pages = {295--300},
	booktitle = {2012 13th {ACIS} International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel Distributed Computing ({SNPD})},
	author = {Jongsawat, N. and Premchaiswadi, W.},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {Adaptation models, Adaptation models, a State and Transition Model, a State and Transition Model, Bayesian Diagnosis, Bayesian Diagnosis, Bayesian methods, Bayesian methods, Bayesian Network, Bayesian Network, Bayesian network model, Bayesian network model, belief networks, belief networks, {BN} model, {BN} model, {BN} tool, {BN} tool, cause-effect relationship, cause-effect relationship, Computational modeling, Computational modeling, decision tool, decision tool, defective software module, defective software module, defect priority level, defect priority level, Evidence or Observations, Evidence or Observations, {GeNIe}, {GeNIe}, Predictive models, Predictive models, probability, probability, Product development, Product development, program testing, program testing, project management, project management, software, software, Software defect detection, Software defect detection, software development lifecycle, software development lifecycle, software development management, software development management, software development project, software development project, software diagnosis, software diagnosis, software expert, software expert, software quality, software quality, Software reliability, Software reliability, Software testing, Software testing, state and transition model, state and transition model, {STM}, {STM}, Testing, Testing}
}

@inproceedings{abdelmoez_bug_2012,
	title = {Bug fix-time prediction model using naive Bayes classifier},
	doi = {10.1109/ICCTA.2012.6523564},
	abstract = {Predicting bug fix-time is an important issue in order to assess the software quality or to estimate the time and effort needed during the bug triaging. Previous work has proposed several bug fix-time prediction models that had taken into consideration various bug report attributes (e.g. severity, number of developers, dependencies) in order to know which bug to fix first and how long it will take to fix it. Our aim is to distinguish the very fast and the very slow bugs in order to prioritize which bugs to start with and which to exclude at the mean time respectively. We used the data of four systems taken from three large open source projects Mozilla, Eclipse, Gnome. We used naïve Bayes classifier to compute our prediction model.},
	eventtitle = {2012 22nd International Conference on Computer Theory and Applications ({ICCTA})},
	pages = {167--172},
	author = {{AbdelMoez}, W. and Kholief, M. and Elsalmy, F.M.},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {Bayes methods, bug fix-time prediction model, bug triaging, Eclipse, {GNOME}, Mozilla, naïve Bayes classifier, pattern classification, program debugging, software quality, software quality assessment}
}

@inproceedings{petre_uml_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {{UML} in practice},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486883},
	series = {{ICSE} '13},
	abstract = {{UML} has been described by some as "the lingua franca" of software engineering. Evidence from industry does not necessarily support such endorsements. How exactly is {UML} being used in industry if it is? This paper presents a corpus of interviews with 50 professional software engineers in 50 companies and identifies 5 patterns of {UML} use.},
	pages = {722--731},
	booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Petre, Marian},
	urldate = {2013-07-08},
	date = {2013},
	%note = {Cited by 0000},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\G929R3K3\\Petre - 2013 - UML in practice.pdf:application/pdf}
}

@book{han_data_2011,
	location = {San Francisco, {CA}, {USA}},
	edition = {3rd},
	title = {Data Mining: Concepts and Techniques},
	isbn = {0123814790, 9780123814791},
	shorttitle = {Data Mining},
	abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
	year = {2011}
}

@report{furnkranz_pairwise_????,
	title = {Pairwise Preference Learning and Ranking},
	url = {http://www.mathematik.uni-marburg.de/~eyke/publications/mpub094.pdf},
	number = {Technical Report {OEFAI}-{TR}-2003-14},
	author = {Fürnkranz, Johannes and Hüllermeier, Eyke},
	urldate = {2014-05-23},
	%note = {00000},
	file = {mpub094.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\TXQIU9D5\\mpub094.pdf:application/pdf}
}

@inproceedings{gousios_lean_2014,
	location = {New York, {NY}, {USA}},
	title = {Lean {GHTorrent}: {GitHub} Data on Demand},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597126},
	doi = {10.1145/2597073.2597126},
	series = {{MSR} '14},
	shorttitle = {Lean {GHTorrent}},
	abstract = {In recent years, {GitHub} has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or {jQuery}) have chosen {GitHub} as their host and have migrated their code base to it. {GitHub} offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, {GitHub} data is, to date, largely underexplored. To facilitate studies of {GitHub}, we have created {GHTorrent}, a scalable, queriable, offline mirror of the data offered through the {GitHub} {REST} {API}. In this paper we present a novel feature of {GHTorrent} designed to offer customisable data dumps on demand. The new {GHTorrent} data-on-demand service offers users the possibility to request via a web form up-to-date {GHTorrent} data dumps for any collection of {GitHub} repositories. We hope that by offering customisable {GHTorrent} data dumps we will not only lower the "barrier for entry" even further for researchers interested in mining {GitHub} data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of {GitHub} studies (since a snapshot of the data on which the results were obtained can now easily accompany each study).},
	pages = {384--387},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Gousios, Georgios and Vasilescu, Bogdan and Serebrenik, Alexander and Zaidman, Andy},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {data on demand, dataset, {GitHub}},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\USD8NF2X\\Gousios et al. - 2014 - Lean GHTorrent GitHub Data on Demand.pdf:application/pdf}
}

@article{catal_investigating_2009,
	title = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
	volume = {179},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025508005173},
	doi = {10.1016/j.ins.2008.12.001},
	abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models’ performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public {NASA} datasets from the {PROMISE} repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public {NASA} datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve ({AUC}) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems ({AIRS}2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
	pages = {1040--1058},
	number = {8},
	journal = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Catal, Cagatay and Diri, Banu},
	urldate = {2013-07-16},
	date = {2009-03-29},
	%note = {Cited by 0053},
	keywords = {artificial immune systems, J48, machine learning, Naive Bayes, random forests, Software fault prediction},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\7987BKC4\\Catal e Diri - 2009 - Investigating the effect of dataset size, metrics .pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\GIXZ9PB4\\S0020025508005173.html:text/html}
}

@article{turhan_relative_2009,
	title = {On the relative value of cross-company and within-company data for defect prediction},
	volume = {14},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-008-9103-7},
	doi = {10.1007/s10664-008-9103-7},
	abstract = {We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company ({CC}) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where {CC} data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor ({NN}) filtering) to {CC} data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company ({WC}) data. As expected, we observe that defect predictors learned from {WC} data outperform the ones learned from {CC} data. However, our analyses also yield defect predictors learned from {NN}-filtered {CC} data, with performance close to, but still not better than, {WC} data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn {WC} defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use {NN}-filtered {CC} data to initiate the defect prediction process and simultaneously start collecting {WC} (local) data. Once enough {WC} data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from {WC} data.},
	pages = {540--578},
	number = {5},
	journal = {Empirical Software Engineering},
	author = {Turhan, Burak and Menzies, Tim and Bener, {AyÅŸe} B. and Stefano, Justin Di},
	urldate = {2013-07-10},
	date = {2009-10},
	langid = {english},
	%note = {Cited by 0097},
	keywords = {Compilers, Cross-company, Cross-company, defect prediction, defect prediction, Interpreters, Learning, Learning, Metrics (product metrics), Metrics (product metrics), Nearest-neighbor filtering, Nearest-neighbor filtering, programming languages, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, Software Engineering/Programming and Operating Systems, Within-company, Within-company}
}

@inproceedings{anvik_automating_2006,
	location = {New York, {NY}, {USA}},
	title = {Automating Bug Report Assignment},
	isbn = {1-59593-375-1},
	url = {http://doi.acm.org/10.1145/1134285.1134457},
	doi = {10.1145/1134285.1134457},
	series = {{ICSE}},
	abstract = {Open-source development projects typically support an open bug repository to which both developers and users can report bugs. A report that appears in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open-source developments are burdened by the rate at which new bug reports appear in the bug repository. The thesis of this work is that the task of triage can be eased by using a semi-automated approach to assign bug reports to developers. The approach consists of constructing a recommender for bug assignments; examined are both a range of algorithms that can be used and the various kinds of information provided to the algorithms. The proposed work seeks to determine through human experimentation a sufficient level of precision for the recommendations, and to analytically determine the trade-offs of the various algorithmic and information choices.},
	pages = {937--940},
	booktitle = {Proceedings of the 28th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Anvik, John},
	urldate = {2014-10-09},
	year = {2006},
	keywords = {bug report assignment, triage}
}

@article{kramer_is_2007,
	title = {Is abstraction the key to computing?},
	volume = {50},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/1232743.1232745},
	doi = {10.1145/1232743.1232745},
	abstract = {Why is it that some software engineers and computer scientists are able to produce clear, elegant designs and programs, while others cannot? Is it possible to improve these skills through education and training? Critical to these questions is the notion of abstraction.},
	pages = {36â€“42},
	number = {4},
	journal = {Commun. {ACM}},
	author = {Kramer, Jeff},
	urldate = {2012-10-23},
	date = {2007-04},
	%note = {Cited by 0206}
}

@inproceedings{singer_navtracks:_2005,
	title = {{NavTracks}: Supporting Navigation in Software Maintenance},
	eventtitle = {{ICSM}},
	pages = {325--334},
	booktitle = {Proceedings 21st {IEEE} International Conference  on Software Maintenance},
	author = {Singer, Janice and Elves, Robert and Storey, Margaret-Anne},
	date = {2005},
	%note = {Cited by 0076}
}

@article{xie_data_2009,
	title = {Data Mining for Software Engineering},
	volume = {42},
	issn = {0018-9162},
	doi = {10.1109/MC.2009.256},
	abstract = {To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining {SE} data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.},
	pages = {55--62},
	number = {8},
	journal = {Computer},
	author = {Xie, Tao and Thummalapenta, S. and Lo, D. and Liu, Chao},
	date = {2009},
	%note = {Cited by 0055},
	keywords = {Cleaning, Cleaning, Clustering algorithms, Clustering algorithms, Computational intelligence, Computational intelligence, Databases, Databases, Data engineering, Data engineering, data graphs, data graphs, data mining, data mining, data sequences, data sequences, Debugging, Debugging, Design and test, Design and test, Dynamic programming, Dynamic programming, Pattern matching, Pattern matching, Software algorithms, Software algorithms, software engineering, software engineering, software productivity, software productivity, software quality, software quality, text mining, text mining}
}

@inproceedings{toth_using_2013,
	title = {Using Version Control History to Follow the Changes of Source Code Elements},
	doi = {10.1109/CSMR.2013.40},
	abstract = {Version control systems store the whole history of the source code. Since the source code of a system is organized into files and folders, the history tells us the concerned files and their changed lines only but software engineers are also interested in which source code elements (e.g. classes or methods) are affected by a change. Unfortunately, in most programming languages source code elements do not follow the file system hierarchy, which means that a file can contain more classes and methods and a class can be stored in more files, which makes it difficult to determine the changes of classes by using the changes of files. To solve this problem we developed an algorithm, which is able to follow the changes of the source code elements by using the changes of files and we successfully applied it on the Web Kit open source system.},
	pages = {319--322},
	booktitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Toth, Z. and Novak, G. and Ferenc, R. and Siket, I.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {configuration management, configuration management, file change, file change, file organisation, file organisation, file system hierarchy, file system hierarchy, Internet, Internet, programming language, programming language, public domain software, public domain software, Repository Mining, Repository Mining, software engineer, software engineer, source code element, source code element, static analysis, static analysis, version control history, version control history, Version control system, Version control system, Web Kit open source system, Web Kit open source system}
}

@inproceedings{platt_fast_1999,
  title={Fast training of support vector machines using sequential minimal optimization},
  author={Platt, John C},
  booktitle={Advances in kernel methods},
  pages={185--208},
  year={1999},
  organization={MIT press}	
}

@inproceedings{canfora_identifying_2007,
	title = {Identifying Changed Source Code Lines from Version Repositories},
	doi = {10.1109/MSR.2007.14},
	abstract = {Observing the evolution of software systems at different levels of granularity has been a key issue for a number of studies, aiming at predicting defects or at studying certain phenomena, such as the presence of clones or of crosscutting concerns. Versioning systems such as {CVS} and {SVN}, however, only provide information about lines added or deleted by a contributor: any change is shown as a sequence of additions and deletions. This provides an erroneous estimate of the amount of code changed. This paper shows how the evolution of changes at source code line level can be inferred from {CVS} repositories, by combining information retrieval techniques and the Levenshtein edit distance. The application of the proposed approach to the {ArgoUML} case study indicates a high precision and recall.},
	pages = {14},
	booktitle = {Mining Software Repositories, 2007. {ICSE} Workshops {MSR} '07. Fourth International Workshop on},
	author = {Canfora, G. and Cerulo, L. and Di Penta, M.},
	date = {2007-05},
	%note = {Cited by 0072},
	keywords = {{ArgoUML} snapshot, {ArgoUML} snapshot, concurrent versioning system, concurrent versioning system, configuration management, configuration management, crosscutting concern, crosscutting concern, information retrieval, information retrieval, software evolution system, software evolution system, software prototyping, software prototyping, software reusability, software reusability, source code lines, source code lines}
}

@book{guo_characterizing_????,
	title = {Characterizing and Predicting Which Bugs Get Fixed: An Empirical Study of Microsoft Windows},
	shorttitle = {Characterizing and Predicting Which Bugs Get Fixed},
	abstract = {We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68 \% and recall of 64 \% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions. Categories and Subject Descriptors:},
	author = {Guo, Philip J. and Zimmermann, Thomas and Murphy, Brendan},
	%note = {Cited by 0068}
}

@inproceedings{rahman_insight_2014,
	location = {New York, {NY}, {USA}},
	title = {An Insight into the Pull Requests of {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597121},
	doi = {10.1145/2597073.2597121},
	series = {{MSR} 2014},
	abstract = {Given the increasing number of unsuccessful pull requests in {GitHub} projects, insights into the success and failure of these requests are essential for the developers. In this paper, we provide a comparative study between successful and unsuccessful pull requests made to 78 {GitHub} base projects by 20,142 developers from 103,192 forked projects. In the study, we analyze pull request discussion texts, project specific information (e.g., domain, maturity), and developer specific information (e.g., experience) in order to report useful insights, and use them to contrast between successful and unsuccessful pull requests. We believe our study will help developers overcome the issues with pull requests in {GitHub}, and project administrators with informed decision making.},
	pages = {364--367},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Commit comments, pull request, topic model},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ECPBPI5T\\Rahman e Roy - 2014 - An Insight into the Pull Requests of GitHub.pdf:application/pdf;ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\U82HA4J5\\Rahman e Roy - 2014 - An Insight into the Pull Requests of GitHub.pdf:application/pdf}
}

@book{gaines_induction_1995,
	title = {Induction of Ripple-Down Rules Applied to Modeling Large Databases},
	abstract = {A methodology for the modeling of large data sets is described which results in rule sets having minimal inter-rule interactions, and being simply maintained. An algorithm for developing such rule sets automatically is described and its efficacy shown with standard test data sets. Comparative studies of manual and automatic modeling of a data set of some nine thousand five hundred cases are reported. A study is reported in which ten years of patient data have been modeled on a month by month basis to determine how well a diagnostic system developed by automated induction would have performed had it been in use throughout the project.},
	author = {Gaines, Brian R. and Compton, Paul},
	date = {1995},
	%note = {Cited by 0112}
}

@inproceedings{kalliamvakou_promises_2014,
	location = {New York, {NY}, {USA}},
	title = {The Promises and Perils of Mining {GitHub}},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597074},
	doi = {10.1145/2597073.2597074},
	series = {{MSR} '14},
	abstract = {With over 10 million git repositories, {GitHub} is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in {GitHub}'s event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from {GitHub}. We document the results of an empirical study aimed at understanding the characteristics of the repositories in {GitHub} and how users take advantage of {GitHub}'s main features---namely commits, pull requests, and issues. Our results indicate that, while {GitHub} is a rich source of data on software development, mining {GitHub} for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that {GitHub} is also being used for free storage and as a Web hosting service; and that almost 40\% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in {GitHub}.},
	pages = {92--101},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00002},
	keywords = {bias, code reviews, git, {GitHub}, mining software repositories},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\FVJK2CBM\\Kalliamvakou et al. - 2014 - The Promises and Perils of Mining GitHub.pdf:application/pdf}
}

@inproceedings{thomas_mining_2011,
	location = {New York, {NY}, {USA}},
	title = {Mining software repositories using topic models},
	isbn = {978-1-4503-0445-0},
	url = {http://doi.acm.org/10.1145/1985793.1986020},
	doi = {10.1145/1985793.1986020},
	series = {{ICSE} '11},
	abstract = {Software repositories, such as source code, email archives, and bug databases, contain unstructured and unlabeled text that is difficult to analyze with traditional techniques. We propose the use of statistical topic models to automatically discover structure in these textual repositories. This discovered structure has the potential to be used in software engineering tasks, such as bug prediction and traceability link recovery. Our research goal is to address the challenges of applying topic models to software repositories.},
	pages = {1138â€“1139},
	booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Thomas, Stephen W.},
	urldate = {2012-07-09},
	date = {2011},
	%note = {Cited by 0012},
	keywords = {lda, lda, mining software repositories, mining software repositories, topic models, topic models}
}

@article{shatnawi_effectiveness_2008,
	title = {The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process},
	volume = {81},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121208000095},
	doi = {10.1016/j.jss.2007.12.794},
	abstract = {Many empirical studies have found that software metrics can predict class error proneness and the prediction can be used to accurately group error-prone classes. Recent empirical studies have used open source systems. These studies, however, focused on the relationship between software metrics and class error proneness during the development phase of software projects. Whether software metrics can still predict class error proneness in a system’s post-release evolution is still a question to be answered. This study examined three releases of the Eclipse project and found that although some metrics can still predict class error proneness in three error-severity categories, the accuracy of the prediction decreased from release to release. Furthermore, we found that the prediction cannot be used to build a metrics model to identify error-prone classes with acceptable accuracy. These findings suggest that as a system evolves, the use of some commonly used metrics to identify which classes are more prone to errors becomes increasingly difficult and we should seek alternative methods (to the metric-prediction models) to locate error-prone classes if we want high accuracy.},
	pages = {1868--1882},
	number = {11},
	journal = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Shatnawi, Raed and Li, Wei},
	urldate = {2012-12-14},
	date = {2008-11},
	%note = {Cited by 0042},
	keywords = {Class error proneness, Design evolution, Empirical study, Error-severity categories, Object-oriented metrics, Open source software},
	file = {ScienceDirect Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\PD6H83C9\\Shatnawi and Li - 2008 - The effectiveness of software metrics in identifyi.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\83QGZS67\\Shatnawi and Li - 2008 - The effectiveness of software metrics in identifyi.html:text/html}
}

@inproceedings{xie_improving_2008,
	title = {Improving software reliability and productivity via mining program source code},
	doi = {10.1109/IPDPS.2008.4536384},
	abstract = {A software system interacts with third-party libraries through various {APIs}. Insufficient documentation and constant refactorings of third-party libraries make {API} library reuse difficult and error prone. Using these library {APIs} often needs to follow certain usage patterns. These patterns aid developers in addressing commonly faced programming problems such as what checks should precede or follow {API} calls, how to use a given set of {APIs} for a given task, or what {API} method sequence should be used to obtain one object from another. Ordering rules (specifications) also exist between {APIs}, and these rules govern the secure and robust operation of the system using these {APIs}. These patterns and rules may not be well documented by the {API} developers. Furthermore, usage patterns and specifications might change with library refactorings, requiring changes in the software that reuse the library. To address these issues, we develop novel techniques (and their supporting tools) based on mining source code, assisting developers in productively reusing third party libraries to build reliable and secure software.},
	pages = {1--5},
	booktitle = {{IEEE} International Symposium on Parallel and Distributed Processing, 2008. {IPDPS} 2008},
	author = {Xie, Tao and Acharya, M. and Thummalapenta, S. and Taneja, K.},
	date = {2008},
	%note = {Cited by 0005},
	keywords = {{API} library reuse, {API} library reuse, application program interfaces, application program interfaces, Computer errors, Computer errors, Computer Science, Computer Science, data mining, data mining, Documentation, Documentation, Productivity, Productivity, Programming, Programming, program source code mining, program source code mining, Robustness, Robustness, secure software, secure software, software development management, software development management, software libraries, software libraries, software productivity, software productivity, Software reliability, Software reliability, software system, software system, Software systems, Software systems, third-party library, third-party library}
}

@article{lamport_time_1978,
	title = {Time, clocks, and the ordering of events in a distributed system},
	volume = {21},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/359545.359563},
	doi = {10.1145/359545.359563},
	abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
	pages = {558â€“565},
	number = {7},
	journal = {Commun. {ACM}},
	author = {Lamport, Leslie},
	urldate = {2012-11-14},
	date = {1978-07},
	%note = {Cited by 8035},
	keywords = {clock synchronization, clock synchronization, computer networks, computer networks, distributed systems, distributed systems, multiprocess systems, multiprocess systems}
}

@book{_c:textbackslashgnutextbackslashouttextbackslashbnettextbackslashcore30_b.eps_????,
	title = {c:{\textbackslash}textbackslashgnu{\textbackslash}textbackslashout{\textbackslash}{textbackslashBNET}{\textbackslash}textbackslashcore30\_b.eps - a12-aljarah.pdf},
	url = {http://delivery.acm.org/10.1145/2030000/2020402/a12-aljarah.pdf?ip=200.129.173.3&id=2020402&acc=ACTIVE%20SERVICE&key=C2716FEBFA981EF14D39E795746BF179BD02EBA2BB1F340F&CFID=255439464&CFTOKEN=29465794&__acm__=1382376959_3fe6056b1247ae6d9bc0b52237188932},
	urldate = {2013-10-21},
	%note = {Cited by 0000}
}

@inproceedings{bhattacharya_empirical_2013,
	title = {An Empirical Analysis of Bug Reports and Bug Fixing in Open Source Android Apps},
	doi = {10.1109/CSMR.2013.23},
	abstract = {Smartphone platforms and applications (apps) have gained tremendous popularity recently. Due to the novelty of the smartphone platform and tools, and the low barrier to entry for app distribution, apps are prone to errors, which affects user experience and requires frequent bug fixes. An essential step towards correcting this situation is understanding the nature of the bugs and bug-fixing processes associated with smartphone platforms and apps. However, prior empirical bug studies have focused mostly on desktop and server applications. Therefore, in this paper, we perform an in-depth empirical study on bugs in the Google Android smartphone platform and 24 widely-used open-source Android apps from diverse categories such as communication, tools, and media. Our analysis has three main thrusts. First, we define several metrics to understand the quality of bug reports and analyze the bug-fix process, including developer involvement. Second, we show how differences in bug life-cycles can affect the bug-fix process. Third, as Android devices carry significant amounts of security-sensitive information, we perform a study of Android security bugs. We found that, although contributor activity in these projects is generally high, developer involvement decreases in some projects, similarly, while bug-report quality is high, bug triaging is still a problem. Finally, we observe that in Android apps, security bug reports are of higher quality but get fixed slower than non-security bugs. We believe that the findings of our study could potentially benefit both developers and users of Android apps.},
	eventtitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {133--143},
	author = {Bhattacharya, P. and Ulanova, L. and Neamtiu, I. and Koduru, S.C.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {Android security bugs, app distribution, bug fixing, bug fixing process, bug life-cycle, bug report, bug reports, developer involvement, empirical studies, Google Android, Google Android smartphone, mobile computing, open source Android apps, operating systems (computers), program debugging, security bugs, security of data, smart phone application, smartphone apps, smart phone platform, Smart phones}
}

@article{gao_choosing_2011,
	title = {Choosing software metrics for defect prediction: an investigation on feature selection techniques},
	volume = {41},
	rights = {Copyright Â© 2011 John Wiley \& Sons, Ltd.},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.1043/abstract},
	doi = {10.1002/spe.1043},
	shorttitle = {Choosing software metrics for defect prediction},
	abstract = {The selection of software metrics for building software quality prediction models is a search-based software engineering problem. An exhaustive search for such metrics is usually not feasible due to limited project resources, especially if the number of available metrics is large. Defect prediction models are necessary in aiding project managers for better utilizing valuable project resources for software quality improvement. The efficacy and usefulness of a fault-proneness prediction model is only as good as the quality of the software measurement data. This study focuses on the problem of attribute selection in the context of software quality estimation. A comparative investigation is presented for evaluating our proposed hybrid attribute selection approach, in which feature ranking is first used to reduce the search space, followed by a feature subset selection. A total of seven different feature ranking techniques are evaluated, while four different feature subset selection approaches are considered. The models are trained using five commonly used classification algorithms. The case study is based on software metrics and defect data collected from multiple releases of a large real-world software system. The results demonstrate that while some feature ranking techniques performed similarly, the automatic hybrid search algorithm performed the best among the feature subset selection methods. Moreover, performances of the defect prediction models either improved or remained unchanged when over 85were eliminated. Copyright Â© 2011 John Wiley \& Sons, Ltd.},
	pages = {579â€“606},
	number = {5},
	journal = {Software: Practice and Experience},
	author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Wang, Huanjing and Seliya, Naeem},
	urldate = {2013-07-23},
	date = {2011},
	langid = {english},
	%note = {Cited by 0025},
	keywords = {attribute selection, attribute selection, defect prediction, defect prediction, feature ranking, feature ranking, feature subset selection, feature subset selection, search-based software engineering, search-based software engineering, Software metric, Software metric, software quality, software quality}
}

@inproceedings{holmes_using_2005,
	location = {New York, {NY}, {USA}},
	title = {Using structural context to recommend source code examples},
	isbn = {1-58113-963-2},
	url = {http://doi.acm.org/10.1145/1062455.1062491},
	doi = {10.1145/1062455.1062491},
	series = {{ICSE} '05},
	abstract = {When coding to a framework, developers often become stuck, unsure of which class to subclass, which objects to instantiate and which methods to call. Example code that demonstrates the use of the framework can help developers make progress on their task. In this paper, we describe an approach for locating relevant code in an example repository that is based on heuristically matching the structure of the code under development to the example code. Our tool improves on existing approaches in two ways. First, the structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. Second, the repository can be generated easily from existing applications. We demonstrate the utility of this approach by reporting on a case study involving two subjects completing four programming tasks within the Eclipse integrated development environment framework.},
	pages = {117--125},
	publisher = {{ACM}},
	author = {Holmes, Reid and Murphy, Gail C.},
	urldate = {2012-09-06},
	date = {2005},
	%note = {Cited by 0195},
	keywords = {development environment framework, examples, recommender, software structure}
}

@inproceedings{hata_bug_2012,
	location = {Piscataway, {NJ}, {USA}},
	title = {Bug Prediction Based on Fine-grained Module Histories},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337247},
	series = {{ICSE} '12},
	abstract = {There have been many bug prediction models built with historical metrics, which are mined from version histories of software modules. Many studies have reported the effectiveness of these historical metrics. For prediction levels, most studies have targeted package and file levels. Prediction on a fine-grained level, which represents the method level, is required because there may be interesting results compared to coarse-grained (package and file levels) prediction. These results include good performance when considering quality assurance efforts, and new findings about the correlations between bugs and histories. However, fine-grained prediction has been a challenge because obtaining method histories from existing version control systems is a difficult problem. To tackle this problem, we have developed a fine-grained version control system for Java, Historage. With this system, we target Java software and conduct fine-grained prediction with well-known historical metrics. The results indicate that fine-grained (method-level) prediction outperforms coarse-grained (package and file levels) prediction when taking the efforts necessary to find bugs into account. Using a correlation analysis, we show that past bug information does not contribute to method-level bug prediction.},
	pages = {200--210},
	booktitle = {Proceedings of the 34th International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru},
	urldate = {2014-09-10},
	date = {2012},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\873R846D\\Hata et al. - 2012 - Bug Prediction Based on Fine-grained Module Histor.pdf:application/pdf}
}

@inproceedings{morrison_is_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Is programming knowledge related to age? an exploration of stack overflow},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487102},
	series = {{MSR} '13},
	shorttitle = {Is programming knowledge related to age?},
	abstract = {Becoming an expert at programming is thought to take an estimated 10,000 hours of deliberate practice . But what happens after that? Do programming experts continue to develop, do they plateau, or is there a decline at some point? A diversity of opinion exists on this matter, but many seem to think that aging brings a decline in adoption and absorption of new programming knowledge. We develop several research questions on this theme, and draw on data from {StackOverflow} to address these questions. The goal of this research is to support career planning and staff development for programmers by identifying age-related trends in {StackOverflow} data. We observe that programmer reputation scores increase relative to age well into the 50â€™s, that programmers in their 30â€™s tend to focus on fewer areas relative to those younger or older in age, and that there is not a strong correlation between age and scores in specific knowledge areas.},
	pages = {69â€“72},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Morrison, Patrick and Murphy-Hill, Emerson},
	urldate = {2013-11-05},
	date = {2013},
	%note = {Cited by 0001}
}

@article{fenton_critique_1999,
	title = {A Critique of Software Defect Prediction Models},
	volume = {25},
	issn = {0098-5589},
	url = {http://dx.doi.org/10.1109/32.815326},
	doi = {10.1109/32.815326},
	abstract = {Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the "quality" of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the "Goldilock's Conjecture," that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian Belief Networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of "software decomposition" in order to test hypotheses about defect introduction and help construct a better science of software engineering.},
	pages = {675--689},
	number = {5},
	journal = {{IEEE} Trans. Softw. Eng.},
	author = {Fenton, Norman E. and Neil, Martin},
	urldate = {2013-08-01},
	date = {1999-09},
	%note = {Cited by 0739},
	keywords = {Bayesian Belief Networks., complexity metrics, defects, fault-density, Software faults and failures},
	file = {Fenton e Neil - 1999 - A Critique of Software Defect Prediction Models.html:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DT77SIEG\\Fenton e Neil - 1999 - A Critique of Software Defect Prediction Models.html:text/html;Fenton e Neil - 1999 - A Critique of Software Defect Prediction Models.pdf:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\FVVGBKD8\\Fenton e Neil - 1999 - A Critique of Software Defect Prediction Models.pdf:application/pdf}
}

@article{taghi_m._khoshgoftaar_data_1999,
	title = {{DATA} {MINING} {FOR} {PREDICTORS} {OF} {SOFTWARE} {QUALITY}},
	volume = {09},
	issn = {0218-1940, 1793-6403},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218194099000309},
	doi = {10.1142/S0218194099000309},
	pages = {547--563},
	number = {5},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Taghi M. Khoshgoftaar and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
	urldate = {2013-02-14},
	date = {1999-10},
	%note = {Cited by 0040}
}

@inproceedings{couto_uncovering_2012,
	title = {Uncovering Causal Relationships between Software Metrics and Bugs},
	doi = {10.1109/CSMR.2012.31},
	abstract = {Bug prediction is an important challenge for software engineering research. It consist in looking for possible early indicators of the presence of bugs in a software. However, despite the relevance of the issue, most experiments designed to evaluate bug prediction only investigate whether there is a linear relation between the predictor and the presence of bugs. However, it is well known that standard regression models cannot filter out spurious relations. Therefore, in this paper we describe an experiment to discover more robust evidences towards causality between software metrics (as predictors) and the occurrence of bugs. For this purpose, we have relied on Granger Causality Test to evaluate whether past changes in a given time series are useful to forecast changes in another series. As its name suggests, Granger Test is a better indication of causality between two variables. We present and discuss the results of experiments on four real world systems evaluated over a time frame of almost four years. Particularly, we have been able to discover in the history of metrics the causes - in the terms of the Granger Test - for 64\% to 93\% of the defects reported for the systems considered in our experiment.},
	eventtitle = {2012 16th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {223--232},
	booktitle = {2012 16th European Conference on Software Maintenance and Reengineering ({CSMR})},
	author = {Couto, C. and Silva, C. and Valente, M.T. and Bigonha, R. and Anquetil, N.},
	date = {2012},
	%note = {Cited by 0009},
	keywords = {Bug prediction, bug prediction evaluation, causality, causal relationships, Computer bugs, Granger causality test, Granger Test, Mathematical model, program debugging, regression analysis, Reliability, software, software bugs, software engineering research, Software metrics, software performance evaluation, spurious relations, standard regression models, statistical testing, Time series analysis},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\9RPRDZMR\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XWSDZNH6\\Couto et al. - 2012 - Uncovering Causal Relationships between Software M.pdf:application/pdf}
}

@article{briand_comprehensive_2000,
	title = {A comprehensive evaluation of capture-recapture models for estimating software defect content},
	volume = {26},
	issn = {0098-5589},
	doi = {10.1109/32.852741},
	abstract = {An important requirement to control the inspection of software artifacts is to be able to decide, based on more objective information, whether the inspection can stop or whether it should continue to achieve a suitable level of artifact quality. A prediction of the number of remaining defects in an inspected artifact can be used for decision making. Several studies in software engineering have considered capture-recapture models to make a prediction. However, few studies compare the actual number of remaining defects to the one predicted by a capture-recapture model on real software engineering artifacts. The authors focus on traditional inspections and estimate, based on actual inspections data, the degree of accuracy of relevant state-of-the-art capture-recapture models for which statistical estimators exist. In order to assess their robustness, we look at the impact of the number of inspectors and the number of actual defects on the estimators' accuracy based on actual inspection data. Our results show that models are strongly affected by the number of inspectors, and therefore one must consider this factor before using capture-recapture models. When the number of inspectors is too small, no model is sufficiently accurate and underestimation may be substantial. In addition, some models perform better than others in a large number of conditions and plausible reasons are discussed. Based on our analyses, we recommend using a model taking into account that defects have different probabilities of being detected and the corresponding Jackknife Estimator. Furthermore, we calibrate the prediction models based on their relative error, as previously computed on other inspections. We identified theoretical limitations to this approach which were then confirmed by the data},
	pages = {518--540},
	number = {6},
	journal = {{IEEE} Transactions on Software Engineering},
	author = {Briand, L.C. and El Emam, K. and Freimut, B.G. and Laitenberger, O.},
	date = {2000},
	%note = {Cited by 0138},
	keywords = {Animals, artifact quality, Biological system modeling, capture-recapture models, Computer Society, decision making, estimator accuracy, inspection, inspections data, Jackknife Estimator, objective information, Predictive models, probabilities, probability, real software engineering artifacts, relative error, Robustness, software defect content estimation, software development management, software engineering, software performance evaluation, software quality, State estimation, statistical estimators, traditional inspections}
}

@article{jain_data_1999,
	title = {Data clustering: a review},
	volume = {31},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/331499.331504},
	doi = {10.1145/331499.331504},
	shorttitle = {Data clustering},
	abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
	pages = {264â€“323},
	number = {3},
	journal = {{ACM} Comput. Surv.},
	author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
	urldate = {2013-07-05},
	date = {1999-09},
	%note = {Cited by 8000},
	keywords = {cluster analysis, cluster analysis, clustering applications, clustering applications, exploratory data analysis, exploratory data analysis, incremental clustering, incremental clustering, similarity indices, similarity indices, unsupervised learning, unsupervised learning}
}

@misc{rocha_mps.br_2007,
	title = {{MPS}.{BR} - Melhoria de Processo do Software Brasileiro. Guia de Implementação – Parte 2: Nível F.},
	abstract = {Este guia contém orientações para a 
implementação do nível F do Modelo 
de Referência {MR}-{MPS}.},
	publisher = {Ana Regina C. Rocha},
	author = {Rocha, Ana Regina C.},
	date = {2007},
	%note = {Cited by 0000},
	file = {softex2007.pdf:D\:\\Universidade\\DINTER\\Disciplinas\\Gerência de Configuração\\Leituras\\softex2007.pdf:application/pdf}
}

@article{anvik_reducing_2011,
	title = {Reducing the Effort of Bug Report Triage: Recommenders for Development-oriented Decisions},
	volume = {20},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/2000791.2000794},
	doi = {10.1145/2000791.2000794},
	shorttitle = {Reducing the Effort of Bug Report Triage},
	abstract = {A key collaborative hub for many software development projects is the bug report repository. Although its use can improve the software development process in a number of ways, reports added to the repository need to be triaged. A triager determines if a report is meaningful. Meaningful reports are then organized for integration into the project's development process. To assist triagers with their work, this article presents a machine learning approach to create recommenders that assist with a variety of decisions aimed at streamlining the development process. The recommenders created with this approach are accurate; for instance, recommenders for which developer to assign a report that we have created using this approach have a precision between 70\% and 98\% over five open source projects. As the configuration of a recommender for a particular project can require substantial effort and be time consuming, we also present an approach to assist the configuration of such recommenders that significantly lowers the cost of putting a recommender in place for a project. We show that recommenders for which developer should fix a bug can be quickly configured with this approach and that the configured recommenders are within 15\% precision of hand-tuned developer recommenders.},
	pages = {10:1--10:35},
	number = {3},
	journal = {Transactions on Software Engineering and Methodology},
	author = {Anvik, John and Murphy, Gail C.},
	urldate = {2014-06-30},
	year = {2011},
	keywords = {bug report triage, configuration assistance, machine learning, recommendation, task assignment},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\WX39JQIA\\Anvik e Murphy - 2011 - Reducing the Effort of Bug Report Triage Recommen.pdf:application/pdf}
}

@thesis{_fine-grained_????,
	title = {Fine-grained code changes and bugs: Improving bug prediction},
	url = {http://dx.doi.org/10.5167/uzh-61703},
	type = {phdthesis},
	%note = {Cited by 0000}
}

@inproceedings{linares-vasquez_triaging_2012,
	title = {Triaging incoming change requests: Bug or commit history, or code authorship?},
	doi = {10.1109/ICSM.2012.6405306},
	shorttitle = {Triaging incoming change requests},
	abstract = {There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, {ArgoUML}, {JEdit}, and {MuCommander}, is reported. Our approach is compared with two representative approaches: (1) using machine learning on past bug reports, and (2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information.},
	eventtitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
	pages = {451--460},
	booktitle = {2012 28th {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Linares-Vasquez, M. and Hossen, K. and Dang, Hoang and Kagdi, H. and Gethers, M. and Poshyvanyk, D.},
	date = {2012},
	%note = {Cited by 0005},
	keywords = {Accuracy, {ArgoUML}, bug fixing, change request, code authorship, commit log history, data mining, expert developer recommendations, header comments, information retrieval, information retrieval technique, {JEdit}, Large scale integration, learning (artificial intelligence), machine learning, {MuCommander}, open source projects, program debugging, public domain software, recommendation accuracies, recommender systems, software change request textual description, software maintenance, source code authorship information, source code files, support vector machines, triaging, Unified Modeling Language},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\NTP3DHQT\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\7A2N2IDV\\Linares-Vasquez et al. - 2012 - Triaging incoming change requests Bug or commit h.pdf:application/pdf}
}

@article{ying_predicting_2004,
	title = {Predicting source code changes by mining change history},
	volume = {30},
	issn = {0098-5589},
	doi = {10.1109/TSE.2004.52},
	abstract = {Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.},
	pages = {574 -- 586},
	number = {9},
	journal = {Software Engineering, {IEEE} Transactions on},
	author = {Ying, A.T.T. and Murphy, G.C. and Ng, R. and Chu-Carroll, M.C.},
	date = {2004-09},
	%note = {Cited by 0324},
	keywords = {association rules, association rules, change history, change history, code base, code base, configuration management, configuration management, data mining, data mining, data mining technique, data mining technique, Eclipse open source project, Eclipse open source project, modification task, modification task, Mozilla open source project, Mozilla open source project, pattern classification, pattern classification, pattern clustering, pattern clustering, program verification, program verification, software developers, software developers, software maintainability, software maintainability, software maintenance, software maintenance, software tools, software tools, source code changes prediction, source code changes prediction}
}

@inproceedings{yamashita_magnet_2014,
	location = {New York, {NY}, {USA}},
	title = {Magnet or Sticky? An {OSS} Project-by-project Typology},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597116},
	doi = {10.1145/2597073.2597116},
	series = {{MSR} 2014},
	shorttitle = {Magnet or Sticky?},
	abstract = {For Open Source Software ({OSS}) projects, retaining existing contributors and attracting new ones is a major concern. In this paper, we expand and adapt a pair of population migration metrics to analyze migration trends in a collection of open source projects. Namely, we study: (1) project stickiness, i.e., its tendency to retain existing contributors and (2) project magnetism, i.e., its tendency to attract new contributors. Using quadrant plots, we classify projects as attractive (highly magnetic and sticky), stagnant (highly sticky, weakly magnetic), fluctuating (highly magnetic, weakly sticky), or terminal (weakly magnetic and sticky). Through analysis of the {MSR} challenge dataset, we find that: (1) quadrant plots can effectively identify at-risk projects, (2) stickiness is often motivated by professional activity and (3) transitions among quadrants as a project ages often coincides with interesting events in the evolution history of a project.},
	pages = {344--347},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Yamashita, Kazuhiro and {McIntosh}, Shane and Kamei, Yasutaka and Ubayashi, Naoyasu},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Developer migration, Magnet, open source, Sticky},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\J975VRRJ\\Yamashita et al. - 2014 - Magnet or Sticky An OSS Project-by-project Typolo.pdf:application/pdf}
}

@inproceedings{zimmermann_mining_2004,
	location = {Washington, {DC}, {USA}},
	title = {Mining Version Histories to Guide Software Changes},
	isbn = {0-7695-2163-0},
	url = {http://dl.acm.org/citation.cfm?id=998675.999460},
	series = {{ICSE} '04},
	abstract = {We apply data mining to version histories in order toguide programmers along related changes: "Programmerswho changed these functions also changed. . . ". Given aset of existing changes, such rules (a) suggest and predictlikely further changes, (b) show up item coupling that is indetectableby program analysis, and (c) prevent errors dueto incomplete changes. After an initial change, our {ROSEprototype} can correctly predict 26\% of further files to bechanged Â¿ and 15\% of the precise functions or variables.The topmost three suggestions contain a correct locationwith a likelihood of 64\%.},
	pages = {563â€“572},
	booktitle = {Proceedings of the 26th International Conference on Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Zimmermann, Thomas and Weisgerber, Peter and Diehl, Stephan and Zeller, Andreas},
	urldate = {2012-07-12},
	date = {2004},
	%note = {Cited by 0000}
}

@inproceedings{demott_towards_2011,
	title = {Towards an automatic exploit pipeline},
	abstract = {A continuous and fully automated software exploit discovery and development pipeline for real-world problems has not yet been achieved, but is desired by defenders and attackers alike. We have made significant steps toward that goal by combining and enhancing known bug hunting and analysis techniques. The first step is the implementation of an easy-to-use distributed fuzzer. Single fuzzers take too long to produce the number of results required. Since distributed fuzzers achieve high-output (typically many found bugs) sorting is required, which we include. We add another layer of triage support by combining in an enhanced fault localization process. Our work automates much of the process so that human resources are only needed at a few key checkpoints along the pipeline, arguably enhancing overall system efficiency. We demonstrate our process on contrived code, the Siemens suite, and two real-world pieces of code: Firefox and Java.},
	eventtitle = {Internet Technology and Secured Transactions ({ICITST}), 2011 International Conference for},
	pages = {323--329},
	booktitle = {Internet Technology and Secured Transactions ({ICITST}), 2011 International Conference for},
	author = {{DeMott}, J.D. and Enbody, R.J. and Punch, W.F.},
	date = {2011},
	%note = {Cited by 0005},
	keywords = {analysis technique, automated software exploit discovery, automatic exploit pipeline, Automatic Vulnerability Discovery and Exploitation, bug hunting, checkpointing, checkpoints, Computer bugs, contrived code, Debugging, development pipeline, distributed fuzzer, Distributed Fuzzing, Fault Localization, fault localization process, Firefox, high-output sorting, human resources, Java, Noise, pipeline processing, Pipelines, program debugging, Security, security of data, Siemens suite, software, software fault tolerance, software security, Software Testing and Debugging, systems analysis, triage},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\3RE9QSBR\\articleDetails.html:text/html}
}

@inproceedings{weicheng_mining_2013,
	location = {Washington, {DC}, {USA}},
	title = {Mining {GitHub}: Why Commit Stops – Exploring the Relationship Between Developer's Commit Pattern and File Version Evolution},
	isbn = {978-1-4799-2144-7},
	url = {http://dx.doi.org/10.1109/APSEC.2013.133},
	doi = {10.1109/APSEC.2013.133},
	series = {{APSEC} '13},
	shorttitle = {Mining {GitHub}},
	abstract = {Using the freeware in {GitHub}, we are often confused by a phenomenon: the new version of {GitHub} freeware usually was released in an indefinite frequency, and developers often committed nothing for a long time. This evolution phenomenon interferes with our own development plan and architecture design. Why do these updates happen at that time? Can we predict {GitHub} software version evolution by developers' activities? This paper aims to explore the developer commit patterns in {GitHub}, and try to mine the relationship between these patterns (if exists) and code evolution. First, we define four metrics to measure commit activity and code evolution: the changes in each commit, the time between two commits, the author of each changes, and the source code dependency. Then, we adopt visualization techniques to explore developers' commit activity and code evolution. Visual techniques are used to describe the progress of the given project and the authors' contributions. To analyze the commit logs in {GitHub} software repository automatically, Commits Analysis Tool ({CAT}) is designed and implemented. Finally, eight open source projects in {GitHub} are analyzed using {CAT}, and we find that: 1) the file changes in the previous versions may affect the file depend on it in the next version, 2) the average days around "huge commit" is 3 times of that around normal commit. Using these two patterns and developer's commit model, we can predict when his next commit comes and which file may be changed in that commit. Such information is valuable for project planning of both {GitHub} projects and other projects which use {GitHub} freeware to develop software.},
	pages = {165--169},
	booktitle = {Proceedings of the 2013 20th Asia-Pacific Software Engineering Conference ({APSEC})},
	publisher = {{IEEE} Computer Society},
	author = {Weicheng, Yang and Beijun, Shen and Ben, Xu},
	urldate = {2014-06-08},
	date = {2013},
	keywords = {commit pattern, {GitHub}, Repository Mining, version evolution, visualization technology}
}

@article{griffin_github_2013,
	title = {{GitHub} in the classroom: not just for group projects},
	volume = {28},
	issn = {1937-4771},
	url = {http://dl.acm.org/citation.cfm?id=2458539.2458551},
	shorttitle = {{GitHub} in the classroom},
	abstract = {Version control is something that students are not usually introduced to until they get to industry. One popular distributed version control system known as "Git", has become even more popular in recent years as the basis for a popular website called "{GitHub}". It is a web-based hosting service for software development projects that utilize the Git revision control system. Github is hugely popular in industry and many companies use programmers' {GitHub} portfolios as a basis for hiring. The only downside to using a site like {GitHub} is it emphasizes "social coding", meaning individuals can not only see, but can "clone" your code to either start working on a project with you, or take it in a different direction. This obviously will not work for standard programming assignments, where students need to keep projects private. {GitHub} has realized this problem, and offers university students free private repositories to work on projects in school. One simple example that motivated me to start using Git in the classroom was the ability to "branch" and "merge". We have all seen a students flash drive contents with: Prog1v1, Prog1v2, Prog1v3... Prog1vN, or at least something similar. What these students have essentially done is "branch" their own code, with no real convenient method of getting all their changes and additions, back into Prog1v1. I teach incremental programming, and urge the student to get something working, before adding the next component. If a student uses Git, when they reach a point where the code is stable, they can simply create a new "branch", switch to that branch, work on that branch, then when it's working, "merge" it back into the original Prog1v1. Or, if it's totally off course, abandon that branch and move in another direction, without destroying the original Prog1v1. In addition, some of the questions I asked myself before using {GitHub} in the classroom, were: 1) Is version control necessary or worth the extra confusion it may add to the already heavy workload in a programming class? 2) What level of student should version control be introduced to? 3) Is distributed version control only good for group projects, or will individual students benefit? All these questions, plus more, will be answered in a hands on approach to learning Git and {GitHub}.},
	pages = {74--74},
	number = {4},
	journal = {J. Comput. Sci. Coll.},
	author = {Griffin, Terry and Seals, Shawn},
	urldate = {2013-10-23},
	date = {2013-04},
	%note = {Cited by 0000}
}

@inproceedings{herzig_mining_2009,
	title = {Mining the Jazz repository: Challenges and opportunities},
	doi = {10.1109/MSR.2009.5069495},
	shorttitle = {Mining the Jazz repository},
	abstract = {By integrating various development and collaboration tools into one single platform, the Jazz environment offers several opportunities for software repository miners. In particular, Jazz offers full traceability from the initial requirements via work packages and work assignments to the final changes and tests; all these features can be easily accessed and leveraged for better prediction and recommendation systems. In this paper, we share our initial experiences from mining the Jazz repository. We also give a short overview of the retrieved data sets and discuss possible problems of the Jazz repository and the platform itself.},
	pages = {159 --162},
	booktitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	author = {Herzig, K. and Zeller, A.},
	date = {2009-05},
	%note = {Cited by 0008},
	keywords = {collaboration tools, collaboration tools, Collaborative software, Collaborative software, Collaborative tools, Collaborative tools, Collaborative work, Collaborative work, data mining, data mining, full traceability, full traceability, groupware, groupware, information retrieval, information retrieval, Jazz environment, Jazz environment, Jazz repository mining, Jazz repository mining, Joining processes, Joining processes, Packaging, Packaging, Programming, Programming, recommendation systems, recommendation systems, retrieved data sets, retrieved data sets, software repository miners, software repository miners, software tools, software tools, System testing, System testing, work assignments, work assignments, work packages, work packages}
}

@inproceedings{van_der_lingen_experimental_2004,
	title = {An experimental, pluggable infrastructure for modular configuration management policy composition},
	doi = {10.1109/ICSE.2004.1317479},
	abstract = {Building a configuration management ({CM}) system is a difficult endeavor that regularly requires tens of thousands of lines of code to be written. To reduce this effort, several experimental infrastructures have been developed that provide reusable repositories upon which to build a {CM} system. In this paper, we push the idea of reusability even further. Whereas existing infrastructures only reuse a generic {CM} model (i.e., the data structures used to capture the evolution of artifacts), we have developed an experimental infrastructure, called {MCCM}, that additionally allows reuse of {CM} policies (i.e., the rules by which a user evolves artifacts stored in a {CM} system). The key contribution underlying {MCCM} is that a {CM} policy is not a monolithic entity; instead, it can be composed from small modules that each addresses a unique dimension of concern. Using the pluggable architecture and base set of modules of {MCCM}, then, the core of a desired new {CM} system can be rapidly composed by choosing appropriate existing modules and implementing any remaining modules only as needed. We demonstrate our approach by showing how the use of {MCCM} significantly reduces the effort involved in creating several representative {CM} systems.},
	pages = {573--582},
	booktitle = {26th International Conference on Software Engineering, 2004. {ICSE} 2004. Proceedings},
	author = {van der Lingen, R. and Van der Hoek, Andre},
	date = {2004},
	keywords = {Computer Science, Computer Science, configuration management, configuration management, data structures, data structures, experimental infrastructure, experimental infrastructure, Graphical user interfaces, Graphical user interfaces, Informatics, Informatics, {MCCM}, {MCCM}, pluggable infrastructure, pluggable infrastructure, policy composition, policy composition, software engineering, software engineering, software reusability, software reusability}
}

@inproceedings{dambros_extensive_2010,
	title = {An extensive comparison of bug prediction approaches},
	doi = {10.1109/MSR.2010.5463279},
	abstract = {Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available data set consisting of several software systems, and provide an extensive comparison of the explanative and predictive power of well-known bug prediction approaches, together with novel approaches we devised. Based on the results, we discuss the performance and stability of the approaches with respect to our benchmark and deduce a number of insights on bug prediction models.},
	pages = {31--41},
	booktitle = {2010 7th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {D'Ambros, M. and Lanza, M. and Robbes, R.},
	date = {2010},
	%note = {Cited by 0060},
	keywords = {bug prediction approaches, bug prediction approaches, Computer bugs, Computer bugs, Computer Science, Computer Science, Entropy, Entropy, Informatics, Informatics, Open source software, Open source software, Power system modeling, Power system modeling, Predictive models, Predictive models, program debugging, program debugging, resource allocation, resource allocation, resource allocation problem, resource allocation problem, software defects, software defects, software engineering, software engineering, Software systems, Software systems, Stability, Stability}
}

@article{jiang_techniques_2008,
	title = {Techniques for evaluating fault prediction models},
	volume = {13},
	issn = {1382-3256, 1573-7616},
	url = {http://link.springer.com/article/10.1007/s10664-008-9079-3},
	doi = {10.1007/s10664-008-9079-3},
	abstract = {Many statistical techniques have been proposed to predict fault-proneness of program modules in software engineering. Choosing the â€œbestâ€ candidate among many available models involves performance assessment and detailed comparison, but these comparisons are not simple due to the applicability of varying performance measures. Classifying a software module as fault-prone implies the application of some verification activities, thus adding to the development cost. Misclassifying a module as fault free carries the risk of system failure, also associated with cost implications. Methodologies for precise evaluation of fault prediction models should be at the core of empirical software engineering research, but have attracted sporadic attention. In this paper, we overview model evaluation techniques. In addition to many techniques that have been used in software engineering studies before, we introduce and discuss the merits of cost curves. Using the data from a public repository, our study demonstrates the strengths and weaknesses of performance evaluation techniques and points to a conclusion that the selection of the â€œbestâ€ model cannot be made without considering project cost characteristics, which are specific in each development environment.},
	pages = {561--595},
	number = {5},
	journal = {Empirical Software Engineering},
	author = {Jiang, Yue and Cukic, Bojan and Ma, Yan},
	urldate = {2013-08-01},
	date = {2008-10},
	langid = {english},
	%note = {Cited by 0050},
	keywords = {Compilers, empirical studies, empirical studies, Fault-prediction models, Fault-prediction models, Interpreters, Model evaluation, Model evaluation, Predictive models in software engineering, Predictive models in software engineering, programming languages, Programming Languages, Compilers, Interpreters, Software Engineering/Programming and Operating Systems, Software Engineering/Programming and Operating Systems}
}

@misc{briandoll_10_Million,
	title = {10 Million Repositories},
	howpublished = {https://github.com/blog/1724-10-million-repositories},
	abstract = {What an incredible and productive year! A few days ago the 10 millionth repository was created on {GitHub}.com, just in time for the new year.

The first million repositories were created in just under 4 years; 3 years, 8 months and 15 days to be exact. This last million took just 48 days. In fact, over 5.5M repositories — more than half of the repositories on the site — were created this year alone.

To celebrate, here's a look at some of the popular, interesting, and noteworthy projects that came online in 2013:

Project Open Data, the repo behind the White House's Open Data Policy
Docker, the open source application container engine
Tate Metadata, metadata for 70,000 works of art that the Tate owns or jointly owns with the National Galleries of Scotland
Lots of great projects released by Amazon [2] [3], Facebook, Google, Microsoft, Netflix, Square, Twitter, and Walmart [2], just to name a few.
Check out the Explore and Trending pages to find more great stuff on GitHub
Here's a quick   showing the path to 10M:



We'll be sharing some more stats from the year soon, but for now, happy holidays and here's to the next 10 million!},
	titleaddon = {GitHub},
	author = {Briandoll},
	note = {Accessed: 2014-06-25},
	%note = {00000},
	file = {Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\HFBHEJUV\\1724-10-million-repositories.html:text/html}
}

@inproceedings{canfora_how_2005,
	title = {How software repositories can help in resolving a new change request},
	abstract = {In open source development, software evolution tasks are usually managed with a bug tracker system, such as Bugzilla [1], and a versioning system, such as {CVS} [2]. This provides for a huge amount of historical data regarding bug resolutions and new enhancement feature implementations. We discuss how software repositories can help developers in managing a new change request, either a bug or an enhancement feature. The hypothesis is that data stored in software repositories are a good descriptor on how past change requests have been resolved. Textual descriptions of fixed change requests stored in software repositories, both Bugzilla and {CVS}, are used to index developers and source files as documents in an information retrieval system. For a new change request, such indexes can be useful to identify the most appropriate developers to resolve it, or to predict the set of impacted source files. 1.},
	booktitle = {In Workshop on Empirical Studies in Reverse Engineering},
	author = {Canfora, Gerardo and Cerulo, Luigi},
	date = {2005},
	%note = {Cited by 0036}
}

@article{peng_user_2012,
	title = {User preferences based software defect detection algorithms selection using {MCDM}},
	volume = {191},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025510001751},
	doi = {10.1016/j.ins.2010.04.019},
	abstract = {A variety of classification algorithms for software defect detection have been developed over the years. How to select an appropriate classifier for a given task is an important issue in Data mining and knowledge discovery ({DMKD}). Many studies have compared different types of classification algorithms and the performances of these algorithms may vary using different performance measures and under different circumstances. Since the algorithm selection task needs to examine several criteria, such as accuracy, computational time, and misclassification rate, it can be modeled as a multiple criteria decision making ({MCDM}) problem. The goal of this paper is to use a set of {MCDM} methods to rank classification algorithms, with empirical results based on the software defect detection datasets. Since the preferences of the decision maker ({DM}) play an important role in algorithm evaluation and selection, this paper involved the {DM} during the ranking procedure by assigning user weights to the performance measures. Four {MCDM} methods are examined using 38 classification algorithms and 13 evaluation criteria over 10 public-domain software defect datasets. The results indicate that the boosting of {CART} and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. Though the {MCDM} methods provide some conflicting results for the selected software defect datasets, they agree on most top-ranked classification algorithms.},
	pages = {3--13},
	journal = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Peng, Yi and Wang, Guoxun and Wang, Honggang},
	urldate = {2013-09-12},
	date = {2012-05-15},
	%note = {Cited by 0029},
	keywords = {Algorithm selection, Classification algorithm, Knowledge-driven data mining, Multi-criteria decision making ({MCDM}), Software defect detection}
}

@article{levy_distributed_1990,
	title = {Distributed file systems: concepts and examples},
	volume = {22},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/98163.98169},
	doi = {10.1145/98163.98169},
	shorttitle = {Distributed file systems},
	abstract = {The purpose of a distributed file system ({DFS}) is to allow users of physically distributed computers to share data and storage resources by using a common file system. A typical configuration for a {DFS} is a collection of workstations and mainframes connected by a local area network ({LAN}). A {DFS} is implemented as part of the operating system of each of the connected computers. This paper establishes a viewpoint that emphasizes the dispersed structure and decentralization of both data and control in the design of such systems. It defines the concepts of transparency, fault tolerance, and scalability and discusses them in the context of {DFSs}. The paper claims that the principle of distributed operation is fundamental for a fault tolerant and scalable {DFS} design. It also presents   alternatives for the semantics of sharing and methods for providing access to remote files. A survey of contemporary {UNIX}-based systems, namely, {UNIX} United, Locus, Sprite, Sun's Network File System, and {ITC}'s Andrew, illustrates the concepts and demonstrates various implementations and design alternatives. Based on the assessment of these systems, the paper makes the point that a departure from the extending centralized file systems over a communication network is necessary to accomplish sound distributed file system design.},
	pages = {321--374},
	number = {4},
	journal = {{ACM} Comput. Surv.},
	author = {Levy, Eliezer and Silberschatz, Abraham},
	urldate = {2012-11-14},
	date = {1990-12},
	%note = {Cited by 0268}
}

@inproceedings{cubranic_automatic_2004,
	title = {Automatic bug triage using text categorization},
	abstract = {Bug triage, deciding what to do with an incoming bug report, is taking up increasing amount of developer resources in large open-source projects. In this paper, we propose to apply machine learning techniques to assist in bug triage by using text categorization to predict the developer that should work on the bug based on the bug’s description. We demonstrate our approach on a collection of 15,859 bug reports from a large open-source project. Our evaluation shows that our prototype, using supervised Bayesian learning, can correctly predict 30 \% of the report assignments to developers. 1},
	pages = {92--97},
	publisher = {{KSI} Press},
	author = {Čubranić, Davor},
	date = {2004},
	%note = {Cited by 0141},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\SBHVAMV8\\Čubranić - 2004 - Automatic bug triage using text categorization.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\R4X4MN85\\summary.html:text/html}
}

@inproceedings{kiefer_mining_2007,
	location = {Washington, {DC}, {USA}},
	title = {Mining Software Repositories with {iSPAROL} and a Software Evolution Ontology},
	isbn = {0-7695-2950-X},
	url = {http://dx.doi.org/10.1109/MSR.2007.21},
	doi = {10.1109/MSR.2007.21},
	series = {{MSR} '07},
	abstract = {One of the most important decisions researchers face when analyzing the evolution of software systems is the choice of a proper data analysis/exchange format. Most existing formats have to be processed with special programs written specifically for that purpose and are not easily extendible. Most scientists, therefore, use their own database( s) requiring each of them to repeat the work of writing the import/export programs to their format. We present {EvoOnt}, a software repository data exchange format based on the Web Ontology Language ({OWL}). {EvoOnt} includes software, release, and bug-related information. Since {OWL} describes the semantics of the data, {EvoOnt} is (1) easily extendible, (2) comes with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The paper also shows {iSPARQL} – our {SPARQL}-based Semantic Web query engine containing similarity joins. Together with {EvoOnt}, {iSPARQL} can accomplish a sizable number of tasks sought in software repository mining projects, such as an assessment of the amount of change between versions or the detection of bad code smells. To illustrate the usefulness of {EvoOnt} (and {iSPARQL}), we perform a series of experiments with a real-world Java project. These show that a number of software analyses can be reduced to simple {iSPARQL} queries on an {EvoOnt} dataset.},
	pages = {10â€“},
	booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
	publisher = {{IEEE} Computer Society},
	author = {Kiefer, Christoph and Bernstein, Abraham and Tappolet, Jonas},
	urldate = {2012-07-09},
	date = {2007},
	%note = {Cited by 0051}
}

@inproceedings{ghezzi_replicating_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Replicating mining studies with {SOFAS}},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487152},
	series = {{MSR} '13},
	abstract = {The replication of studies in mining software repositories ({MSR}) is essential to compare different mining techniques or assess their findings across many projects. However, it has been shown that very few of these studies can be easily replicated. Their replication is just as fundamental as the studies themselves and is one of the main threats to validity that they suffer from. In this paper, we show how we can alleviate this problem with our {SOFAS} framework. {SOFAS} is a platform that enables a systematic and repeatable analysis of software projects by providing extensible and composable analysis workflows. These workflows can be applied on a multitude of software projects, facilitating the replication and scaling of mining studies. In this paper, we show how and to which degree replication can be achieved. We investigated the mining studies of {MSR} from 2004 to 2011 and found that from 88 studies published in the {MSR} proceedings so far, we can fully replicate 25 empirical studies. Additionally, we can replicate 27 additional studies to a large extent. These studies account for 30\% and 32\%, respectively, of the mining studies published. To support our claim we describe in detail one large study that we replicated and discuss how replication with {SOFAS} works for the other studies investigated. To discuss the potential of our platform we also characterise how studies can be easily enriched to deliver even more comprehensive answers by extending the analysis workflows provided by the platform.},
	pages = {363--372},
	publisher = {{IEEE} Press},
	author = {Ghezzi, Giacomo and Gall, Harald C.},
	urldate = {2013-07-03},
	date = {2013},
	%note = {Cited by 0000}
}

@inproceedings{shihab_industrial_2012,
	location = {New York, {NY}, {USA}},
	title = {An industrial study on the risk of software changes},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393670},
	doi = {10.1145/2393596.2393670},
	series = {{FSE} '12},
	abstract = {Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67\%, and a precision improvement of 87\% (using developer specific models) and 37\% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.},
	pages = {62:1â€“62:11},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming},
	urldate = {2013-07-03},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {bug inducing changes, bug inducing changes, change metrics, change metrics, change risk, change risk, code metrics, code metrics}
}

@inproceedings{kim_ta-re:_2006,
	location = {New York, {NY}, {USA}},
	title = {{TA}-{RE}: an exchange language for mining software repositories},
	isbn = {1-59593-397-2},
	url = {http://doi.acm.org/10.1145/1137983.1137990},
	doi = {10.1145/1137983.1137990},
	series = {{MSR} '06},
	shorttitle = {{TA}-{RE}},
	abstract = {Software repositories have been getting a lot of attention from researchers in recent years. In order to analyze software repositories, it is necessary to first extract raw data from the version control and problem tracking systems. This poses two challenges: (1) extraction requires a non-trivial effort, and (2) the results depend on the heuristics used during extraction. These challenges burden researchers that are new to the community and make it difficult to benchmark software repository mining since it is almost impossible to reproduce experiments done by another team. In this paper we present the {TA}-{RE} corpus. {TA}-{RE} collects extracted data from software repositories in order to build a collection of projects that will simplify extraction process. Additionally the collection can be used for benchmarking. As the first step we propose an exchange language capable of making sharing and reusing data as simple as possible.},
	pages = {22--25},
	booktitle = {Proceedings of the 2006 international workshop on Mining software repositories},
	publisher = {{ACM}},
	author = {Kim, Sunghun and Zimmermann, Thomas and Kim, Miryung and Hassan, Ahmed and Mockus, Audris and Girba, Tudor and Pinzger, Martin and Whitehead,Jr., E. James and Zeller, Andreas},
	urldate = {2012-07-09},
	date = {2006},
	%note = {Cited by 0020},
	keywords = {analysis, corpus, prediction, software repository mining},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\XQTIA8P7\\Kim et al. - 2006 - TA-RE an exchange language for mining software re.pdf:application/pdf}
}

@inproceedings{dallmeier_extraction_2007,
	location = {New York, {NY}, {USA}},
	title = {Extraction of bug localization benchmarks from history},
	isbn = {978-1-59593-882-4},
	url = {http://doi.acm.org/10.1145/1321631.1321702},
	doi = {10.1145/1321631.1321702},
	series = {{ASE} '07},
	abstract = {Researchers have proposed a number of tools for automatic bug localization. Given a program and a description of the failure, such tools pinpoint a set of statements that are most likely to contain the bug. Evaluating bug localization tools is a difficult task because existing benchmarks are limited in size of subjects and number of bugs. In this paper we present {iBUGS}, an approach that semiautomatically extracts benchmarks for bug localization from the history of a project. For {ASPECTJ}, we extracted 369 bugs, 223 out of these had associated test cases. We demonstrate the relevance of our dataset with a case study on the bug localization tool {AMPLE}},
	pages = {433â€“436},
	booktitle = {Proceedings of the twenty-second {IEEE}/{ACM} international conference on Automated software engineering},
	publisher = {{ACM}},
	author = {Dallmeier, Valentin and Zimmermann, Thomas},
	urldate = {2013-03-02},
	date = {2007},
	%note = {Cited by 0068},
	keywords = {benchmarking, benchmarking, defect localization, defect localization}
}

@thesis{kim_adaptive_2006,
	title = {{ADAPTIVE} {BUG} {PREDICTION} {BY} {ANALYZING} {PROJECT} {HISTORY}},
	type = {phdthesis},
	author = {Kim, Sung},
	date = {2006}
}

@article{lu_ability_2011,
	title = {The ability of object-oriented metrics to predict change-proneness: a meta-analysis},
	volume = {17},
	issn = {1382-3256, 1573-7616},
	url = {http://link-springer-com.ez24.periodicos.capes.gov.br/article/10.1007/s10664-011-9170-z},
	doi = {10.1007/s10664-011-9170-z},
	shorttitle = {The ability of object-oriented metrics to predict change-proneness},
	pages = {200--242},
	number = {3},
	journal = {Empirical Software Engineering},
	author = {Lu, Hongmin and Zhou, Yuming and Xu, Baowen and Leung, Hareton and Chen, Lin},
	urldate = {2013-06-19},
	date = {2011-07-21},
	%note = {Cited by 0000},
	file = {The ability of object-oriented metrics to predict change-proneness\: a meta-analysis - Springer:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\JA562PI2\\s10664-011-9170-z.html:text/html}
}

@inproceedings{wolf_predicting_2009,
	location = {Washington, {DC}, {USA}},
	title = {Predicting build failures using social network analysis on developer communication},
	isbn = {978-1-4244-3453-4},
	url = {http://dx.doi.org/10.1109/ICSE.2009.5070503},
	doi = {10.1109/ICSE.2009.5070503},
	series = {{ICSE} '09},
	abstract = {A critical factor in work group coordination, communication has been studied extensively. Yet, we are missing objective evidence of the relationship between successful coordination outcome and communication structures. Using data from {IBM}'s Jazzâ„¢ project, we study communication structures of development teams with high coordination needs. We conceptualize coordination outcome by the result of their code integration build processes (successful or failed) and study team communication structures with social network measures. Our results indicate that developer communication plays an important role in the quality of software integrations. Although we found that no individual measure could indicate whether a build will fail or succeed, we leveraged the combination of communication structure measures into a predictive model that indicates whether an integration will fail. When used for five project teams, our predictive model yielded recall values between 55\% and 75\%, and precision values between 50\% to 76\%.},
	pages = {1â€“11},
	booktitle = {Proceedings of the 31st International Conference on Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Wolf, Timo and Schroter, Adrian and Damian, Daniela and Nguyen, Thanh},
	urldate = {2013-03-28},
	date = {2009},
	%note = {Cited by 0077}
}

@inproceedings{kononenko_mining_2014,
	location = {New York, {NY}, {USA}},
	title = {Mining Modern Repositories with Elasticsearch},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597091},
	doi = {10.1145/2597073.2597091},
	series = {{MSR} 2014},
	abstract = {Organizations are generating, processing, and retaining data at a rate that often exceeds their ability to analyze it effectively; at the same time, the insights derived from these large data sets are often key to the success of the organizations, allowing them to better understand how to solve hard problems and thus gain competitive advantage. Because this data is so fast-moving and voluminous, it is increasingly impractical to analyze using traditional offline, read-only relational databases.   Recently, new "big data" technologies and architectures, including Hadoop and {NoSQL} databases, have evolved to better support the needs of organizations analyzing such data. In particular, Elasticsearch - a distributed full-text search engine - explicitly addresses issues of scalability, big data search, and performance that relational databases were simply never designed to support. In this paper, we reflect upon our own experience with Elasticsearch and highlight its strengths and weaknesses for performing modern mining software repositories research.},
	pages = {328--331},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Kononenko, Oleksii and Baysal, Olga and Holmes, Reid and Godfrey, Michael W.},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00001},
	keywords = {agile data, Elasticsearch, Scalability},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\EVKPP3TK\\Kononenko et al. - 2014 - Mining Modern Repositories with Elasticsearch.pdf:application/pdf}
}

@article{catal_software_2011,
	title = {Software fault prediction: A literature review and current trends},
	volume = {38},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417410011681},
	doi = {10.1016/j.eswa.2010.10.024},
	shorttitle = {Software fault prediction},
	abstract = {Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.},
	pages = {4626--4636},
	number = {4},
	journal = {Expert Systems with Applications},
	author = {Catal, Cagatay},
	urldate = {2013-06-15},
	date = {2011-04},
	%note = {Cited by 0023},
	keywords = {Automated fault prediction models, Automated fault prediction models, Expert systems, Expert systems, machine learning, machine learning, software engineering, software engineering, Software quality engineering, Software quality engineering, Statistical methods, Statistical methods}
}

@inproceedings{park_requirements_2010,
	location = {Riverton, {NJ}, {USA}},
	title = {Requirements attributes to predict requirements related defects},
	url = {http://dx.doi.org/10.1145/1923947.1923953},
	doi = {10.1145/1923947.1923953},
	series = {{CASCON} '10},
	abstract = {Literature suggests that requirements defects are a very costly problem to fix. Understanding how requirements changes influence the overall quality of software is important. Having some defect predictors at the requirements stage may help the stakeholders avoid making choices that could bring about catastrophic defect numbers at the end or at least be prepared for it. In this paper, six requirements-related attributes are analyzed to discover if they can be used for determining the occurrences of requirements-related defects. We measured two types of attributes: point and aggregate. The point attributes include time estimates, priority and ownership. The aggregate attributes include the number of indirect stakeholders, the number of related stories and the story creation time. Our analysis is based on data from the development of the {IBM} Jazz system. Our result shows that the number of indirect stakeholders and the number of related stories are good predictors for the number of defects, but other attributes show no or little correlation with the defects.},
	pages = {42â€“56},
	booktitle = {Proceedings of the 2010 Conference of the Center for Advanced Studies on Collaborative Research},
	publisher = {{IBM} Corp.},
	author = {Park, Shelly and Maurer, Frank and Eberlein, Armin and Fung, Tak-Shing},
	urldate = {2013-03-28},
	date = {2010},
	%note = {Cited by 0003}
}

@inproceedings{oliveira_towards_2014,
	location = {New York, {NY}, {USA}},
	title = {Towards Semantic Diff of {XML} Documents},
	isbn = {978-1-4503-2469-4},
	url = {http://doi.acm.org/10.1145/2554850.2554893},
	doi = {10.1145/2554850.2554893},
	series = {{SAC} '14},
	abstract = {Applications are increasingly using {XML} to represent semi-structured data and, consequently, a large amount of {XML} documents is available worldwide. As {XML} documents evolve over time, comparing {XML} documents to understand their evolution becomes fundamental. The main focus of existing research for comparing {XML} documents resides in identifying syntactic changes. However, a deeper notion of the change meaning is usually desired. This paper presents an inference-based {XML} evolution approach using Prolog to deal with this problem. Differently from existing {XML} diff approaches, our approach composes multiple syntactic changes, which usually have a common purpose, to infer semantic changes. We evaluated our approach through ten versions of an employment {XML} document. In this evaluation, we could observe that each new version introduced syntactic changes that could be summarized into semantic changes.},
	pages = {833--838},
	booktitle = {Proceedings of the 29th Annual {ACM} Symposium on Applied Computing},
	publisher = {{ACM}},
	author = {Oliveira, Alessandreia and Murta, Leonardo and Braganholo, Vanessa},
	urldate = {2014-09-15},
	date = {2014},
	keywords = {inference, Prolog, semantic diff, semi-structural data management, {XML} documents},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\2KK343WN\\Oliveira et al. - 2014 - Towards Semantic Diff of XML Documents.pdf:application/pdf}
}

@inproceedings{mende_replication_2010,
	location = {New York, {NY}, {USA}},
	title = {Replication of defect prediction studies: problems, pitfalls and recommendations},
	isbn = {978-1-4503-0404-7},
	url = {http://doi.acm.org/10.1145/1868328.1868336},
	doi = {10.1145/1868328.1868336},
	series = {{PROMISE} '10},
	shorttitle = {Replication of defect prediction studies},
	abstract = {Background: The main goal of the {PROMISE} repository is to enable reproducible, and thus verifiable or refutable research. Over time, plenty of data sets became available, especially for defect prediction problems. Aims: In this study, we investigate possible problems and pitfalls that occur during replication. This information can be used for future replication studies, and serve as a guideline for researchers reporting novel results. Method: We replicate two recent defect prediction studies comparing different data sets and learning algorithms, and report missing information and problems. Results: Even with access to the original data sets, replicating previous studies may not lead to the exact same results. The choice of evaluation procedures, performance measures and presentation has a large influence on the reproducibility. Additionally, we show that trivial and random models can be used to identify overly optimistic evaluation measures. Conclusions: The best way to conduct easily reproducible studies is to share all associated artifacts, e.g. scripts and programs used. When this is not an option, our results can be used to simplify the replication task for other researchers.},
	pages = {5:1â€“5:10},
	booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	publisher = {{ACM}},
	author = {Mende, Thilo},
	urldate = {2013-08-01},
	date = {2010},
	%note = {Cited by 0008},
	keywords = {defect prediction model, defect prediction model, replication, replication}
}

@inproceedings{huang_predicting_2009,
	title = {Predicting Defect-Prone Software Modules at Different Logical Levels},
	doi = {10.1109/ICRCCS.2009.19},
	abstract = {Effective software defect estimation can bring cost reduction and efficient resources allocation in software development and testing. Usually, estimation of defect-prone modules is based on the supervised learning of the modules at the same logical level. Various practical issues may limit the availability or quality of the attribute-value vectors extracting from the high-level modules by software metrics. In this paper, the problem of estimating the defect in high-level software modules is investigated with a multi-instance learning ({MIL}) perspective. In detail, each high-level module is regarded as a bag of its low-level components, and the learning task is to estimate the defect-proneness of the bags. Several typical supervised learning and {MIL} algorithms are evaluated on a mission critical project from {NASA}. Compared to the selected supervised schemas, the {MIL} methods improve the performance of the software defect estimation models.},
	pages = {37--40},
	booktitle = {International Conference on Research Challenges in Computer Science, 2009. {ICRCCS} '09},
	author = {Huang, Peng and Zhu, Jie},
	date = {2009},
	%note = {Cited by 0001},
	keywords = {attribute-value vectors extracting, attribute-value vectors extracting, Availability, Availability, bags defect-proneness, bags defect-proneness, cost reduction, cost reduction, Costs, Costs, defect-prone software modules, defect-prone software modules, high level modules, high level modules, high level software modules, high level software modules, kernel methods, kernel methods, learning (artificial intelligence), learning (artificial intelligence), learning task, learning task, Mission critical systems, Mission critical systems, multi-instance learning, multi-instance learning, {NASA}, {NASA}, Programming, Programming, program testing, program testing, Resource management, Resource management, software defect estimation, software defect estimation, software defect estimation model, software defect estimation model, software development, software development, Software metrics, Software metrics, Software performance, Software performance, Software testing, Software testing, supervised learning, supervised learning, supervised schema, supervised schema, support vector machine, support vector machine}
}

@inproceedings{nguyen_global_2008,
	title = {Global Software Development and Delay: Does Distance Still Matter?},
	doi = {10.1109/ICGSE.2008.39},
	shorttitle = {Global Software Development and Delay},
	abstract = {Nowadays, distributed development is common in software development. Besides many advantages, research in the last decade has consistently found that distribution has a negative impact on collaboration in general, and communication and task completion time in particular. Adapted processes, practices and tools are demanded to overcome these challenges. We report on an empirical study of communication structures and delay, as well as task completion times in {IBM}'s distributed development project Jazz. The Jazz project explicitly focuses on distributed collaboration and has adapted processes and tools to overcome known challenges. We explored the effect of distance on communication and task completion time and use social network analysis to obtain insights about the collaboration in the Jazz project. We discuss our findings in the light of existing literature on distributed collaboration and delays.},
	eventtitle = {{IEEE} International Conference on Global Software Engineering, 2008. {ICGSE} 2008},
	pages = {45--54},
	author = {Nguyen, T. and Wolf, T. and Damian, D.},
	date = {2008},
	%note = {Cited by 0060},
	keywords = {collaboration, Collaborative software, Collaborative tools, Collaborative work, Communication industry, Delay effects, distributed collaboration, distributed software development, global software development, {IBM} distributed development project, Jazz project, Programming, social network analysis, Social network services, software development management, software engineering, Sun}
}

@book{_bigquery-github-strata.pdf_????,
	title = {bigquery-github-strata.pdf},
	url = {http://www.igvita.com/slides/2012/bigquery-github-strata.pdf},
	urldate = {2013-10-25},
	%note = {Cited by 0000}
}

@inproceedings{emad_shihab_understanding_2010,
	location = {September 16-17, 2010, Bolzano-Bozen, Italy.},
	title = {Understanding the Impact of Code and Process Metrics on Post-release Defects: A Case Study on the Eclipse Project},
	isbn = {978-1-4503-0039},
	shorttitle = {{ESEM} ’10},
	abstract = {Research studying the quality of software applications continues to
grow rapidly with researchers building regressionmodels that com-bine a large number of metrics. However, these models are hard to
deploy in practice due to the cost associated with collecting all the
needed metrics, the complexity of the models and the black box
nature of the models. For example, techniques such as {PCA} merge
a large number of metrics into composite metrics that are no longer
easy to explain. In this paper, we use a statistical approach recently
proposed by Cataldo et al. to create explainable regression mod-els. A case study on the Eclipse open source project shows that
only 4 out of the 34 code and process metrics impacts the likeli-hood of finding a post-release defect. In addition, our approach is
able to quantify the impact of these metrics on the likelihood of
finding post-release defects. Finally, we demonstrate that our sim-ple models achieve comparable performance over more complex
{PCA}-based models while providing practitioners with intuitive ex-planations for its predictions},
	author = {Emad Shihab and Zhen Ming Jiang and Walid M. Ibrahim and Bram Adams and Ahmed E. Hassan},
	date = {2010},
	%note = {Cited by 0024}
}

@article{richards_two_2009,
	title = {Two decades of Ripple Down Rules research},
	volume = {24},
	doi = {10.1017/S0269888909000241},
	pages = {159--184},
	number = {2},
	journal = {The Knowledge Engineering Review},
	author = {Richards, Debbie},
	date = {2009},
	%note = {Cited by 0029},
	file = {Cambridge Journals Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\TXCH8BD7\\Richards - 2009 - Two decades of Ripple Down Rules research.html:text/html}
}

@online{center_for_history_and_new_media_guia_????-1,
	title = {Guia de Início Rápido},
	url = {http://zotero.org/support/quick_start_guide},
	author = {Center for History \{and\} New Media},
	%note = {Cited by 0000}
}

@article{nguyen_survey_2008,
	title = {A survey of techniques for internet traffic classification using machine learning},
	volume = {10},
	issn = {1553-877X},
	doi = {10.1109/SURV.2008.080406},
	abstract = {The research community has begun looking for {IP} traffic classification techniques that do not rely on `well {knownÂ}¿ {TCP} or {UDP} port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning ({ML}) techniques to {IP} traffic classification - an inter-disciplinary blend of {IP} networking and data mining techniques. We provide context and motivation for the application of {ML} techniques to {IP} traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of {ML} strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of {ML}-based traffic classifiers in operational {IP} networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.},
	pages = {56--76},
	number = {4},
	journal = {{IEEE} Communications Surveys Tutorials},
	author = {Nguyen, T.T.T. and Armitage, G.},
	date = {2008},
	%note = {Cited by 0335},
	keywords = {Flow clustering, Government, inspection, Internet, Internet Protocol, Intrusion detection, machine learning, Payload inspection, Payloads, Protocols, Real Time, Statistical traffic properties, {TCPIP}, Telecommunication traffic, Telephony, Traffic classification},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\C8GZMV97\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\3B2CRPZM\\Nguyen e Armitage - 2008 - A survey of techniques for internet traffic classi.pdf:application/pdf}
}

@inproceedings{kagdi_comparing_2007,
	title = {Comparing Approaches to Mining Source Code for Call-Usage Patterns},
	doi = {10.1109/MSR.2007.3},
	abstract = {Two approaches for mining function-call usage patterns from source code are compared The first approach, itemset mining, has recently been applied to this problem. The other approach, sequential-pattern mining, has not been previously applied to this problem. Here, a call-usage pattern is a composition of function calls that occur in a function definition. Both approaches look for frequently occurring patterns that represent standard usage of functions and identify possible errors. Itemset mining produces unordered patterns, i.e., sets of function calls, whereas, sequential-pattern mining produces partially ordered patterns, i.e., sequences of function calls. The trade-off between the additional ordering context given by sequential-pattern mining and the efficiency of itemset mining is investigated. The two approaches are applied to the Lima kernel v2.6.14 and results show that mining ordered patterns is worth the additional cost.},
	pages = {20},
	author = {Kagdi, H. and Collard, M.L. and Maletic, J.I.},
	date = {2007-05},
	%note = {Cited by 0023},
	keywords = {call-usage pattern, data mining, itemset mining, Lima kernel v2.6.14, sequential-pattern mining, software engineering, source code}
}

@inproceedings{borg_analyzing_2013,
	title = {Analyzing Networks of Issue Reports},
	doi = {10.1109/CSMR.2013.18},
	abstract = {Completely analyzed and closed issue reports in software development projects, particularly in the development of safety-critical systems, often carry important information about issue-related change locations. These locations may be in the source code, as well as traces to test cases affected by the issue, and related design and requirements documents. In order to help developers analyze new issues, knowledge about issue clones and duplicates, as well as other relations between the new issue and existing issue reports would be useful. This paper analyses, in an exploratory study, issue reports contained in two Issue Management Systems ({IMS}) containing approximately 20.000 issue reports. The purpose of the analysis is to gain a better understanding of relationships between issue reports in {IMSs}. We found that link-mining explicit references can reveal complex networks of issue reports. Furthermore, we found that textual similarity analysis might have the potential to complement the explicitly signaled links by recommending additional relations. In line with work in other fields, links between software artifacts have a potential to improve search and navigation in large software engineering projects.},
	eventtitle = {2013 17th European Conference on Software Maintenance and Reengineering ({CSMR})},
	pages = {79--88},
	author = {Borg, M. and Pfahl, D. and Runeson, P.},
	date = {2013},
	%note = {Cited by 0001},
	keywords = {change locations, data mining, impact analysis, {IMS}, information retrieval, issue clones, issue duplicates, issue management systems, issue reports, link mining, link-mining explicit references, network analysis, project management, safety-critical software, safety-critical system development, safety development, software artifacts, software development projects, software engineering projects, software management, source code, test cases, textual similarity analysis}
}

@inproceedings{matragkas_analysing_2014,
	location = {New York, {NY}, {USA}},
	title = {Analysing the 'Biodiversity' of Open Source Ecosystems: The {GitHub} Case},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597119},
	doi = {10.1145/2597073.2597119},
	series = {{MSR} 2014},
	shorttitle = {Analysing the 'Biodiversity' of Open Source Ecosystems},
	abstract = {In nature the diversity of species and genes in ecological communities affects the functioning of these communities. Biologists have found out that more diverse communities appear to be more productive than less diverse communities. Moreover such communities appear to be more stable in the face of perturbations. In this paper, we draw the analogy between ecological communities and Open Source Software ({OSS}) ecosystems, and we investigate the diversity and structure of {OSS} communities. To address this question we use the {MSR} 2014 challenge dataset, which includes data from the top-10 software projects for the top programming languages on {GitHub}. Our findings show that {OSS} communities on {GitHub} consist of 3 types of users (core developers, active users, passive users). Moreover, we show that the percentage of core developers and active users does not change as the project grows and that the majority of members of large projects are passive users.},
	pages = {356--359},
	booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
	publisher = {{ACM}},
	author = {Matragkas, Nicholas and Williams, James R. and Kolovos, Dimitris S. and Paige, Richard F.},
	urldate = {2014-06-06},
	date = {2014},
	%note = {00000},
	keywords = {Data and knowledge visualization, data mining},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CA2QZ85J\\Matragkas et al. - 2014 - Analysing the 'Biodiversity' of Open Source Ecosys.pdf:application/pdf}
}

@inproceedings{mcdonald_performance_2013,
	location = {New York, {NY}, {USA}},
	title = {Performance and Participation in Open Source Software on {GitHub}},
	isbn = {978-1-4503-1952-2},
	url = {http://doi.acm.org/10.1145/2468356.2468382},
	doi = {10.1145/2468356.2468382},
	series = {{CHI} {EA} '13},
	abstract = {A few studies have attempted to provide metrics of success in open source software ({OSS}) projects but the role a code hosting workspace plays in how performance is viewed and measured is little examined. We conducted qualitative, exploratory research with lead and core developers on three successful projects on {GitHub} to understand how {OSS} communities on {GitHub} measure success. These results were obtained in connection with a larger project that is designed to understand the structure of code hosting platforms in relation to participation and performance. We report two main findings. First, lead and core members of the projects we interviewed display a nuanced understanding of community participation in their assessment of success. Second, they attribute increased participation on their projects to the features and usability provided by {GitHub}.},
	pages = {139--144},
	booktitle = {{CHI} '13 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {{McDonald}, Nora and Goggins, Sean},
	urldate = {2014-06-09},
	date = {2013},
	keywords = {Open source software, performance, social computing},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ZE2XITGZ\\McDonald e Goggins - 2013 - Performance and Participation in Open Source Softw.pdf:application/pdf}
}

@inproceedings{abdelmoez_improving_2013,
	title = {Improving bug fix-time prediction model by filtering out outliers},
	doi = {10.1109/TAEECE.2013.6557301},
	abstract = {Bug fix time prediction models have been used to predict the fix-time of newly reported bugs in order to help out the developer during the triaging process by prioritizing which bugs to fix first. While constructing these prediction models, we deal with large data sets. It is very likely that these data sets contain outliers that would affect the predictive power of the prediction models. For example, conspicuous bugs are those taking less than a few minutes to get fixed. Also, there are other bugs that take years to get fixed. To improve the quality of the prediction models, a filtering step was proposed to remove these outliers. Our objective is to improve the accuracy of the prediction models by eliminating the effect of these kinds of bugs. Thus in this paper, we examine the distribution of fix-time attribute to identify clearly the potential outliers relative to the data sets. Therefore, we identify several thresholds for filtering out data sets. Also in general, Filtering out using the mild outlier threshold outperforms all other thresholds; bugs were correctly classified into fast which denotes 71\% of the fast bugs were classified correctly.},
	eventtitle = {2013 International Conference on Technological Advances in Electrical, Electronics and Computer Engineering ({TAEECE})},
	pages = {359--364},
	author = {{AbdelMoez}, W. and Kholief, M. and Elsalmy, F.M.},
	date = {2013},
	%note = {Cited by 0000},
	keywords = {bug elimination, bug fix-time prediction model, bug fix-time prediction models, Computer bugs, dataset filtering, filtering outliers, filtering step, fix-time attribute distribution, improving prediction models, outlier filtering, outlier threshold, Predictive models, program debugging, triaging process}
}

@report{hall_correlation-based_1999,
	title = {Correlation-based Feature Selection for Machine Learning},
	abstract = {A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. {CFS} (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. {CFS} was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), {IB}1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that {CFS} quickly identifies and screens irrelevant, redundant,...},
	author = {Hall, Mark A.},
	date = {1999},
	%note = {02438},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ZBP73QPM\\Hall - 1999 - Correlation-based Feature Selection for Machine Le.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\KMKISQTE\\summary.html:text/html}
}

@inproceedings{bhattacharya_bug-fix_2011,
	location = {New York, {NY}, {USA}},
	title = {Bug-fix time prediction models: can we do better?},
	isbn = {978-1-4503-0574-7},
	url = {http://doi.acm.org/10.1145/1985441.1985472},
	doi = {10.1145/1985441.1985472},
	series = {{MSR} '11},
	shorttitle = {Bug-fix time prediction models},
	abstract = {Predicting bug-fix time is useful in several areas of software evolution, such as predicting software quality or coordinating development effort during bug triaging. Prior work has proposed bug-fix time prediction models that use various bug report attributes (e.g., number of developers who participated in fixing the bug, bug severity, number of patches, bug-opener's reputation) for estimating the time it will take to fix a newly-reported bug. In this paper we take a step towards constructing more accurate and more general bug-fix time prediction models by showing how existing models fail to validate on large projects widely-used in bug studies. In particular, we used multivariate and univariate regression testing to test the prediction significance of existing models on 512,474 bug reports from five open source projects: Eclipse, Chrome and three products from the Mozilla project (Firefox, Seamonkey and Thunderbird). The results of our regression testing indicate that the predictive power of existing models is between 30\% and 49\% and that there is a need for more independent variables (attributes) when constructing a prediction model. Additionally, we found that, unlike in prior recent studies on commercial software, in the projects we examined there is no correlation between bug-fix likelihood, bug-opener's reputation and the time it takes to fix a bug. These findings indicate three open research problems: (1) assessing whether prioritizing bugs using bug-opener's reputation is beneficial, (2) identifying attributes which are effective in predicting bug-fix time, and (3) constructing bug-fix time prediction models which can be validated on multiple projects.},
	pages = {207--210},
	publisher = {{ACM}},
	author = {Bhattacharya, Pamela and Neamtiu, Iulian},
	urldate = {2013-08-22},
	date = {2011},
	%note = {Cited by 0010},
	keywords = {bug-fix time, bug report triage, issue tracking, statistical model}
}

@inproceedings{menzies_automated_2008,
	title = {Automated severity assessment of software defect reports},
	doi = {10.1109/ICSM.2008.4658083},
	abstract = {In mission critical systems, such as those developed by {NASA}, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue. The paper presents a new and automated method named {SEVERIS} (severity issue assessment), which assists the test engineer in assigning severity levels to defect reports. {SEVERIS} is based on standard text mining and machine learning techniques applied to existing sets of defect reports. A case study on using {SEVERIS} with data from {NASApsilas} Project and Issue Tracking System ({PITS}) is presented in the paper. The case study results indicate that {SEVERIS} is a good predictor for issue severity levels, while it is easy to use and efficient.},
	pages = {346--355},
	booktitle = {{IEEE} International Conference on Software Maintenance, 2008. {ICSM} 2008},
	author = {Menzies, T. and Marcus, A.},
	date = {2008},
	%note = {Cited by 0036},
	keywords = {automated severity assessment, automated severity assessment, Automatic testing, Automatic testing, Computer bugs, Computer bugs, Computer Science, Computer Science, Costs, Costs, data mining, data mining, learning (artificial intelligence), learning (artificial intelligence), machine learning techniques, machine learning techniques, Mission critical systems, Mission critical systems, {NASA}, {NASA}, personnel, personnel, resource allocation, resource allocation, Robots, Robots, severity issue assessment, severity issue assessment, software defect reports, software defect reports, software engineering, software engineering, Software testing, Software testing, System testing, System testing, text mining, text mining}
}

@article{catal_software_2012,
	title = {Software mining and fault prediction},
	volume = {2},
	rights = {Copyright © 2012 John Wiley \& Sons, Inc.},
	issn = {1942-4795},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/widm.1067/abstract},
	doi = {10.1002/widm.1067},
	abstract = {Mining software repositories ({MSRs}) such as source control repositories, bug repositories, deployment logs, and code repositories provide useful patterns for practitioners. Instead of using these repositories as record-keeping ones, we need to transform them into active repositories that can guide the decision processes inside the company. By {MSRs} with several data mining algorithms, effective software fault prediction models can be built and error-prone modules can be detected prior to the testing phase. We discuss numerous real-world challenges in building accurate fault prediction models and present some solutions to these challenges. © 2012 Wiley Periodicals, Inc.},
	pages = {420--426},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Catal, Cagatay},
	urldate = {2013-07-10},
	date = {2012},
	langid = {english},
	%note = {Cited by 0000}
}

@article{conradi_version_1998,
	title = {Version models for software configuration management},
	volume = {30},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/280277.280280},
	doi = {10.1145/280277.280280},
	abstract = {After more than 20 years of research and practice in software configuration management ({SCM}), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the {SCM} discipline and classify them according to a detailed taxonomy.},
	pages = {232â€“282},
	number = {2},
	journal = {{ACM} Comput. Surv.},
	author = {Conradi, Reidar and Westfechtel, Bernhard},
	urldate = {2012-07-09},
	date = {1998-06},
	%note = {Cited by 0698},
	keywords = {changes, changes, configuration rules, configuration rules, configurations, configurations, revisions, revisions, variants, variants, versions, versions}
}

@article{salfner_survey_2010,
	title = {A survey of online failure prediction methods},
	volume = {42},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/1670679.1670680},
	doi = {10.1145/1670679.1670680},
	abstract = {With the ever-growing complexity and dynamicity of computer systems, proactive fault management is an effective approach to enhancing availability. Online failure prediction is the key to such techniques. In contrast to classical reliability methods, online failure prediction is based on runtime monitoring and a variety of models and methods that use the current state of a system and, frequently, the past experience as well. This survey describes these methods. To capture the wide spectrum of approaches concerning this area, a taxonomy has been developed, whose different approaches are explained and major concepts are described in detail.},
	pages = {10:1--10:42},
	number = {3},
	journal = {{ACM} Comput. Surv.},
	author = {Salfner, Felix and Lenk, Maren and Malek, Miroslaw},
	urldate = {2013-07-23},
	date = {2010-03},
	%note = {Cited by 0115},
	keywords = {Error, failure prediction, fault, prediction metrics, runtime monitoring},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\INEAM274\\Salfner et al. - 2010 - A survey of online failure prediction methods.pdf:application/pdf}
}

@article{gray_software_2013,
	title = {Software Defect Prediction Using Static Code Metrics: Formulating a Methodology},
	url = {http://uhra.herts.ac.uk/handle/2299/11067},
	shorttitle = {Software Defect Prediction Using Static Code Metrics},
	abstract = {Software defect prediction is motivated by the huge costs incurred as a result of
software failures. In an effort to reduce these costs, researchers have been utilising
software metrics to try and build predictive models capable of locating the most
defect-prone parts of a system. These areas can then be subject to some form of
further analysis, such as a manual code review. It is hoped that such defect predictors
will enable software to be produced more cost effectively, and/or be of higher
quality.
In this dissertation I identify many data quality and methodological issues in
previous defect prediction studies. The main data source is the {NASA} Metrics
Data Program Repository. The issues discovered with these well-utilised data sets
include many examples of seemingly impossible values, and much redundant data.
The redundant, or repeated data points are shown to be the cause of potentially
serious data mining problems. Other methodological issues discovered include the
violation of basic data mining principles, and the misleading reporting of classifier
predictive performance.
The issues discovered lead to a new proposed methodology for software defect
prediction. The methodology is focused around data analysis, as this appears to
have been overlooked in many prior studies. The aim of the methodology is to be
able to obtain a realistic estimate of potential real-world predictive performance, and
also to have simple performance baselines with which to compare against the actual
performance achieved. This is important as quantifying predictive performance
appropriately is a difficult task.
The findings of this dissertation raise questions about the current defect prediction
body of knowledge. So many data-related and/or methodological errors have
previously occurred that it may now be time to revisit the fundamental aspects of
this research area, to determine what we really know, and how we should proceed.},
	author = {Gray, David Philip Harry},
	urldate = {2013-07-20},
	date = {2013-06-24},
	langid = {english},
	%note = {Cited by 0000},
	keywords = {code metrics, data quality, defect prediction, Fault prediction, machine learning, software engineering},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\M3EB96FR\\Gray - 2013 - Software Defect Prediction Using Static Code Metri.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\4BC65JNX\\11067.html:text/html}
}

@article{sliwerski_when_2005,
	title = {When do changes induce fixes?},
	volume = {30},
	issn = {0163-5948},
	url = {http://doi.acm.org/10.1145/1082983.1083147},
	doi = {10.1145/1082983.1083147},
	abstract = {As a software system evolves, programmers make changes that sometimes cause problems. We analyze {CVS} archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as {CVS}) to a bug database (such as {BUGZILLA}). In a first investigation of the {MOZILLA} and {ECLIPSE} history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.},
	pages = {1--5},
	number = {4},
	journal = {{SIGSOFT} Softw. Eng. Notes},
	author = {Śliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
	urldate = {2013-08-13},
	date = {2005-05},
	%note = {Cited by 0328}
}

@article{vandecruys_mining_2008,
	title = {Mining software repositories for comprehensible software fault prediction models},
	volume = {81},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121207001902},
	doi = {10.1016/j.jss.2007.07.034},
	shorttitle = {Software Process and Product Measurement},
	abstract = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization ({ACO})-based classification technique {AntMiner}+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by {AntMiner}+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the {AntMiner}+ models can be considered superior to the latter models.},
	pages = {823--839},
	number = {5},
	journal = {Journal of Systems and Software},
	author = {Vandecruys, Olivier and Martens, David and Baesens, Bart and Mues, Christophe and De Backer, Manu and Haesen, Raf},
	urldate = {2013-02-27},
	date = {2008-05},
	%note = {Cited by 0040},
	keywords = {Ant Colony Optimization, Ant Colony Optimization, Classification, Classification, Comprehensibility, Comprehensibility, Fault prediction, Fault prediction, Software mining, Software mining}
}

@inproceedings{lamkanfi_eclipse_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {The eclipse and mozilla defect tracking dataset: a genuine dataset for mining bug information},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487125},
	series = {{MSR} '13},
	shorttitle = {The eclipse and mozilla defect tracking dataset},
	abstract = {The analysis of bug reports is an important subfield within the mining software repositories community. It explores the rich data available in defect tracking systems to uncover interesting and actionable information about the bug triaging process. While bug data is readily accessible from systems like Bugzilla and {JIRA}, a common database schema and a curated dataset could significantly enhance future research because it allows for easier replication. Consequently, in this paper we propose the Eclipse and Mozilla Defect Tracking Dataset, a representative database of bug data, filtered to contain only genuine defects (i.e., no feature requests) and designed to cover the whole bug-triage life cycle (i.e., store all intermediate actions). We have used this dataset ourselves for predicting bug severity, for studying bug-fixing time and for identifying erroneously assigned components.},
	pages = {203â€“206},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Lamkanfi, Ahmed and {PÃ}©rez, Javier and Demeyer, Serge},
	urldate = {2013-08-05},
	date = {2013},
	%note = {Cited by 0000}
}

@article{salzberg_c4.5:_1994,
	title = {C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},
	volume = {16},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/BF00993309},
	doi = {10.1007/BF00993309},
	shorttitle = {C4.5},
	pages = {235--240},
	number = {3},
	journal = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Salzberg, Steven L.},
	urldate = {2014-09-02},
	date = {1994-09-01},
	langid = {english},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computing Methodologies, Language Translation and Linguistics, Simulation and Modeling},
	file = {Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\U5IWSBKD\\Salzberg - 1994 - C4.5 Programs for Machine Learning by J. Ross Qui.pdf:application/pdf;Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\WBKH8AJH\\10.html:text/html}
}

@inproceedings{lee_micro_2011,
	location = {New York, {NY}, {USA}},
	title = {Micro interaction metrics for defect prediction},
	isbn = {978-1-4503-0443-6},
	url = {http://doi.acm.org/10.1145/2025113.2025156},
	doi = {10.1145/2025113.2025156},
	series = {{ESEC}/{FSE} '11},
	abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics ({MIMs}) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of {MIMs} in defect prediction, we build defect prediction (classification and regression) models using {MIMs}, traditional metrics, and their combinations. Our experimental results show that {MIMs} significantly improve defect classification and regression accuracy.},
	pages = {311â€“321},
	booktitle = {Proceedings of the 19th {ACM} {SIGSOFT} symposium and the 13th European conference on Foundations of software engineering},
	publisher = {{ACM}},
	author = {Lee, Taek and Nam, Jaechang and Han, {DongGyun} and Kim, Sunghun and In, Hoh Peter},
	urldate = {2013-07-03},
	date = {2011},
	%note = {Cited by 0011},
	keywords = {defect prediction, defect prediction, micro interaction metrics, micro interaction metrics, mylyn, mylyn}
}

@inproceedings{ratzinger_mining_2007,
	title = {Mining Software Evolution to Predict Refactoring},
	doi = {10.1109/ESEM.2007.9},
	abstract = {Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, prepositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects.},
	pages = {354--363},
	booktitle = {First International Symposium on Empirical Software Engineering and Measurement, 2007. {ESEM} 2007},
	author = {Ratzinger, J. and Sigmund, T. and Vorburger, P. and Gall, H.},
	date = {2007},
	%note = {Cited by 0033},
	keywords = {Classification algorithms, Classification algorithms, data mining, data mining, data mining features, data mining features, decision support, decision support, decision trees, decision trees, development history, development history, feature extraction, feature extraction, History, History, information systems, information systems, logistic model trees, logistic model trees, mining software evolution, mining software evolution, object-oriented methods, object-oriented methods, Object oriented modeling, Object oriented modeling, object-oriented systems, object-oriented systems, open source projects, open source projects, Open source software, Open source software, Particle measurements, Particle measurements, Prediction algorithms, Prediction algorithms, prepositional rule learners, prepositional rule learners, software engineering, software engineering, software projects, software projects, Software systems, Software systems, versioning systems, versioning systems}
}

@inproceedings{hassan_road_2008,
	title = {The road ahead for Mining Software Repositories},
	doi = {10.1109/FOSM.2008.4659248},
	abstract = {Source control repositories, bug repositories, archived communications, deployment logs, and code repositories are examples of software repositories that are commonly available for most software projects. The mining software repositories ({MSR}) field analyzes and cross-links the rich data available in these repositories to uncover interesting and actionable information about software systems. By transforming these repositories from static record-keeping ones into active repositories, we can guide decision processes in modern software projects. For example, data in source control repositories, traditionally used to archive code, could be linked with data in bug repositories to help practitioners propagate complex changes and to warn them about risky code based on prior changes and bugs. In this paper, we present a brief history of the {MSR} field and discuss several recent achievements and results of using {MSR} techniques to support software research and practice. We then discuss the various opportunities and challenges that lie in the road ahead for this important and emerging field.},
	eventtitle = {Frontiers of Software Maintenance, 2008. {FoSM} 2008.},
	pages = {48 --57},
	booktitle = {Frontiers of Software Maintenance, 2008. {FoSM} 2008.},
	author = {Hassan, A.E.},
	date = {2008-10-28},
	%note = {Cited by 0075},
	keywords = {Application software, archived communications, bug repositories, code repositories, Communication system control, Computer bugs, data mining, decision making, decision processes, deployment logs, History, Information analysis, mining software repositories field, modern software projects, program debugging, project management, Resource management, Roads, Runtime, software engineering, Software systems, source control repositories, Testing},
	file = {IEEE Xplore Abstract Record:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\DHNM374J\\abs_all.html:text/html;IEEE Xplore Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\8KB9S9ZF\\Hassan - 2008 - The road ahead for Mining Software Repositories.pdf:application/pdf}
}

@inproceedings{jeong_improving_2009,
	location = {New York, {NY}, {USA}},
	title = {Improving bug triage with bug tossing graphs},
	isbn = {978-1-60558-001-2},
	url = {http://doi.acm.org/10.1145/1595696.1595715},
	doi = {10.1145/1595696.1595715},
	series = {{ESEC}/{FSE} '09},
	abstract = {bug report is typically assigned to a single developer who is then responsible for fixing the bug. In Mozilla and Eclipse, between 37\%-44\% of bug reports are "tossed" (reassigned) to other developers, for example because the bug has been assigned by accident or another developer with additional expertise is needed. In any case, tossing increases the time-to-correction for a bug. In this paper, we introduce a graph model based on Markov chains, which captures bug tossing history. This model has several desirable qualities. First, it reveals developer networks which can be used to discover team structures and to find suitable experts for a new task. Second, it helps to better assign developers to bug reports. In our experiments with 445,000 bug reports, our model reduced tossing events, by up to 72\%. In addition, the model increased the prediction accuracy by up to 23 percentage points compared to traditional bug triaging approaches.},
	pages = {111â€“120},
	booktitle = {Proceedings of the the 7th joint meeting of the European software engineering conference and the {ACM} {SIGSOFT} symposium on The foundations of software engineering},
	publisher = {{ACM}},
	author = {Jeong, Gaeul and Kim, Sunghun and Zimmermann, Thomas},
	urldate = {2013-08-22},
	date = {2009},
	%note = {Cited by 0093},
	keywords = {bug report assignment, bug report assignment, bug tossing, bug tossing, bug triage, bug triage, issue tracking, issue tracking, machine learning, machine learning, problem tracking, problem tracking}
}

@inproceedings{haderer_squaner:_2010,
	title = {{SQUANER}: A framework for monitoring the quality of software systems},
	doi = {10.1109/ICSM.2010.5609684},
	shorttitle = {{SQUANER}},
	abstract = {Despite the large number of quality models and publicly available quality assessment tools like {PMD}, Checkstyle, or {FindBugs}, very few studies have investigated the use of quality models by developers in their daily activities. One reason for this lack of studies is the absence of integrated environments for monitoring the evolution of software quality. We propose {SQUANER} (Software {QUality} {ANalyzER}), a framework for monitoring the evolution of the quality of object-oriented systems. {SQUANER} connects directly to the {SVN} of a system, extracts the source code, and perform quality evaluations and faults predictions every time a commit is made by a developer. After quality analysis, a feedback is provided to developers with instructions on how to improve their code.},
	pages = {1--4},
	booktitle = {2010 {IEEE} International Conference on Software Maintenance ({ICSM})},
	author = {Haderer, N. and Khomh, F. and Antoniol, G.},
	date = {2010},
	%note = {Cited by 0001},
	keywords = {Electronic mail, Electronic mail, faults prediction, faults prediction, Measurement, Measurement, Monitoring, Monitoring, object-oriented methods, object-oriented methods, Object oriented modeling, Object oriented modeling, object-oriented systems, object-oriented systems, Quality assessment, Quality assessment, quality assessment tool, quality assessment tool, quality evaluation, quality evaluation, software fault tolerance, software fault tolerance, software maintenance, software maintenance, software quality, software quality, software quality analysis, software quality analysis, Software {QUality} {ANalyzER}, Software {QUality} {ANalyzER}, Software systems, Software systems, {SQUANER}, {SQUANER}}
}

@inproceedings{sinha_mince:_2012,
	title = {{MINCE}: Mining change history of Android project},
	doi = {10.1109/MSR.2012.6224271},
	shorttitle = {{MINCE}},
	abstract = {An analysis of commit history of Android reveals that Android has a code base of 550K files, where on an average each file has been modified 8.7 times. 41\% of files have been modified at-least once. In terms of contributors, it has an overall contributor community of 1563, with 58.5\% of them having made {\textbackslash}textgreater; 5 commits. Moreover, the contributor community shows high churn levels, with only 13 of contributors continuing from 2005 to 2011. In terms of industry participation, Google \& Android account for 22\% of developers. Intel and {RedHat} account for 2\% of contributors each and {IBM}, Oracle, {TI}, {SGI} account for another 1\% each. Android code can be classified into 5 sub-projects: kernel, platform, device, tools and toolchain. In this paper, we profile each of these sub-projects in terms of change volumes, contributor and industry participation. We further picked specific framework topics such as {UI}, security, whose understanding is required from perspective of developing apps over Android, and present some insights on community participation around the same.},
	pages = {132--135},
	booktitle = {2012 9th {IEEE} Working Conference on Mining Software Repositories ({MSR})},
	author = {Sinha, V.S. and Mani, S. and Gupta, M.},
	date = {2012},
	%note = {Cited by 0003},
	keywords = {Android code, Android code, Android project, Android project, Androids, Androids, change volumes, change volumes, churn levels, churn levels, codes, codes, Communities, Communities, Companies, Companies, contributor community, contributor community, data mining, data mining, device subprojects, device subprojects, Google \& Android account, Google \& Android account, History, History, Humanoid robots, Humanoid robots, {IBM}, {IBM}, industry participation, industry participation, Intel and {RedHat} account, Intel and {RedHat} account, Kernel, Kernel, kernel subprojects, kernel subprojects, {MINCE}, {MINCE}, operating systems (computers), operating systems (computers), Oracle, Oracle, platform subproject, platform subproject, {SGI}, {SGI}, Smart phones, Smart phones, {TI}, {TI}, toolchain subprojects, toolchain subprojects, tools subprojects, tools subprojects}
}

@inproceedings{hassan_predicting_2009,
	location = {Washington, {DC}, {USA}},
	title = {Predicting faults using the complexity of code changes},
	isbn = {978-1-4244-3453-4},
	url = {http://dx.doi.org/10.1109/ICSE.2009.5070510},
	doi = {10.1109/ICSE.2009.5070510},
	series = {{ICSE} '09},
	abstract = {Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.},
	pages = {78â€“88},
	booktitle = {Proceedings of the 31st International Conference on Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Hassan, Ahmed E.},
	urldate = {2013-02-14},
	date = {2009},
	%note = {Cited by 0120}
}

@inproceedings{cotroneo_is_2010,
	title = {Is software aging related to software metrics?},
	doi = {10.1109/WOSAR.2010.5722096},
	abstract = {This work presents an empirical analysis aiming at investigating what kind of relationship exists between software aging and several static features of the software. While past studies on software aging focused on predicting the aging effects by monitoring and analytically modeling resource consumption at runtime, this study intends to explore if the static features of the software, as derived by its source code, presents potential relationships with software aging. We adopt a set of common software metrics concerning program structure, such as size and cyclomatic complexity, along with some features specifically developed for this study; metrics were then computed from ten complex software applications affected by aging. A statistical analysis to infer their relationship with software aging was carried out. Results encourage further investigations in this direction, since they show that software aging effects are related to the static features of software.},
	pages = {1 --6},
	booktitle = {2010 {IEEE} Second International Workshop on Software Aging and Rejuvenation ({WoSAR})},
	author = {Cotroneo, D. and Natella, R. and Pietrantuono, R.},
	date = {2010-11},
	%note = {Cited by 0007},
	keywords = {Aging, Aging, Aging-related bugs, Aging-related bugs, Complexity theory, Complexity theory, Computer bugs, Computer bugs, Correlation, Correlation, program structure, program structure, resource consumption, resource consumption, software, software, Software aging, Software aging, Software complexity metrics, Software complexity metrics, Software metrics, Software metrics, software performance evaluation, software performance evaluation, statistical analysis, statistical analysis}
}

@inproceedings{zimmermann_characterizing_2012,
	location = {Piscataway, {NJ}, {USA}},
	title = {Characterizing and predicting which bugs get reopened},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337363},
	series = {{ICSE} 2012},
	abstract = {Fixing bugs is an important part of the software development process. An underlying aspect is the effectiveness of fixes: if a fair number of fixed bugs are reopened, it could indicate instability in the software system. To the best of our knowledge there has been on little prior work on understanding the dynamics of bug reopens. Towards that end, in this paper, we characterize when bug reports are reopened by using the Microsoft Windows operating system project as an empirical case study. Our analysis is based on a mixed-methods approach. First, we categorize the primary reasons for reopens based on a survey of 358 Microsoft employees. We then reinforce these results with a large-scale quantitative study of Windows bug reports, focusing on factors related to bug report edits and relationships between people involved in handling the bug. Finally, we build statistical models to describe the impact of various metrics on reopening bugs ranging from the reputation of the opener to how the bug was found.},
	pages = {1074--1083},
	booktitle = {Proceedings of the 2012 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Zimmermann, Thomas and Nagappan, Nachiappan and Guo, Philip J. and Murphy, Brendan},
	urldate = {2013-10-23},
	date = {2012},
	%note = {Cited by 0016}
}

@inproceedings{kim_predicting_2007,
	location = {Washington, {DC}, {USA}},
	title = {Predicting Faults from Cached History},
	isbn = {0-7695-2828-7},
	url = {http://dx.doi.org/10.1109/ICSE.2007.66},
	doi = {10.1109/ICSE.2007.66},
	series = {{ICSE} '07},
	abstract = {We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10\% of the source code files; these files account for 73\%-95\% of faults– a significant advance beyond the state of the art.},
	pages = {489â€“498},
	booktitle = {Proceedings of the 29th international conference on Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Kim, Sunghun and Zimmermann, Thomas and Whitehead Jr., E. James and Zeller, Andreas},
	urldate = {2013-07-23},
	date = {2007},
	%note = {Cited by 0171}
}

@article{callaway_immunocytochemical_1989,
	title = {Immunocytochemical evidence for the presence of histamine and {GABA} in photoreceptors of the barnacle (Balanus nubilus)},
	volume = {3},
	issn = {0952-5238},
	abstract = {Biochemical evidence indicates that {GABA} and histamine may both be synthesized by barnacle photoreceptors (Koike \& Tsuda, 1980; Timpe \& Stuart, 1984; Callaway \& Stuart, 1989b). We used antisera against {GABA}- and histamine-protein conjugates to determine whether the photoreceptors contain either or both of these antigens. Both antisera labeled all of the photoreceptors in each of the three ocelli. Histamine-like immunoreactivity was found throughout each photoreceptor cell but was most intense at their presynaptic terminals. Histamine-like immunoreactivity was blocked by preincubation of the antibody either with histamine or with a histamine-protein conjugate. {GABA}-like immunoreactivity was found in all parts of the photoreceptors including the cell body, axon, rhabdomeric dendrites, and presynaptic terminals. {GABA}-protein conjugates blocked the {GABA}-like labeling of the photoreceptors, while protein conjugates with histamine, L-glutamate, L-glutamine, beta-alanine, and taurine did not. Histamine-like immunoreactivity in the supraesophageal ganglion was confined to the photoreceptor terminals and a second, loose plexus of endings in the main neuropil. {GABA}-like immunoreactivity, in contrast, was found in approximately twenty-five pairs of neurons of this ganglion. In the cirral nerves, which are expected to contain inhibitory motoneurons, unidentified axons also labeled with the {GABA} antiserum.},
	pages = {289--299},
	number = {4},
	journal = {Visual neuroscience},
	shortjournal = {Vis. Neurosci.},
	author = {Callaway, J C and Stuart, A E and Edwards, J S},
	date = {1989-10},
	%note = {Cited by 0027},
	keywords = {Animals, Antibody Specificity, Cross Reactions, gamma-Aminobutyric Acid, Ganglia, Histamine, Immunoenzyme Techniques, Neurons, Photoreceptor Cells, Thoracica}
}

@inproceedings{demeyer_happy_2013,
	location = {Piscataway, {NJ}, {USA}},
	title = {Happy birthday! a trend analysis on past {MSR} papers},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487151},
	series = {{MSR} '13},
	abstract = {On the occasion of the 10th anniversary of the {MSR} conference, it is a worthwhile exercise to meditate on the past, present and future of our research discipline. Indeed, since the {MSR} community has experienced a big influx of researchers bringing in new ideas, state-of-the art technology and contemporary research methods it is unclear what the future might bring. In this paper, we report on a text mining exercise applied on the complete corpus of {MSR} papers to reflect on where we come from; where we are now; and where we should be going. We address issues like the trendy (and outdated) research topics; the frequently (and less frequently) cited cases; the popular (and emerging) mining infrastructure; and finally the proclaimed actionable information which we are deemed to uncover.},
	pages = {353â€“362},
	booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
	publisher = {{IEEE} Press},
	author = {Demeyer, Serge and Murgia, Alessandro and Wyckmans, Kevin and Lamkanfi, Ahmed},
	urldate = {2013-07-03},
	date = {2013},
	%note = {Cited by 0000}
}

@inproceedings{john_estimating_1995,
	title = {Estimating Continuous Distributions in Bayesian Classifiers},
	abstract = {When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995 1 Introduction In rec...},
	pages = {338--345},
	booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
	publisher = {Morgan Kaufmann},
	author = {John, George and Langley, Pat},
	year = {1995},
	file = {Citeseer - Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\9I6BH9DV\\John e Langley - 1995 - Estimating Continuous Distributions in Bayesian Cl.pdf:application/pdf;Citeseer - Snapshot:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\CW79TQ2X\\summary.html:text/html}
}


@book{quinlan_c4.5:_1993,
	address = {San Francisco, {CA}, {USA}},
	title = {C4.5: Programs for Machine Learning},
	isbn = {1558602402},
	shorttitle = {C4.5},
	abstract = {From the Publisher:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on {ID}3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the {UNIX} environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation.  C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.  This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Quinlan, J. Ross},
	year = {1993}
}

@inproceedings{storey_r_2014,
	location = {New York, {NY}, {USA}},
	title = {The (R) Evolution of Social Media in Software Engineering},
	isbn = {978-1-4503-2865-4},
	url = {http://doi.acm.org/10.1145/2593882.2593887},
	doi = {10.1145/2593882.2593887},
	series = {{FOSE} 2014},
	abstract = {Software developers rely on media to communicate, learn, collaborate, and coordinate with others. Recently, social media has dramatically changed the landscape of software engineering, challenging some old assumptions about how developers learn and work with one another. We see the rise of the social programmer who actively participates in online communities and openly contributes to the creation of a large body of crowdsourced socio-technical content.   In this paper, we examine the past, present, and future roles of social media in software engineering. We provide a review of research that examines the use of different media channels in software engineering from 1968 to the present day. We also provide preliminary results from a large survey with developers that actively use social media to understand how they communicate and collaborate, and to gain insights into the challenges they face. We find that while this particular population values social media, traditional channels, such as face-to-face communication, are still considered crucial. We synthesize findings from our historical review and survey to propose a roadmap for future research on this topic. Finally, we discuss implications for research methods as we argue that social media is poised to bring about a paradigm shift in software engineering research.},
	pages = {100--116},
	booktitle = {Proceedings of the on Future of Software Engineering},
	publisher = {{ACM}},
	author = {Storey, Margaret-Anne and Singer, Leif and Cleary, Brendan and Figueira Filho, Fernando and Zagalsky, Alexey},
	urldate = {2014-06-05},
	date = {2014},
	%note = {00001},
	keywords = {collaboration, social media, software engineering},
	file = {ACM Full Text PDF:D\:\\Softwares\\Zotero Standalone\\dados\\storage\\ENKVV3X8\\Storey et al. - 2014 - The (R) Evolution of Social Media in Software Engi.pdf:application/pdf}
}

@inproceedings{ossher_sourcererdb:_2009,
	title = {{SourcererDB}: An aggregated repository of statically analyzed and cross-linked open source Java projects},
	doi = {10.1109/MSR.2009.5069501},
	shorttitle = {{SourcererDB}},
	abstract = {The open source movement has made vast quantities of source code available online for free, providing an extremely large dataset for empirical study and potential resuse. A major difficulty in exploiting this potential fully is that the data are currently scattered between competing source code repositories, none of which are structured for empirical analysis and cross-project comparison. As a result, software researchers and developers are left to compile their own datasets, resulting in duplicated effort and limited results. To address this challenge, we built {SourcererDB}, an aggregated repository of statically analyzed and cross-linked open source Java projects. {SourcererDB} contains local snapshots of 2,852 Java projects taken from Sourceforge, Apache and Java.net. These projects are statically analyzed to extract rich structural information, which is then stored in a relational database. References to entities in the 16,058 external jars are resolved and grouped, allowing for cross-project usage information to be accessed easily. This paper describes: (a) the mechanism for resolving and grouping these cross-project references, (b) the structure of and the metamodel for the {SourcererDB} repository, and (d) end-user dataset access mechanisms. Our goal in building {SourcererDB} is to provide a rich dataset of source code to facilitate the sharing of extracted data and to encourage reuse and repeatability of experiments.},
	pages = {183--186},
	booktitle = {6th {IEEE} International Working Conference on Mining Software Repositories, 2009. {MSR} '09},
	author = {Ossher, J. and Bajracharya, Sushil and Linstead, E. and Baldi, P. and Lopes, C.},
	date = {2009},
	%note = {Cited by 0023},
	keywords = {collaboration, collaboration, cross-linked open source Java project, cross-linked open source Java project, data mining, data mining, Information analysis, Information analysis, Java, Java, Open source software, Open source software, program diagnostics, program diagnostics, project management, project management, public domain software, public domain software, query processing, query processing, relational database, relational database, relational databases, relational databases, Scattering, Scattering, Search engines, Search engines, Solids, Solids, source code, source code, {SourcererDB}, {SourcererDB}, Standardization, Standardization, static analysis, static analysis, structural information extraction, structural information extraction}
}

@inproceedings{challagulla_empirical_2005,
	title = {Empirical assessment of machine learning based software defect prediction techniques},
	doi = {10.1109/WORDS.2005.32},
	abstract = {The wide-variety of real-time software systems, including telecontrol/telepresence systems, robotic systems, and mission planning systems, can entail dynamic code synthesis based on runtime mission-specific requirements and operating conditions. This necessitates the need for dynamic dependability assessment to ensure that these systems perform as specified and not fail in catastrophic ways. One approach in achieving this is to dynamically assess the modules in the synthesized code using software defect prediction techniques. Statistical models; such as stepwise multi-linear regression models and multivariate models, and machine learning approaches, such as artificial neural networks, instance-based reasoning, Bayesian-belief networks, decision trees, and rule inductions, have been investigated for predicting software quality. However, there is still no consensus about the best predictor model for software defects. In this paper; we evaluate different predictor models on four different real-time software defect data sets. The results show that a combination of {IR} and instance-based learning along with the consistency-based subset evaluation technique provides a relatively better consistency in accuracy prediction compared to other models. The results also show that "size" and "complexity" metrics are not sufficient for accurately predicting real-time software defects.},
	pages = {263--270},
	booktitle = {10th {IEEE} International Workshop on Object-Oriented Real-Time Dependable Systems, 2005. {WORDS} 2005},
	author = {Challagulla, V.U.B. and Bastani, F.B. and Yen, I.-Ling and Paul, R.A.},
	date = {2005},
	%note = {Cited by 0004},
	keywords = {Artificial neural networks, Artificial neural networks, Bayesian-belief networks, Bayesian-belief networks, Bayesian methods, Bayesian methods, belief networks, belief networks, consistency-based subset evaluation, consistency-based subset evaluation, decision trees, decision trees, dynamic code synthesis, dynamic code synthesis, dynamic dependability assessment, dynamic dependability assessment, instance-based reasoning, instance-based reasoning, learning (artificial intelligence), learning (artificial intelligence), machine learning, machine learning, mission planning systems, mission planning systems, multivariate models, multivariate models, Network synthesis, Network synthesis, Predictive models, Predictive models, predictor model, predictor model, real-time software systems, real-time software systems, Real time systems, Real time systems, regression analysis, regression analysis, Regression tree analysis, Regression tree analysis, robotic systems, robotic systems, Robots, Robots, rule inductions, rule inductions, Runtime, Runtime, runtime mission-specific requirements, runtime mission-specific requirements, safety-critical software, safety-critical software, software defect prediction, software defect prediction, software performance evaluation, software performance evaluation, software quality, software quality, Software systems, Software systems, statistical models, statistical models, stepwise multilinear regression models, stepwise multilinear regression models, telecontrol systems, telecontrol systems, telepresence systems, telepresence systems}
}

@inproceedings{yang_empirical_2012,
	title = {An Empirical Study on Improving Severity Prediction of Defect Reports Using Feature Selection},
	volume = {1},
	doi = {10.1109/APSEC.2012.144},
	abstract = {In software maintenance, severity prediction on defect reports is an emerging issue obtaining research attention due to the considerable triaging cost. In the past research work, several text mining approaches have been proposed to predict the severity using advanced learning models. Although these approaches demonstrate the effectiveness of predicting the severity, they do not discuss the problem of how to find the indicators in good quality. In this paper, we discuss whether feature selection can benefit the severity prediction task with three commonly used feature selection schemes, Information Gain, Chi-Square, and Correlation Coefficient, based on the Multinomial Naive Bayes classification approach. We have conducted empirical experiments with four open-source components from Eclipse and Mozilla. The experimental results show that these three feature selection schemes can further improve the predication performance in over half the cases.},
	pages = {240--249},
	booktitle = {Software Engineering Conference ({APSEC}), 2012 19th Asia-Pacific},
	author = {Yang, Cheng-Zen and Hou, Chun-Chi and Kao, Wei-Chen and Chen, Ing-Xiang},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {advanced learning models, advanced learning models, Bayes methods, Bayes methods, chi-square, chi-square, correlation coefficient, correlation coefficient, data mining, data mining, defect reports, defect reports, Eclipse, Eclipse, feature extraction, feature extraction, Feature Selection, Feature Selection, Frequency measurement, Frequency measurement, information gain, information gain, learning (artificial intelligence), learning (artificial intelligence), Mozilla, Mozilla, multinomial naive Bayes classification approach, multinomial naive Bayes classification approach, Niobium, Niobium, open-source components, open-source components, pattern classification, pattern classification, Performance evaluation, Performance evaluation, Predictive models, Predictive models, public domain software, public domain software, severity prediction, severity prediction, software, software, software maintenance, software maintenance, text mining, text mining, text mining approaches, text mining approaches, Training, Training, triaging cost, triaging cost}
}

@inproceedings{zimmermann_knowledge_2006,
	location = {Tokyo},
	title = {Knowledge Collaboration by Mining Software Repositories},
	abstract = {We will give a short overview on recent approaches to support developers by mining software repositories and outline current and future challenges from which knowledge collaboration can benefit.},
	booktitle = {Proceedings of the 2nd International Workshop on Supporting Knowledge Collaboration in Software Development},
	publisher = {Yunwen Ye \& Masao Ohira},
	author = {Zimmermann, Thomas},
	date = {2006},
	%note = {Cited by 0010}
}

@book{goldschmidt_data_2005,
	location = {Rio de Janeiro},
	edition = {3ª},
	title = {Data mining: um guia prático},
	publisher = {Elsevier},
	author = {Goldschmidt, Ronaldo and Passos, Emmanuel},
	date = {2005},
	%note = {Cited by 0026}
}

@inproceedings{nguyen_multi-layered_2012,
	location = {New York, {NY}, {USA}},
	title = {Multi-layered approach for recovering links between bug reports and fixes},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393671},
	doi = {10.1145/2393596.2393671},
	series = {{FSE} '12},
	abstract = {The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar. This paper introduces {MLink}, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that {MLink} can improve the state-of-the-art bug-to-fix link recovery methods by 11--18\%, 13--17\%, and 8--17\% in F-score, recall, and precision, respectively.},
	pages = {63:1--63:11},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Nguyen, Tien N.},
	urldate = {2013-07-03},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {bugs, bug-to-fix links, fixes, mining software repository}
}

@inproceedings{canavera_mining_2012,
	location = {New York, {NY}, {USA}},
	title = {Mining the execution history of a software system to infer the best time for its adaptation},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393616},
	doi = {10.1145/2393596.2393616},
	series = {{FSE} '12},
	abstract = {An important challenge in dynamic adaptation of a software system is to prevent inconsistencies (failures) and disruptions in its operations during and after change. Several prior techniques have solved this problem with various tradeoffs. All of them, however, assume the availability of detailed component dependency models. This paper presents a complementary technique that solves this problem in settings where such models are either not available, difficult to build, or outdated due to the evolution of the software. Our approach first mines the execution history of a software system to infer a stochastic component dependency model, representing the probabilistic sequence of interactions among the system's components. We then demonstrate how this model could be used at runtime to infer the "best time" for adaptation of the system's components. We have thoroughly evaluated this research on a multi-user real world software system and under varying conditions.},
	pages = {18:1â€“18:11},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Canavera, Kyle R. and Esfahani, Naeem and Malek, Sam},
	urldate = {2013-07-03},
	date = {2012},
	%note = {Cited by 0000},
	keywords = {component dependency, component dependency, data mining, data mining, dynamic adaptation, dynamic adaptation}
}

@inproceedings{wu_drex:_2011,
	title = {{DREX}: Developer Recommendation with K-Nearest-Neighbor Search and Expertise Ranking},
	doi = {10.1109/APSEC.2011.15},
	shorttitle = {{DREX}},
	abstract = {This paper proposes a new approach called {DREX} (Developer Recommendation with k-nearest-neighbor search and Expertise ranking) to developer recommendation for bug resolution based on K-Nearest-Neighbor search with bug similarity and expertise ranking with various metrics, including simple frequency and social network metrics. We collect Mozilla Fire fox open bug repository as the experimental data set and compare different ranking metrics on the performance of recommending capable developers for bugs. Our experimental results demonstrate that, when recommending 10 developers for each one of the 250 testing bugs, {DREX} has produced better performance than traditional methods with multi-labeled text categorization. The best performance obtained by two metrics as Out-Degree and Frequency, is with recall as 0.6 on average. Moreover, other social network metrics such as Degree and Page Rank have produced comparable performance on developer recommendation as Frequency when used for developer expertise ranking.},
	eventtitle = {Software Engineering Conference ({APSEC}), 2011 18th Asia Pacific},
	pages = {389--396},
	author = {Wu, Wenjin and Zhang, Wen and Yang, Ye and Wang, Qing},
	date = {2011},
	%note = {Cited by 0007},
	keywords = {bug repository, bug resolution, bug similarity, Computer bugs, developer recommendation, {DREX}, expertise ranking, Fires, K-nearest-neighbor search, {KNN} Search, Measurement, Mozilla Fire fox, Open Bug Repository, Page Rank, program debugging, social networking (online), social network metrics, Social network services, software, Testing, Vectors}
}

@book{amo_curso_2003,
	title = {Curso de data mining},
	author = {Amo, Sandra A. de},
	date = {2003},
	%note = {Cited by 0002}
}

@inproceedings{rahman_recalling_2012,
	location = {New York, {NY}, {USA}},
	title = {Recalling the "imprecision" of cross-project defect prediction},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393669},
	doi = {10.1145/2393596.2393669},
	series = {{FSE} '12},
	abstract = {There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these {IR}-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a range of time-and-cost vs quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, viz., 5\%, 10\% or 20\% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction!},
	pages = {61:1â€“61:11},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th International Symposium on the Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
	urldate = {2013-07-03},
	date = {2012},
	%note = {Cited by 0009},
	keywords = {empirical software engineering, empirical software engineering, Fault prediction, Fault prediction, inspection, inspection}
}

@article{kagdi_assigning_2012,
	title = {Assigning change requests to software developers},
	volume = {24},
	rights = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.530/abstract},
	doi = {10.1002/smr.530},
	abstract = {The paper presents an approach to recommend a ranked list of expert developers to assist in the implementation of software change requests (e.g., bug reports and feature requests). An Information Retrieval ({IR})-based concept location technique is first used to locate source code entities, e.g., files and classes, relevant to a given textual description of a change request. The previous commits from version control repositories of these entities are then mined for expert developers. The role of the {IR} method in selectively reducing the mining space is different from previous approaches that textually index past change requests and/or commits. The approach is evaluated on change requests from three open-source systems: {ArgoUML}, Eclipse, and {KOffice}, across a range of accuracy criteria. The results show that the overall accuracies of the correctly recommended developers are between 47 and 96\% for bug reports, and between 43 and 60\% for feature requests. Moreover, comparison results with two other recommendation alternatives show that the presented approach outperforms them with a substantial margin. Project leads or developers can use this approach in maintenance tasks immediately after the receipt of a change request in a free-form text. Copyright © 2011 John Wiley \& Sons, Ltd.},
	pages = {3--33},
	number = {1},
	journal = {Journal of Software: Evolution and Process},
	author = {Kagdi, Huzefa and Gethers, Malcom and Poshyvanyk, Denys and Hammad, Maen},
	urldate = {2013-09-13},
	date = {2012},
	langid = {english},
	keywords = {concept and feature location, developer recommendation, information retrieval, mining software repositories, software evolution and maintenance}
}